{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AtaxxZero\n",
    "## This algorithm tried to reimplement AlphaGo Zero for Ataxx\n",
    "## however, the computation to train an AI from scratch can be too heavy, given my skills of code optimization and hardware limitation\n",
    "## therefore, minor adjustments are made to the algorithms to make it plausible for this algorithm to give a rather satisfactory result in an acceptable period.\n",
    "\n",
    "### modifications:\n",
    "1. One major difference between AlphaGo Zero and Ataxx Zero is that Ataxx Zero relies __one engineered value function__. From the beginning of the training, the q value of each node is a combination of q from the hybrid network and a greedy function (output is monotone increasing with regard to difference of piece no. of each player).\n",
    "2. Another major modification is Ataxx Zero apply MCTS to a __very shallow depth, currently being 3__. This change significantly reduce the searching time, thus accelerate training greatly.\n",
    "3. The combination of 3 layer MCTS and an engineered value function guarantees a good performance of the algorithm in even before training, i.e. hybrid network output random probability and value. The behavior of Ataxx Zero before training should __resemble an impaired MinMax Searching with a depth of 3__. From a practical perspective, it wins 90% of the game with a greedy player(which attempts to maximize no.my_piece - no.opponent's_piece). With reinforcement learning, the algorithm is expected to behave better.\n",
    "4. When actually applied in game, I plan to reduce the searching depth to 2 to further improve the speed, but expect the algorithm to work better than itself before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "from Cython.Compiler.Options import directive_scopes, directive_types\n",
    "directive_types['linetrace'] = True\n",
    "directive_types['binding'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import importlib\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from multiprocessing import Queue, Pool, Process\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from math import sqrt, log, exp\n",
    "from numpy import unravel_index\n",
    "from random import choice, random, sample\n",
    "from operator import itemgetter\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Reshape, Lambda\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LocallyConnected2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, AlphaDropout, ConvLSTM2D, AvgPool2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import add, concatenate, multiply, Multiply\n",
    "from keras.initializers import VarianceScaling, RandomUniform\n",
    "from keras.optimizers import Adam, SGD, rmsprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils, multi_gpu_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.engine.topology import Container\n",
    "from keras.optimizers import SGD, Adadelta, Adagrad\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_keras_backend(backend):\n",
    "    os.environ['KERAS_BACKEND'] = backend\n",
    "    importlib.reload(K)\n",
    "    K.set_image_dim_ordering('th')\n",
    "    assert K.backend() == backend\n",
    "\n",
    "def set_omp_threads(n):\n",
    "    n = str(n)\n",
    "    os.environ['OMP_NUM_THREADS'] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "set_keras_backend('tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rot_policy_dict():\n",
    "    augment_dict = {}\n",
    "    rot_m_0 = np.array([[0, -1], [1, 0]]) \n",
    "    center = np.array([3, 3])\n",
    "    for is_flip in [False, True]:\n",
    "        for rot_time in range(4):\n",
    "            if not (is_flip == False and rot_time == 0):\n",
    "                tmp_dict = {}\n",
    "                # get rot matrix\n",
    "                rot_m = np.eye(2)\n",
    "                for i in range(rot_time):\n",
    "                    rot_m = rot_m_0.dot(rot_m)\n",
    "\n",
    "                for r in range(7):\n",
    "                    for c in range(7):\n",
    "                        for dr in range(-2, 3):\n",
    "                            for dc in range(-2, 3):\n",
    "                                start = np.array([r, c])\n",
    "                                end = np.array([r+dr, c+dc])\n",
    "                                if (dr != 0 or dc != 0) and \\\n",
    "                                    (start[0] < 7 and start[0] >= 0) and (start[1] < 7 and start[1] >= 0):\n",
    "                                    new_start = (start - center)\n",
    "                                    new_end = (end - center)\n",
    "                                    if is_flip:\n",
    "                                        new_start[1] = -new_start[1]\n",
    "                                        new_end[1] = -new_end[1]\n",
    "                                    new_start = rot_m.dot(new_start) + center\n",
    "                                    new_end = rot_m.dot(new_end) + center\n",
    "                                    tmp_dict[(tuple(start), tuple(end))] = (tuple(new_start), tuple(new_end))\n",
    "                augment_dict[(is_flip, rot_time)] = tmp_dict\n",
    "    return augment_dict\n",
    "augment_dict = get_rot_policy_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_policy(data, is_flip, rot_time):\n",
    "    if is_flip == False and rot_time == 0:\n",
    "        return data\n",
    "    \n",
    "    global policy_dict, policy_list, augment_dict\n",
    "    out = np.zeros_like(data)\n",
    "    for i in range(792):\n",
    "        tmp_data = data[i]\n",
    "        if tmp_data > 0:\n",
    "            move = policy_list[i]\n",
    "            move_after_rot = augment_dict[(is_flip, rot_time)][move]\n",
    "            i_after_rot = policy_dict[move_after_rot]\n",
    "            out[i_after_rot] = tmp_data\n",
    "    return out\n",
    "            \n",
    "def augment_data(train_data):\n",
    "    out = []\n",
    "    # input is a list for follows\n",
    "    feature_map = train_data[0]\n",
    "    action_mask = train_data[1]\n",
    "    frequency_map = train_data[2]\n",
    "    value = train_data[3]\n",
    "    \n",
    "    # do 7 times augmentation\n",
    "    for is_flip in [False, True]:\n",
    "        for rot_time in range(4):\n",
    "            # do feature map augmentation\n",
    "            if is_flip:\n",
    "                tmp_feature_map = np.fliplr(feature_map)\n",
    "            tmp_feature_map = np.rot90(feature_map, k=rot_time, axes=(1, 2))\n",
    "            # augment two policy related data\n",
    "            tmp_action_mask = augment_policy(action_mask, is_flip, rot_time)\n",
    "            tmp_frequency_map = augment_policy(frequency_map, is_flip, rot_time)\n",
    "            # append them to out\n",
    "            out.append([tmp_feature_map, tmp_action_mask, tmp_frequency_map, value])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## might be a bit of ugly, but it's really efficient to isolate functions that can be accelerated by cython and jit\n",
    "1. all the dictionary lookup and creation are all integrated in the following cell, which at least reduce the running time for 40%\n",
    "2. memoryview in cython is a strong weapon in terms of algorithm speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "# -a -f --compile-args=-DCYTHON_TRACE=1\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "\n",
    "def get_policy_dict_list():\n",
    "    index=0\n",
    "    policy_dict = {}\n",
    "    policy_list = []\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            for dr in range(-2, 3):\n",
    "                for dc in range(-2, 3):\n",
    "                    new_r = r + dr\n",
    "                    new_c = c + dc\n",
    "                    if (dr != 0 or dc != 0) and (new_r < 7 and new_r >= 0) and (new_c < 7 and new_c >= 0):\n",
    "                        policy_dict[((r, c), (new_r, new_c))] = index\n",
    "                        policy_list.append(((r, c), (new_r, new_c)))\n",
    "                        index += 1\n",
    "    return policy_dict, policy_list\n",
    "\n",
    "policy_dict, policy_list = get_policy_dict_list()\n",
    "\n",
    "# this is for expand last two lines\n",
    "def assign_children(children, np.float32_t[:] p_array):\n",
    "    for move in children:\n",
    "        children[move] = np.float32(p_array[policy_dict[move]])\n",
    "\n",
    "cdef class Ataxx:\n",
    "    cdef public np.int8_t[:, :] data\n",
    "\n",
    "    def __init__(self, board=None):\n",
    "        if board is None:                  # if there is no initialization given\n",
    "            self.data = np.zeros((7, 7), dtype=np.int8)   # then generate a board with starting init, and black(-1) takes first turn\n",
    "            self.data[0, 0] = -1           \n",
    "            self.data[6, 6] = -1\n",
    "            self.data[0, 6] = 1\n",
    "            self.data[6, 0] = 1\n",
    "        else:\n",
    "            self.data = board.copy()\n",
    "            \n",
    "    def reset(self, board=None):\n",
    "        if board is None:\n",
    "            self.data = np.zeros((7, 7), dtype=np.int8)\n",
    "            self.data[0, 0] = -1           \n",
    "            self.data[6, 6] = -1\n",
    "            self.data[0, 6] = 1\n",
    "            self.data[6, 0] = 1\n",
    "        else:\n",
    "            self.data = board.copy()\n",
    "        \n",
    "    def get_feature_map(self, turn, move):\n",
    "        cdef int j, k\n",
    "        cdef np.int8_t[:, :, :] out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "        \n",
    "        # edge\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j == 0 or j == 8 or k == 0 or k == 8:\n",
    "                    out[0, j, k] = 1\n",
    "         \n",
    "        # my pieces\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j > 0 and j < 8 and k > 0 and k < 8:\n",
    "                    if self.data[j-1, k-1] == turn:\n",
    "                        out[1, j, k] = 1\n",
    "        \n",
    "        # op pieces\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j > 0 and j < 8 and k > 0 and k < 8:\n",
    "                    if self.data[j-1, k-1] == -turn:\n",
    "                        out[2, j, k] = 1\n",
    "         \n",
    "        # last move\n",
    "        if not move is None:               \n",
    "            out[3, move[0][0]+1, move[0][1]+1] = 1\n",
    "            out[4, move[1][0]+1, move[1][1]+1] = 1\n",
    "            \n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            for j in range(9):\n",
    "                for k in range(9):\n",
    "                    out[5, j, k] = 1\n",
    "        return np.array(out)\n",
    "    \n",
    "    def plot(self, is_next_move=False, turn=None):                        # plot the board\n",
    "        image = self.data.copy()\n",
    "        if is_next_move:\n",
    "            if turn not in [-1, 1]:\n",
    "                raise ValueError(\"Turn must be -1 or 1, or Must input a turn for next moves\")\n",
    "            else:\n",
    "                next_moves = self.get_moves(turn)\n",
    "                if len(next_moves) == 0:\n",
    "                    raise ValueError(\"Game is over already\")\n",
    "                next_pos = list(zip(*next_moves))[1]\n",
    "                for pos in next_pos:\n",
    "                    image[pos] = turn / 2\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.xticks(range(7), range(7))\n",
    "        plt.yticks(range(7), range(7))\n",
    "        plt.show()\n",
    "                \n",
    "    def is_valid(self, turn, pos):\n",
    "        cdef int dr, dc, r = pos[0], c = pos[1], new_r, new_c\n",
    "        if self.data[r, c] != 0:\n",
    "            return False\n",
    "        else:\n",
    "            for dr in range(-2, 3):\n",
    "                for dc in range(-2, 3):\n",
    "                    new_r = r+dr\n",
    "                    new_c = c+dc\n",
    "                    if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and self.data[new_r, new_c] == turn:\n",
    "                        return True\n",
    "            return False \n",
    "        \n",
    "    def get_moves(self, turn, return_node_info=False):\n",
    "        cdef int r, c, dr, dc, new_r, new_c\n",
    "        cdef np.int8_t[:] action_mask = np.zeros(792, dtype=np.int8)\n",
    "        next_moves = []\n",
    "        corr_dict = {}\n",
    "        children_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "                if self.is_valid(turn, (r, c)): # duplicate move\n",
    "                    for dr in range(-2, 3):\n",
    "                        for dc in range(-2, 3):\n",
    "                            new_r = r+dr\n",
    "                            new_c = c+dc\n",
    "                            if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and self.data[new_r, new_c] == turn:\n",
    "                                if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                                    if has_duplicate_move: \n",
    "                                        cur_move = ((new_r, new_c), (r, c))\n",
    "                                        corr_dict[cur_move] = dup_move\n",
    "                                        # update action mask\n",
    "                                        if return_node_info: \n",
    "                                            action_mask[policy_dict[cur_move]] = 1\n",
    "                                    elif self.data[new_r, new_c] == turn:\n",
    "                                        dup_move = ((new_r, new_c), (r, c))\n",
    "                                        next_moves.append(dup_move) \n",
    "                                        has_duplicate_move = True\n",
    "                                        # preparing children nodes and action mask\n",
    "                                        if return_node_info: \n",
    "                                            children_dict[dup_move] = None\n",
    "                                            action_mask[policy_dict[dup_move]] = 1\n",
    "                                elif self.data[new_r, new_c] == turn:\n",
    "                                    cur_move = ((new_r, new_c), (r, c))\n",
    "                                    next_moves.append(cur_move) \n",
    "                                    # preparing children nodes and action mask\n",
    "                                    if return_node_info:\n",
    "                                        children_dict[cur_move] = None\n",
    "                                        action_mask[policy_dict[cur_move]] = 1\n",
    "                                else:\n",
    "                                    continue\n",
    "        if return_node_info:\n",
    "            return next_moves, corr_dict, children_dict, np.array(action_mask)\n",
    "        else:\n",
    "            return next_moves\n",
    "    \n",
    "    def get_greedy_move(self, turn, moves=None):\n",
    "        cdef int x0, y0, x1, y1, dr, dc, tmp_score, best_score = -50\n",
    "        # get all possible moves if not provided\n",
    "        if moves is None:\n",
    "            moves, corr_dict, _, _ = self.get_moves(turn, return_node_info=True)\n",
    "            for item in corr_dict:\n",
    "                moves.append(item)\n",
    "        \n",
    "        if len(moves) == 0:\n",
    "            raise ValueError('No Possible Moves')\n",
    "        \n",
    "        best_moves = []\n",
    "        # calculate greedy move\n",
    "        for (x0, y0), (x1, y1) in moves:\n",
    "            tmp_score = 0\n",
    "            if abs(x0-x1) <= 1 and abs(y0-y1) <= 1:\n",
    "                tmp_score += 1\n",
    "            for dr in range(-1, 2):\n",
    "                for dc in range(-1, 2):\n",
    "                    try:\n",
    "                        if x1+dr >= 0 and y1+dc >= 0:\n",
    "                            tmp_score += self.data[x1+dr, y1+dc] == -turn\n",
    "                    except:\n",
    "                        pass\n",
    "            if tmp_score > best_score:\n",
    "                best_moves = [((x0, y0), (x1, y1))]\n",
    "                best_score = tmp_score\n",
    "            elif tmp_score == best_score:\n",
    "                best_moves.append(((x0, y0), (x1, y1)))\n",
    "        return choice(best_moves)\n",
    "        \n",
    "    def move_to(self, turn, pos0, pos1):\n",
    "        cdef int dr, dc, x0 = pos0[0], y0 = pos0[1], x1 = pos1[0], y1 = pos1[1]\n",
    "        \n",
    "        if not self.is_valid(turn, pos1):\n",
    "            raise ValueError(\"This move: \" + str((pos0, pos1)) + \" of turn: \" + str(turn) + \" is invalid\") \n",
    "        elif self.data[x0, y0] != turn:\n",
    "            raise ValueError(\"The starting position is not your piece\")\n",
    "        else:\n",
    "            self.data[x1, y1] = turn\n",
    "            if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "                self.data[x0, y0] = 0\n",
    "\n",
    "            for dr in range(-1, 2):                  # infection mode!!!!\n",
    "                for dc in range(-1, 2):\n",
    "                    if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                        if self.data[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                            self.data[x1+dr, y1+dc] = turn\n",
    "                            \n",
    "    @staticmethod                       \n",
    "    def get_manual_q(int turn, np.int8_t[:, :] board):\n",
    "        '''consider linear growth of win prob with regard to n_diff\n",
    "        when diff >= 10, the slope grow a bit\n",
    "        when diff >= 35, consider win prob close to 1 or -1\n",
    "        ''' \n",
    "        cdef int r, c, turn_no = 0, op_no = 0\n",
    "        cdef float max1=0.9, max2=0.95, diff, sign\n",
    "        # get no diff of turns\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if board[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif board[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        diff = turn_no - op_no\n",
    "        if abs(diff) > 30:\n",
    "            return diff / abs(diff)\n",
    "        else:\n",
    "            return diff / 30\n",
    "        \n",
    "        # ignore the rest for now\n",
    "        sign = diff\n",
    "        diff = abs(diff)\n",
    "        if diff < 35:\n",
    "            diff = (diff / 35) ** 2 * max1\n",
    "        else:\n",
    "            diff = max2\n",
    "\n",
    "        if sign < 0:\n",
    "            return -diff\n",
    "        else:\n",
    "            return diff\n",
    "    \n",
    "    def evaluate(self, turn, this_turn, max_score=1, min_score=0.001):\n",
    "        cdef int r, c, turn_no=0, op_no=0\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if self.data[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif self.data[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        if len(self.get_moves(this_turn)) == 0:# if one of them can no longer move, count and end\n",
    "            if turn_no > op_no:\n",
    "                return max_score\n",
    "            else:\n",
    "                return -max_score\n",
    "        else:\n",
    "            value = turn_no - op_no\n",
    "        return value * min_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cell, a recursive alpha-beta pruning based on piece no. difference evalutation function is implemented.\n",
    "* quite amazingly, the programming process was finished within 30 min, with only two cycle of debugging.\n",
    "1. Very interestingly, seemingly if we apply count diff as evaluation, it plays not as good if we apply count pieces~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "# -a -f --compile-args=-DCYTHON_TRACE=1\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choice\n",
    "\n",
    "'''These methods are for Min max'''\n",
    "def evaluate(np.int8_t[:, :] board, int turn):\n",
    "    cdef int r, c, turn_no = 0, op_no = 0\n",
    "    # get no diff of turns\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            if board[r, c] == turn:\n",
    "                turn_no += 1\n",
    "            elif board[r, c] == -turn:\n",
    "                op_no += 1\n",
    "    return (turn_no - op_no)\n",
    "        \n",
    "def is_valid(np.int8_t[:, :] board, turn, pos):\n",
    "    cdef int dr, dc, r = pos[0], c = pos[1], new_r, new_c\n",
    "    if board[r, c] != 0:\n",
    "        return False\n",
    "    else:\n",
    "        for dr in range(-2, 3):\n",
    "            for dc in range(-2, 3):\n",
    "                new_r = r+dr\n",
    "                new_c = c+dc\n",
    "                if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                    return True\n",
    "        return False \n",
    "    \n",
    "\n",
    "def next_move(np.int8_t[:, :] board, turn):\n",
    "    cdef int r, c, dr, dc, new_r, new_c\n",
    "    next_moves = []\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "            if is_valid(board, turn, (r, c)): # duplicate move\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r+dr\n",
    "                        new_c = c+dc\n",
    "                        if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                            if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                                if board[new_r, new_c] == turn and not has_duplicate_move:\n",
    "                                    dup_move = ((new_r, new_c), (r, c))\n",
    "                                    has_duplicate_move = True\n",
    "                                    yield dup_move\n",
    "                            elif board[new_r, new_c] == turn:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                yield cur_move\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "def has_next_move(np.int8_t[:, :] board, turn):\n",
    "    try:\n",
    "        next(next_move(board, turn))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "                                \n",
    "def move_to(np.int8_t[:, :] board, turn, pos0, pos1):\n",
    "    cdef int dr, dc, x0 = pos0[0], y0 = pos0[1], x1 = pos1[0], y1 = pos1[1]\n",
    "\n",
    "    if not is_valid(board, turn, pos1):\n",
    "        raise ValueError(\"This move: \" + str((pos0, pos1)) + \" of turn: \" + str(turn) + \" is invalid\") \n",
    "    elif board[x0, y0] != turn:\n",
    "        raise ValueError(\"The starting position is not your piece\")\n",
    "    else:\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "    \n",
    "def min_max(np.int8_t[:, :] board, int turn, int target_turn, int depth=3, int alpha=-100, int beta=100, is_max=True, is_root=True):\n",
    "    '''A recursive alpha beta pruning min_max function\n",
    "    return: board evaluation, chosen move\n",
    "    NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "    cdef int result\n",
    "    if is_root:\n",
    "        best_moves = []\n",
    "    else:\n",
    "        best_move = ((0, 0), (0, 0))\n",
    "    \n",
    "    if depth == 0 or not has_next_move(board, turn): # start to do pruning and selecting once the recursion reaches the end\n",
    "        result = evaluate(board, target_turn)\n",
    "        return result, None\n",
    "    else:\n",
    "        if is_max:\n",
    "            alpha = -100\n",
    "        else:\n",
    "            beta = 100\n",
    "\n",
    "        for move in next_move(board, turn):\n",
    "            result, _ = min_max(move_to(board, turn, move[0], move[1]), \\\n",
    "                                -turn, target_turn, depth-1, alpha, beta, not is_max, False)\n",
    "            # prun the searching tree or update alpha and beta respectively\n",
    "            if is_max:\n",
    "                if result >= beta:\n",
    "                    return 100, None\n",
    "                elif result > alpha:\n",
    "                    alpha = result\n",
    "                    if is_root:\n",
    "                        best_moves = [move]\n",
    "                    else:\n",
    "                        best_move = move\n",
    "                elif result == alpha and is_root:\n",
    "                    best_moves.append(move)\n",
    "            else:\n",
    "                if result <= alpha:\n",
    "                    return -100, None\n",
    "                elif result < beta:\n",
    "                    beta = result\n",
    "                    if is_root:\n",
    "                        best_moves = [move]\n",
    "                    else:\n",
    "                        best_move = move\n",
    "                elif result == beta and is_root:\n",
    "                    best_moves.append(move)\n",
    "        if is_max:\n",
    "            if is_root:\n",
    "                return alpha, choice(best_moves)\n",
    "            else:\n",
    "                return alpha, best_move\n",
    "        else:\n",
    "            if is_root:\n",
    "                return beta, choice(best_moves)\n",
    "            else:\n",
    "                return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the previous  100  rounds, greedy win ratio is:  0.64\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "D = 1\n",
    "greedy_win = 0\n",
    "for _ in range(N):\n",
    "    a = Ataxx()\n",
    "    turn = -1\n",
    "    greedy_turn = choice([-1, 1])\n",
    "    while True:\n",
    "        #a.plot()\n",
    "        if turn == greedy_turn:\n",
    "            best_move = a.get_greedy_move(turn)\n",
    "            #_, best_move = min_max(a.data, turn, turn, depth=D)\n",
    "        else:\n",
    "            _, best_move = min_max(a.data, turn, turn, depth=D)\n",
    "        a.move_to(turn, best_move[0], best_move[1])\n",
    "        turn = -turn\n",
    "        result = a.evaluate(greedy_turn, turn)\n",
    "        if result == 1:\n",
    "            greedy_win += 1\n",
    "            break\n",
    "        elif result == -1:\n",
    "            break\n",
    "print(\"In the previous \", N, \" rounds, greedy win ratio is: \", float(greedy_win) / float(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ideas\n",
    "1. The __life_span__, i.e. how many episode of games will be self_played until the algorithm update the target model, seems to be vital to the stability of training of the network. (considering performance critiria as the win ratio against simple greedy). As is predicted, __a larger life_span will result in more stabled training, but much slower performance improvement__.\n",
    "2. in the get_q function, I implemented different ways of returning q for each board. In the beginning of the training, when most of the Q output are random numbers, __setting manual_q a larger contribution to the final q is very helpful in training__. However, __in the latter stages of training, we should give manual_q a large weight in the final q only when we are very confident of the output of manual_q__, in this case, the confidence is considered to be related to the magnitude of manual_q, as well as init_q(the q output of the network).\n",
    "3. As is observed from the testing result, Q seems to perform worse than P most of the time. One possible explanation is that __Q only learns the most valuable board scenarios, while is bad at evaluating low-quality boards.__, while the during testing for Q, the output of __Q is directly used to evaluate all the next_steps, without the assistance of P__, which is responsible for filtering out useless moves. Under situations where bad board was never met and was given a rather large magnitude Q, the bad move would be selected instead. This can be backed up by the fact that, when BOTH p and q are used to do the testing, the accuracy is significantly raised__(from q 78, p 82 to both 87.2 percent)__. This result is a combination of the contribution of both p and q, but a higher q contribution is believed, as with the growth of rollout times(400 in this experiment), the contribution of q grows significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def for_node(c, parent_n_visit, n_visit, p, q):\n",
    "    return c * p * sqrt(parent_n_visit + 1) / (n_visit + 1) - q / (n_visit + 1) \n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def get_q(init_q, manual_q, mode):\n",
    "    '''Manual_q and init_q are both an estimation for the q value\n",
    "    It seems that considering init_q to be a rectification will not lead to good result'''\n",
    "    if mode == 0:\n",
    "        return manual_q\n",
    "    elif mode == 1:\n",
    "        return init_q\n",
    "    elif mode == 2:\n",
    "        return 0.75 * manual_q + 0.25 * init_q\n",
    "    elif mode == 3:\n",
    "        if abs(manual_q) >= 0.5:\n",
    "            return 0.5 * manual_q + 0.5 * init_q \n",
    "        elif abs(manual_q) >= 0.8:\n",
    "            return manual_q\n",
    "        elif abs(init_q) > 0.15:\n",
    "            return 0.2 * manual_q + 0.8 * init_q\n",
    "        elif abs(init_q) > 0.5:\n",
    "            return init_q\n",
    "        else:\n",
    "            return 0.4 * manual_q + 0.6 * init_q\n",
    "    elif mode == 4:\n",
    "        if abs(manual_q) >= 0.8:\n",
    "            return manual_q\n",
    "        elif abs(manual_q) >= 0.5:\n",
    "            return 0.5 * manual_q + 0.5 * init_q \n",
    "        else:\n",
    "            return init_q\n",
    "    else:\n",
    "        raise ValueError(\"Mode is not specified\")\n",
    "\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def recover_q(q, manual_q):\n",
    "    '''manual q is the initial guess for win ratio\n",
    "    while init q is the rectification'''\n",
    "    return q\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, parent, p=0.0):\n",
    "        self._parent = parent\n",
    "        self._children = {} # a dictionary of action:node\n",
    "        self._corr_dict = {} # a dictionary for duplicated moves\n",
    "        self._n_visit = 0\n",
    "        # from the parent perspective\n",
    "        self._q = 0.0\n",
    "        self._manual_q = -5 # manually deviced q\n",
    "        self._init_q = -5 # learnt q\n",
    "        self._p = p\n",
    "        self._action_mask = None\n",
    "        self._feature_map = None\n",
    "        self._board = None\n",
    "        self._is_expanded = False\n",
    "        self._prev_move = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        out = \"_n_visit: {}, _q: {}, _p: {}, _children: \\n{}\".format(\\\n",
    "                self._n_visit, self._q, self._p, self._children)\n",
    "        return out\n",
    "    \n",
    "    def get_start_q(self, mode=0):\n",
    "        ''' Different mode means different q\n",
    "        mode 0: pure manual Q\n",
    "        mode 1: pure policy Q\n",
    "        mode 2: hybrid Q\n",
    "        '''\n",
    "        assert self._init_q != -5 and self._manual_q != -5\n",
    "        assert self._q == 0\n",
    "        self._q = get_q(self._init_q, self._manual_q, mode)\n",
    "    \n",
    "    def access_children(self, move):\n",
    "        try:\n",
    "            return self._children[move]\n",
    "        except:\n",
    "            return self._children[self._corr_dict[move]]\n",
    "    \n",
    "    def children_generator(self):\n",
    "        for move in self._children:\n",
    "            yield (move, self._children[move])\n",
    "        for move in self._corr_dict:\n",
    "            yield (move, self._children[self._corr_dict[move]])\n",
    "    \n",
    "    def update_all(self, t_v):\n",
    "        node = self\n",
    "        while not node is None: \n",
    "            node._q += t_v\n",
    "            node._n_visit += 1\n",
    "            node = node._parent\n",
    "            t_v = -t_v\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_search_value(parent, node, c):\n",
    "        # return values \n",
    "        try:\n",
    "            value = for_node(c, parent._n_visit, node._n_visit, node._p, node._q)\n",
    "        except:\n",
    "            print(parent)\n",
    "            print(node)\n",
    "            raise\n",
    "        #print(value)\n",
    "        return value\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_frequency_value(node):\n",
    "        try:\n",
    "            return node._n_visit\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def select(self, c):\n",
    "        best_node = [0, 0]\n",
    "        best_node[0], best_node[1] = max(self._children.items(), key=lambda node: self.get_search_value(self, node[1], c))\n",
    "        return best_node\n",
    "        \n",
    "    def get_action_mask(self):\n",
    "        # only generate the action mask once\n",
    "        if not self._action_mask is None:\n",
    "            return self._action_mask\n",
    "        else:\n",
    "            raise ValueError(\"No action mask, request failure\")\n",
    "    \n",
    "    def get_action_frequency_map(self, temp=1):\n",
    "        global policy_dict\n",
    "        out = np.zeros(len(policy_dict))\n",
    "        # record all the n_visit of each node\n",
    "        nodes = self.children_generator()\n",
    "        for node in nodes:\n",
    "            out[policy_dict[node[0]]] = (float(node[1]._n_visit) / 100) ** (1/temp)\n",
    "        # normalize the array\n",
    "        out /= out.sum()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyValueNetwork():\n",
    "    def __init__(self, lr=None, is_load_model=False, is_load_pretrain_model=False, gpu=None, verbose=False):\n",
    "        if gpu is None:\n",
    "            self._sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=24))\n",
    "        else:\n",
    "            config = tf.ConfigProto(log_device_placement=True)\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self._sess = tf.Session(config=config)\n",
    "        K.set_session(self._sess)\n",
    "        \n",
    "        if gpu is None:\n",
    "            self._device = \"cpu\"\n",
    "        else:\n",
    "            self._device = \"gpu:\" + str(gpu)\n",
    "        \n",
    "        if is_load_model:\n",
    "            if not is_load_pretrain_model:\n",
    "                self._model = load_model('MCTS_POLICY_MODEL/AtaxxZero_model.h5')\n",
    "                self._target_model = load_model('MCTS_POLICY_MODEL/AtaxxZero_target_model.h5')\n",
    "                print(\"successfully loaded two models\")\n",
    "            else:\n",
    "                self._model = load_model('MCTS_POLICY_MODEL/pretrain_AtaxxZero_model.h5')\n",
    "                self._target_model = load_model('MCTS_POLICY_MODEL/pretrain_AtaxxZero_target_model.h5')\n",
    "                print(\"successfully loaded two pretrained models\")\n",
    "            if not lr is None:\n",
    "                self.update_learning_rate(lr)\n",
    "        else:\n",
    "            assert not lr is None\n",
    "            self._lr = lr\n",
    "            self._model = self.create_model()\n",
    "            self._target_model = self.create_model()\n",
    "            init = tf.global_variables_initializer()\n",
    "            self._sess.run(init)\n",
    "            print(\"new models generated\")\n",
    "            \n",
    "        # do not synchronize both models\n",
    "        # self.update_target_model(T=1)\n",
    "        # print the model structure\n",
    "        if verbose:\n",
    "            print(self._model.summary())\n",
    "        \n",
    "    def update_learning_rate(self, lr):\n",
    "        try:\n",
    "            print(\"learning rate updated from {} to {}\".format(self._lr, lr))\n",
    "        except:\n",
    "            print(\"compile new learning rate {}\".format(lr))\n",
    "        self._lr = lr\n",
    "        self._model.compile(loss=['categorical_crossentropy', 'mse'], optimizer=Adam(lr=self._lr, decay=1e-6),\\\n",
    "                     loss_weights=[1, 1])\n",
    "        \n",
    "    def create_model(self):\n",
    "        assert K.backend() == 'tensorflow'\n",
    "        \n",
    "        def res_block(res_in):\n",
    "            x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(res_in)\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = add(inputs=[x, res_in])\n",
    "            x = Activation('relu')(x)\n",
    "            return x\n",
    "\n",
    "        with tf.device(self._device):\n",
    "            board_input = Input((6, 9, 9))\n",
    "            mask_input = Input((792, ))\n",
    "            x = Conv2D(64, (3, 3), padding='valid', kernel_regularizer=l2(1e-4))(board_input)\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            for i in range(1):\n",
    "                x = res_block(x)\n",
    "            y = x\n",
    "\n",
    "            x = Conv2D(2, (1, 1), kernel_regularizer=l2(1e-4))(y) # as we have 792 policy compared to 360 policy in go\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(792, activation='softplus', kernel_regularizer=l2(1e-4))(x)\n",
    "            x = multiply(inputs=[x, mask_input])     # this mask will mask any illegal move\n",
    "            action_output = Activation('softmax')(x)\n",
    "\n",
    "            x = Conv2D(1, (1, 1), kernel_regularizer=l2(1e-4))(y)\n",
    "            x = BatchNormalization(axis=1)(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = Flatten()(x)\n",
    "            x = Dense(128, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "            value_output = Dense(1, activation='tanh')(x)\n",
    "\n",
    "            model = Model(input=[board_input, mask_input],output=[action_output, value_output])\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(loss=['categorical_crossentropy', 'mse'], optimizer=Adam(lr=self._lr, decay=1e-6),\\\n",
    "                         loss_weights=[1, 1])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def update_target_model(self, T=0.5):\n",
    "        \"\"\"Inheriting the DDPG concept of updating target weights,\n",
    "        Each time when target weights are updated, only update the target_weights by a small amount\n",
    "        This amount can be adjusted by parameter T\"\"\"\n",
    "\n",
    "        model_weights = self._model.get_weights()\n",
    "        target_weights = self._target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_weights[i] = T * model_weights[i] + (1-T) * target_weights[i]\n",
    "        self._target_model.set_weights(target_weights)\n",
    "        print(\"\\n\\ntarget model updated by T:\", T, \"\\n\\n\")\n",
    "\n",
    "    def predict(self, feature_map, action_mask, is_target=True):\n",
    "        if not is_target:\n",
    "            model = self._model\n",
    "        else:\n",
    "            model = self._target_model\n",
    "        \n",
    "        return self._sess.run(model.outputs, feed_dict={model.inputs[0]: feature_map.reshape(-1, 6, 9, 9), \\\n",
    "                                    model.inputs[1]: action_mask.reshape(-1, 792), K.learning_phase(): 0})\n",
    "    \n",
    "    def save(self, is_pretrain=False):\n",
    "        if not is_pretrain:\n",
    "            self._model.save('MCTS_POLICY_MODEL/AtaxxZero_model.h5')\n",
    "            self._target_model.save('MCTS_POLICY_MODEL/AtaxxZero_target_model.h5')\n",
    "        else:\n",
    "            self._model.save('MCTS_POLICY_MODEL/pretrain_AtaxxZero_model.h5')\n",
    "            self._target_model.save('MCTS_POLICY_MODEL/pretrain_AtaxxZero_target_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relay():\n",
    "    def __init__(self, life_span=80):\n",
    "        self._relay_dict = {}\n",
    "        self._counter = 0\n",
    "        self._life_span = int(life_span * 4)\n",
    "        \n",
    "    def append(self, train_data):\n",
    "        train_data = list((zip(*train_data)))\n",
    "        # stack the small batch\n",
    "        for i in range(4):\n",
    "            train_data[i] = np.stack(train_data[i], axis=0)\n",
    "        # append new data\n",
    "        self._relay_dict[self._counter] = train_data\n",
    "        self._counter += 1\n",
    "        \n",
    "        # remove too old data\n",
    "        remove_index = -1\n",
    "        for index in self._relay_dict:\n",
    "            if self._counter - index > self._life_span:\n",
    "                remove_index = index\n",
    "        if remove_index != -1:\n",
    "            del self._relay_dict[remove_index]\n",
    "                \n",
    "    def get(self, n_data=None):\n",
    "        # concatenate all data\n",
    "        all_data = [[], [], [], []]\n",
    "        for _, data in self._relay_dict.items():\n",
    "            for i in range(4):\n",
    "                all_data[i].append(data[i])\n",
    "        for i in range(4):\n",
    "            all_data[i] = np.concatenate(all_data[i], axis=0)\n",
    "        # sample n_data from all data\n",
    "        length = all_data[0].shape[0]\n",
    "        \n",
    "        # sampling n_data from all data\n",
    "        out = [[], [], [], []]\n",
    "        if n_data >= length:\n",
    "            out = all_data\n",
    "        else:\n",
    "            indexs = range(length)\n",
    "            out_indexs = np.random.choice(indexs, size=n_data, replace=False)\n",
    "            for i in out_indexs:\n",
    "                for j in range(4):\n",
    "                    out[j].append(all_data[j][i])\n",
    "            for i in range(4):\n",
    "                out[i] = np.stack(out[i], axis=0)\n",
    "        \n",
    "        print(length, \" data in database\")\n",
    "        print(n_data, \" data expected\")\n",
    "        print(out[0].shape[0], \" data grabbed\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    def __init__(self, c=1, dep_lim=10, lr=1e-4, life_span=10, is_load_model=False, is_load_pretrain_model=False, gpu=True):\n",
    "        # slow_step means how many step we use to do typical mcts, after that we do fast play\n",
    "        self._c = c\n",
    "        self._dep_lim = dep_lim\n",
    "        self._game = Ataxx()\n",
    "        self._turn = -1\n",
    "        # generate model\n",
    "        self._network = PolicyValueNetwork(lr, is_load_model, is_load_pretrain_model, gpu=gpu)\n",
    "        self._lr = lr\n",
    "        # determine which mode to use, default is 0, to switch mode, must do manually\n",
    "        self._mode = 0\n",
    "        # generate root and expand initially its children\n",
    "        self._root = TreeNode(None) # this one will move in self play mode\n",
    "        self.further_init(self._root, self._game, self._turn, get_p_array=True)\n",
    "        self._root_store = self._root # this is a backup for reset\n",
    "        \n",
    "        \n",
    "    def reset_root(self, move=None):\n",
    "        self._root = TreeNode(None) # this one will move in self play mode\n",
    "        self.further_init(self._root, self._game, self._turn, move, get_p_array=True)\n",
    "        self._root_store = self._root # this is a backup for reset\n",
    "        \n",
    "    def reset(self, left_space=45):\n",
    "        self._game.reset()\n",
    "        self._turn = -1\n",
    "        self._root = TreeNode(None)\n",
    "        \n",
    "        if left_space < 45:\n",
    "            steps = 0\n",
    "            is_terminal = False\n",
    "            result = 45\n",
    "            while not is_terminal and result > left_space:\n",
    "                if np.random.random() < 0.2: # 80 percent using greedy move\n",
    "                    best_move = choice(self._game.get_moves(self._turn))\n",
    "                    \n",
    "                else:\n",
    "                    best_move = self._game.get_greedy_move(self._turn)\n",
    "                    \n",
    "                self.make_a_move(best_move)\n",
    "                is_terminal = abs(self._game.evaluate(1, self._turn)) == 1\n",
    "                result = (np.array(self._game.data) == 0).sum()\n",
    "                steps += 1\n",
    "            if is_terminal:\n",
    "                print(\"reset failure, do reset again\")\n",
    "                self.reset(left_space)\n",
    "        self.reset_root()\n",
    "        try: # tell the _root which move led it here\n",
    "            self._root._move = best_move\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def plot_move_visit_freq(self):\n",
    "        nodes = sorted(self._root._children.items(), key=lambda node: self._root.get_frequency_value(node[1]), reverse=True)\n",
    "        p_sum = 0\n",
    "        for node in nodes:\n",
    "            try:\n",
    "                print(\"{}: n_v:{:>6d} q_all:{:+06.6f} q:{:+06.6f} q_m:{:+06.6f} p:{:06.6f}\"\\\n",
    "                      .format(node[0], node[1]._n_visit, -node[1]._q / (node[1]._n_visit + 1), \\\n",
    "                              -node[1]._init_q, -node[1]._manual_q, node[1]._p))\n",
    "                p_sum += node[1]._p\n",
    "            except:\n",
    "                pass\n",
    "        print(\"########################p_sum is: \", p_sum)\n",
    "                      \n",
    "    def get_next_move(self, is_best=False, is_dirichlet=True, rollout_times=100, t_lim=np.nan):\n",
    "        global policy_list\n",
    "        # do mcts\n",
    "        self.rollout(rollout_times, t_lim)\n",
    "        \n",
    "        if is_best:\n",
    "            # return the best move\n",
    "            index = np.argmax(self._root.get_action_frequency_map())\n",
    "        elif is_dirichlet:\n",
    "            # return a choiced move\n",
    "            prob = (0.75*self._root.get_action_frequency_map(temp=1e-2) \\\n",
    "                         + 0.25*np.random.dirichlet(0.3*np.ones(792))) * self._root._action_mask\n",
    "            index = np.random.choice(range(792), p=prob / prob.sum())\n",
    "        else:\n",
    "            # return move with prob equal to visit frequency\n",
    "            index = np.random.choice(range(792), p=self._root.get_action_frequency_map())\n",
    "            \n",
    "        if index != np.argmax(self._root.get_action_frequency_map()):\n",
    "            print(\"\\n\\nThis is a random move\\n\\n\")\n",
    "            \n",
    "        return policy_list[index]\n",
    "    \n",
    "    def make_a_move(self, next_move):\n",
    "        # move the root to next_move's root\n",
    "        if self._root._children == {}:\n",
    "            self._root = TreeNode(None)\n",
    "        elif type(self._root.access_children(next_move)) is np.float32: # the root may not be neccessarily expanded\n",
    "            self._root = TreeNode(None)\n",
    "        else:\n",
    "            self._root = self._root.access_children(next_move)\n",
    "            self._root._parent = None # necessary for updata_all\n",
    "        \n",
    "        # update the game board\n",
    "        self._game.move_to(self._turn, next_move[0], next_move[1])\n",
    "        self._turn = -self._turn\n",
    "      \n",
    "    def further_init(self, node, game, turn, prev_move=None, get_p_array=False):\n",
    "        global policy_dict, policy_list\n",
    "        node._prev_move = prev_move # tell which move led the node here\n",
    "        # preparing all children\n",
    "        new_moves, node._corr_dict, node._children, node._action_mask \\\n",
    "                                = game.get_moves(turn, return_node_info=True)\n",
    "        # if meet end of the game, generate manual q\n",
    "        if node._children == {}:\n",
    "            if node._manual_q == -5:\n",
    "                node._manual_q = game.get_manual_q(turn, game.data)\n",
    "            # quite tricky here dude, remember to look from the parent perspective\n",
    "            if node._manual_q > 0:\n",
    "                node._q = 1\n",
    "            else:\n",
    "                node._q = -1 \n",
    "            return\n",
    "        \n",
    "        # generate feature map\n",
    "        node._feature_map = game.get_feature_map(turn, prev_move)\n",
    "        node._board = game.data.copy()\n",
    "        # if required, generate p array and q, only if there are children\n",
    "        if get_p_array:\n",
    "            # generate policy prob array\n",
    "            out = self._network.predict(node._feature_map, node._action_mask)\n",
    "            p_array = out[0]\n",
    "            # give p to each child (float32)\n",
    "            for move in new_moves:\n",
    "                node._children[move] = p_array[0][policy_dict[move]]\n",
    "            # init node._q\n",
    "            node._init_q = out[1][0][0] \n",
    "            node._manual_q = game.get_manual_q(turn, game.data)\n",
    "            node.get_start_q(self._mode)\n",
    "        \n",
    "    def expand(self, node, game, turn):\n",
    "        global policy_dict, policy_list\n",
    "        \n",
    "        # if the node was not expanded, take that as a new root and further init it\n",
    "        if node._children == {} and node._q == 0:\n",
    "            self.further_init(node, game, turn, get_p_array=True)\n",
    "        # if end of game, quit expanding\n",
    "        if node._children == {}:\n",
    "            assert node._q != 0\n",
    "            return\n",
    "        \n",
    "        # update expanded state\n",
    "        node._is_expanded = True\n",
    "        \n",
    "        # if there are children\n",
    "        backup_board = game.data.copy() # warning, to backup a memoryview ndarray, use copy()\n",
    "        index_list = []\n",
    "        feature_map = []\n",
    "        action_mask = []\n",
    "        boards = []\n",
    "        for move in node._children:\n",
    "            tmp = node._children[move]\n",
    "            try:\n",
    "                assert type(tmp) is np.float32\n",
    "            except:\n",
    "                print(type(tmp))\n",
    "                raise\n",
    "            new_node = TreeNode(node, p=node._children[move])\n",
    "            game.move_to(turn,  move[0], move[1])\n",
    "            self.further_init(new_node, game, -turn, move, get_p_array=False)\n",
    "            node._children[move] = new_node\n",
    "            # prepare to calculate p for new_node only if it has children\n",
    "            if new_node._children != {}:\n",
    "                index_list.append(new_node)\n",
    "                feature_map.append(new_node._feature_map)\n",
    "                action_mask.append(new_node._action_mask)\n",
    "                boards.append(new_node._board)\n",
    "            # reset the gamer\n",
    "            game.reset(board=backup_board)\n",
    "        # if there are no more node that is expandable, quit\n",
    "        if len(index_list) == 0:\n",
    "            return\n",
    "        \n",
    "        # do batch prediction\n",
    "        # print(\"batch size:\", len(index_list))\n",
    "        feature_map = np.stack(feature_map, axis=0)\n",
    "        action_mask = np.stack(action_mask, axis=0)\n",
    "        out = self._network.predict(feature_map, action_mask)\n",
    "        # get batch manual q \n",
    "        boards = [game.get_manual_q(-turn, board) for board in boards]\n",
    "        # update the result to each child node\n",
    "        for i, child in enumerate(index_list):\n",
    "            # assign q\n",
    "            child._manual_q = boards[i] # neg for display use\n",
    "            child._init_q = out[1][i][0] # same as above\n",
    "            child.get_start_q(self._mode)\n",
    "            # assign p\n",
    "            assign_children(child._children, out[0][i])\n",
    "            \n",
    "\n",
    "    def rollout(self, rollout_times=100, t_lim=np.nan, t_min=2):\n",
    "        start = time.time()\n",
    "        for i in range(int(rollout_times*1.1)): \n",
    "            tmp_node = self._root\n",
    "            tmp_game = Ataxx(self._game.data)\n",
    "            tmp_turn = self._turn\n",
    "            # start mcts\n",
    "            step = 0\n",
    "            while True:\n",
    "                assert self._dep_lim > 0\n",
    "                if step < self._dep_lim:\n",
    "                    # expand the node only when it has never been expanded\n",
    "                    if tmp_node._is_expanded == False:\n",
    "                        self.expand(tmp_node, tmp_game, tmp_turn)\n",
    "\n",
    "                    # check if is leaf node, if so, update the whole tree\n",
    "                    if tmp_node._children == {}:\n",
    "                        t_v = tmp_node._q / (tmp_node._n_visit + 1)\n",
    "                        tmp_node.update_all(t_v)\n",
    "                        break\n",
    "                    else:\n",
    "                        # select a child and continue exploration\n",
    "                        next_move, next_node = tmp_node.select(self._c)\n",
    "                            \n",
    "                        # move to next move and next node\n",
    "                        tmp_game.move_to(tmp_turn, next_move[0], next_move[1])    \n",
    "                        tmp_node = next_node\n",
    "                        tmp_turn = -tmp_turn\n",
    "                else:\n",
    "                    t_v = tmp_node._q / (tmp_node._n_visit + 1)\n",
    "                    tmp_node.update_all(t_v)\n",
    "                    break\n",
    "                # update steps                                    \n",
    "                step += 1\n",
    "            cur_time = time.time() - start\n",
    "            if cur_time > t_lim * 0.999:\n",
    "                print(\"due to time lim, final rollout times: \", i, \"time elapsed: \", cur_time)\n",
    "                break\n",
    "            \n",
    "            if cur_time > t_min and i > rollout_times:\n",
    "                print(\"due to rollout lim, final rollout times: \", i, \"time elapsed: \", cur_time)\n",
    "                break\n",
    "                \n",
    "        \n",
    "    def testing_against_min_max(self, rounds=5, left_space=45, mm_dep=1, c=5, dep_lim=0, rollout_times=400, t_lim=6, verbose=True):\n",
    "        print(\"####               ####\")\n",
    "        print(\"#### start testing ####\")\n",
    "        test_start = time.time()\n",
    "        # record dep_lim and c for restoration\n",
    "        store_dep_lim = self._dep_lim\n",
    "        self._dep_lim = dep_lim\n",
    "        store_c = self._c\n",
    "        self._c = c\n",
    "        # recorder of game result\n",
    "        n_win = 0.0\n",
    "        win_steps = 0.0\n",
    "        lose_steps = 0.0\n",
    "        for r in range(rounds):\n",
    "            tmp_round_s = time.time()\n",
    "            # randomly init the game board if no left_space specified\n",
    "            self.reset(left_space)\n",
    "            # set up start turns\n",
    "            my_turn = choice([-1, 1])\n",
    "            if verbose:\n",
    "                print(\"round:\", r+1)\n",
    "                print(\"this game start with {} space left\".format(left_space))\n",
    "                print(\"self takes turn: \", my_turn)\n",
    "            # start the game\n",
    "            steps = 0\n",
    "            while abs(self._game.evaluate(1, self._turn)) != 1:\n",
    "                # plot the game board\n",
    "                if verbose:\n",
    "                    self._game.plot()\n",
    "                    tmp_s = time.time()\n",
    "                if self._turn == my_turn:\n",
    "                    best_move = self.get_next_move(is_best=True, rollout_times=rollout_times, t_lim=t_lim)\n",
    "                    if verbose:\n",
    "                        print(\"self turn\", my_turn)\n",
    "                        print(self.plot_move_visit_freq())\n",
    "                else:\n",
    "                    _, best_move = min_max(self._game.data, self._turn, self._turn, mm_dep)\n",
    "                    if verbose:\n",
    "                        print(\"greedy turn\", self._turn)\n",
    "                if verbose:\n",
    "                    print(\"this move takes time(s): \", time.time()-tmp_s)\n",
    "                    print(\"chosen move is \", best_move)\n",
    "\n",
    "                # synchronize steps and boards\n",
    "                self.make_a_move(best_move)\n",
    "                # update steps\n",
    "                steps += 1\n",
    "                if steps > 300:\n",
    "                    print(\"steps over 250, game skip\")\n",
    "                    break\n",
    "            if steps <= 300:\n",
    "                is_self_win = self._game.evaluate(my_turn, self._turn) == 1\n",
    "                if is_self_win:\n",
    "                    n_win += 1\n",
    "                    win_steps += steps\n",
    "                else:\n",
    "                    lose_steps += steps\n",
    "                if verbose:\n",
    "                    print(\"this round has steps: {}, time taken: {}, \\n\\n\\nself wins? {}\\n\\n\\n\".format(steps, time.time()-tmp_round_s, is_self_win))\n",
    "            else:\n",
    "                n_win += 0.5\n",
    "        # restore dep lim and c\n",
    "        self._dep_lim = store_dep_lim\n",
    "        self._c = store_c\n",
    "        \n",
    "        # output\n",
    "        print(\"testing took time: \", time.time()-test_start)\n",
    "        print(\"win steps: \", win_steps / (n_win + 1e-5), \"lose steps: \", lose_steps / (rounds - n_win + 1e-5))\n",
    "        print()\n",
    "        if n_win == 0:\n",
    "            return 0\n",
    "        return n_win / rounds\n",
    "    \n",
    "    def tester(self, mm_dep=1, Q=False, P=False, BOTH=False, mode=0, times=200, dep_lim=1, rollout_times=400, verbose=False):\n",
    "        out = {}\n",
    "        mode_store = self._mode\n",
    "        self._mode = mode\n",
    "        print(\"mm_dep is: \", mm_dep)\n",
    "        if Q:\n",
    "            q_ratio = self.testing_against_min_max(\\\n",
    "                rounds=times, left_space=45, mm_dep=mm_dep, c=0, dep_lim=dep_lim, rollout_times=1, t_lim=6, verbose=verbose)\n",
    "            print(\"\\n\\n\\n                        win ratio of Q is {} \\n\\n\\n\\n\\n\".format(q_ratio))\n",
    "            out['Q'] = q_ratio\n",
    "        if P:\n",
    "            p_ratio = self.testing_against_min_max(\\\n",
    "                rounds=times, left_space=45, mm_dep=mm_dep, c=1000000, dep_lim=dep_lim, rollout_times=1, t_lim=6, verbose=verbose)\n",
    "            print(\"\\n\\n\\n                        win ratio of P is {} \\n\\n\\n\\n\\n\".format(p_ratio))\n",
    "            out['P'] = p_ratio\n",
    "        if BOTH: # multiple customizations for this one\n",
    "            both_ratio = self.testing_against_min_max(\\\n",
    "                rounds=int(times), left_space=45, mm_dep=mm_dep, c=self._c, dep_lim=dep_lim, rollout_times=rollout_times, t_lim=6, verbose=verbose)\n",
    "            print(\"\\n\\n\\n                        win ratio of both is {} \\n\\n\\n\\n\\n\".format(both_ratio))\n",
    "            out['BOTH'] = both_ratio\n",
    "        self._mode = mode_store\n",
    "        return out\n",
    "             \n",
    "    def data_collector(self, node, visit_min, train_mode=0):\n",
    "        out = []\n",
    "        if node._n_visit >= visit_min:\n",
    "            if train_mode == 0: # train both p and q\n",
    "                tmp_data = [node._feature_map, \\\n",
    "                            node._action_mask, \\\n",
    "                            node.get_action_frequency_map(),\\\n",
    "                            recover_q(node._q / (node._n_visit + 1), node._manual_q)] # to recover what policy q should be\n",
    "            elif train_mode == 1: # train only q\n",
    "                print(\"Notice: train_mode is q train\")\n",
    "                tmp_data = [node._feature_map, \\\n",
    "                            np.zeros(792), \\\n",
    "                            np.zeros(792),\\\n",
    "                            recover_q(node._q / (node._n_visit + 1), node._manual_q)] # to recover what policy q should be\n",
    "            else:\n",
    "                raise ValueError(\"train_mode not understood\")\n",
    "            tmp_data = augment_data(tmp_data)\n",
    "            out.extend(tmp_data)\n",
    "\n",
    "        # This type of data augumentation reduce increase the entropy of P (low quality, less peaky P distribution)\n",
    "        # however, it improved Q somehow, which is currently 20%(60%) better than P policy when playing against greedy\n",
    "        # considering preserving only the data augmentation of Q (by setting action_mask to 0)\n",
    "        for _, child in node._children.items():\n",
    "            try: \n",
    "                assert child._n_visit >= visit_min * 0.625\n",
    "                tmp_data = [child._feature_map, \\\n",
    "                            np.zeros(792), \\\n",
    "                            np.zeros(792), \\\n",
    "                            recover_q(child._q / (child._n_visit + 1), child._manual_q)]\n",
    "                tmp_data = augment_data(tmp_data)\n",
    "                out.extend(tmp_data)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"no. of data collected: \", len(out))\n",
    "        return out\n",
    "        \n",
    "                \n",
    "    def self_play(self, rollout_times=100, t_lim=np.nan, verbose=True, is_best=False, train_mode=0):\n",
    "        train_data = []\n",
    "        steps = 0\n",
    "        print(\"start new self play\")\n",
    "        start = time.time()\n",
    "        while self._root._children != {} and steps < 250:\n",
    "            print(self._turn, \"'s turn, step no. \", steps)\n",
    "            tmp_s = time.time()\n",
    "            if steps < 8: # to generate different game data\n",
    "                best_move = self.get_next_move(is_best=is_best, is_dirichlet=False, \\\n",
    "                                               rollout_times=rollout_times, t_lim=t_lim)\n",
    "            else:\n",
    "                best_move = self.get_next_move(is_best=is_best, is_dirichlet=True, \\\n",
    "                                               rollout_times=rollout_times, t_lim=t_lim)\n",
    "            print(\"one move takes time(s): \", time.time()-tmp_s)\n",
    "            # record π data\n",
    "            visit_min = rollout_times\n",
    "            train_data.extend(self.data_collector(self._root, visit_min, train_mode=train_mode))\n",
    "            # plot the game board if verbose\n",
    "            if verbose:\n",
    "                self._game.plot()\n",
    "                self.plot_move_visit_freq()\n",
    "            # make the move and move on\n",
    "            self.make_a_move(best_move)\n",
    "            steps += 1\n",
    "        print(\"this self play has {} steps, time elapsed {}\".format(steps, time.time()-start))\n",
    "        print(\"winner is\", np.sign(self._root._q * self._turn))\n",
    "        \n",
    "        return train_data\n",
    "    \n",
    "    def zero_out_pretraining(self): # zero out q output\n",
    "        train_data = []\n",
    "        # collect train data\n",
    "        while len(train_data) < 500000:\n",
    "            tmp_game = Ataxx()\n",
    "            tmp_round_s = time.time()\n",
    "            # randomly init the game board if no left_space specified\n",
    "            # start the game\n",
    "            steps = 0\n",
    "            turn = -1\n",
    "            while abs(tmp_game.evaluate(1, turn)) != 1:\n",
    "                # plot the game board\n",
    "                if steps < 10:\n",
    "                    rand_thresh = 0.5\n",
    "                else:\n",
    "                    rand_thresh = 0.25\n",
    "                if np.random.random() > rand_thresh:\n",
    "                    best_move = tmp_game.get_greedy_move(turn)\n",
    "                else:\n",
    "                    best_move = choice(tmp_game.get_moves(turn))\n",
    "                \n",
    "                # make the move and grab data\n",
    "                tmp_game.move_to(turn, best_move[0], best_move[1])\n",
    "                turn = -turn\n",
    "                tmp_data = [tmp_game.get_feature_map(turn, best_move), \\\n",
    "                            np.zeros(792), \\\n",
    "                            np.zeros(792),\\\n",
    "                            0]\n",
    "                tmp_data = augment_data(tmp_data)\n",
    "                train_data.extend(tmp_data)\n",
    "                # update steps\n",
    "                steps += 1\n",
    "            print(len(train_data))\n",
    "                \n",
    "        # process train data\n",
    "        train_data = list((zip(*train_data)))\n",
    "        for i in range(4):\n",
    "            train_data[i] = np.stack(train_data[i], axis=0)\n",
    "        # do the training\n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=1, mode='auto')\n",
    "        self._network._model.fit(x=[train_data[0], train_data[1]], y=[train_data[2], train_data[3]], verbose=1, \\\n",
    "                                    batch_size=1024, epochs=1024, shuffle=True, validation_split=0.1, callbacks=[es])\n",
    "        # update the model every life_span\n",
    "        self._network.update_target_model()\n",
    "        self._network.save(is_pretrain=True)\n",
    "    \n",
    "    def reinforcement_pretraining(self, rounds=1000, verbose=True): # geting Q value close to manual Q\n",
    "        train_data = []\n",
    "        # collect train data\n",
    "        for r in range(rounds):\n",
    "            tmp_game = Ataxx()\n",
    "            tmp_round_s = time.time()\n",
    "            # randomly init the game board if no left_space specified\n",
    "            if verbose:\n",
    "                print(\"round:\", r+1)\n",
    "            # start the game\n",
    "            steps = 0\n",
    "            turn = -1\n",
    "            while abs(tmp_game.evaluate(1, turn)) != 1:\n",
    "                # plot the game board\n",
    "                if verbose:\n",
    "                    tmp_s = time.time()\n",
    "                if steps < 8:\n",
    "                    rand_thresh = 0.5\n",
    "                else:\n",
    "                    rand_thresh = 0.2\n",
    "                if np.random.random() > rand_thresh:\n",
    "                    best_move = tmp_game.get_greedy_move(turn)\n",
    "                else:\n",
    "                    best_move = choice(tmp_game.get_moves(turn))\n",
    "                \n",
    "                # make the move and grab data\n",
    "                tmp_game.move_to(turn, best_move[0], best_move[1])\n",
    "                turn = -turn\n",
    "                tmp_data = [tmp_game.get_feature_map(turn, best_move), \\\n",
    "                            np.zeros(792), \\\n",
    "                            np.zeros(792),\\\n",
    "                            tmp_game.get_manual_q(turn, tmp_game.data)]\n",
    "                tmp_data = augment_data(tmp_data)\n",
    "                train_data.extend(tmp_data)\n",
    "                # update steps\n",
    "                steps += 1\n",
    "            if verbose:\n",
    "                print(\"this round has {} steps, takes time {}\".format(steps, time.time()-tmp_round_s))\n",
    "            \n",
    "            if len(train_data) > 500000:\n",
    "                # process train data\n",
    "                train_data = list((zip(*train_data)))\n",
    "                for i in range(4):\n",
    "                    train_data[i] = np.stack(train_data[i], axis=0)\n",
    "                # do the training\n",
    "                es = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "                self._network._model.fit(x=[train_data[0], train_data[1]], y=[train_data[2], train_data[3]], verbose=1, \\\n",
    "                                            batch_size=1024, epochs=512, shuffle=True, validation_split=0.1, callbacks=[es])\n",
    "                # update the model every life_span\n",
    "                self._network.update_target_model()\n",
    "                self._network.save(is_pretrain=True)\n",
    "                # clear train_data\n",
    "                train_data = []\n",
    "                \n",
    "        if len(train_data) <= 500000:\n",
    "            # process train data\n",
    "            train_data = list((zip(*train_data)))\n",
    "            for i in range(4):\n",
    "                train_data[i] = np.stack(train_data[i], axis=0)\n",
    "            # do the training\n",
    "            es = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "            self._network._model.fit(x=[train_data[0], train_data[1]], y=[train_data[2], train_data[3]], verbose=1, \\\n",
    "                                        batch_size=1024, epochs=1024, shuffle=True, validation_split=0.1, callbacks=[es])\n",
    "            # update the model every life_span\n",
    "            self._network.update_target_model()\n",
    "            self._network.save(is_pretrain=True)\n",
    "            \n",
    "    def reinforcement_learning(self, episode=1000, rollout_times=100, life_span=25, train_interval=5, T=0.5, t_lim=np.nan, left_space_max=None, self_play_verbose=False, train_mode=0):        \n",
    "        # setting testing parameters\n",
    "        dep_q = 1\n",
    "        dep_p = 3\n",
    "        test_rounds = [250, 200, 100, 30]\n",
    "        # setting up train relay\n",
    "        train_relay = Relay(life_span)\n",
    "        for epi in range(episode):\n",
    "            print(\"episode {} now start\".format(epi))\n",
    "            self.reset_root()\n",
    "            # randomly skip a few steps if left space not specified\n",
    "            if not left_space_max is None:\n",
    "                left_space = left_space_max * np.random.normal(loc=1, scale=0.2)\n",
    "            else:\n",
    "                left_space = 45\n",
    "            self.reset(left_space=left_space)\n",
    "            print(\"left space is {}\".format(left_space))\n",
    "            # start self_play and get train_data\n",
    "            if np.random.random() < 0.05:\n",
    "                train_data = self.self_play(rollout_times=rollout_times, t_lim=t_lim, verbose=True, train_mode=train_mode)\n",
    "            else:\n",
    "                train_data = self.self_play(rollout_times=rollout_times, t_lim=t_lim, verbose=self_play_verbose, train_mode=train_mode)\n",
    "            # store it in relay\n",
    "            train_relay.append(train_data)\n",
    "            # do training every 5 epi and update target model and reset tree\n",
    "            if epi >= life_span and epi%train_interval == 0:\n",
    "                train_data = train_relay.get(1800*life_span)\n",
    "                print(\"start training, training data no. {}\".format(train_data[0].shape[0]))\n",
    "                # do training\n",
    "                es = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=64, verbose=1, mode='auto')\n",
    "                self._network._model.fit(x=[train_data[0], train_data[1]], y=[train_data[2], train_data[3]], verbose=1, \\\n",
    "                                            batch_size=1024, epochs=512, shuffle=True, validation_split=0.15, callbacks=[es])\n",
    "                print(\"saving files\")\n",
    "                self._network.save()\n",
    "                # update the model every life_span, but dont update for the first life_span\n",
    "                if epi%life_span == 0 and epi >= 2*life_span:\n",
    "                    self._network.update_target_model(T=T)\n",
    "            # test the performance of the AI\n",
    "            if epi % (life_span) == 0:\n",
    "                \"\"\"Try to test so that a reasonable result is given\"\"\"\n",
    "                print(\"\\n\\nstart testing against min max\")\n",
    "                if dep_q != 1:\n",
    "                    self.tester(mm_dep=1, Q=True, mode=1, times=250)\n",
    "                if dep_p != 1:\n",
    "                    self.tester(mm_dep=1, P=True, mode=1, times=250)\n",
    "                q_result = self.tester(mm_dep=dep_q, Q=True, mode=1, times=test_rounds[dep_q-1])['Q']\n",
    "                p_result = self.tester(mm_dep=dep_p, P=True, mode=1, times=test_rounds[dep_p-1])['P']\n",
    "                if q_result < 0.20:\n",
    "                    if dep_q > 1:\n",
    "                        dep_q -= 1\n",
    "                elif q_result > 0.55:\n",
    "                    if dep_q < 4:\n",
    "                        self.tester(mm_dep=dep_q+1, Q=True, mode=1, times=test_rounds[dep_q])\n",
    "                        if q_result > 0.80:\n",
    "                            dep_q += 1\n",
    "                if p_result < 0.20:\n",
    "                    if dep_p > 1:\n",
    "                        dep_p -= 1\n",
    "                elif p_result > 0.55:\n",
    "                    if dep_p < 4:\n",
    "                        self.tester(mm_dep=dep_p+1, P=True, mode=1, times=test_rounds[dep_p])\n",
    "                        if p_result > 0.80:\n",
    "                            dep_p += 1\n",
    "            # a flawed way of annealing lr\n",
    "            if epi % (life_span) == life_span - 1 and self._lr >= 5e-6:\n",
    "                self._lr = self._lr / 1.5\n",
    "                self._network.update_learning_rate(self._lr)\n",
    "            print(\"episode {} finished\".format(epi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yuze/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yuze/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yuze/anaconda3/envs/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "successfully loaded two models\n",
      "compile new learning rate 1e-06\n"
     ]
    }
   ],
   "source": [
    "player = MCTS(gpu=1, lr=1e-6, c=3, is_load_model=True, is_load_pretrain_model=False, dep_lim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 now start\n",
      "left space is 45\n",
      "start new self play\n",
      "-1 's turn, step no.  0\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.3195915222167969\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  1\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.3857433795928955\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  2\n",
      "one move takes time(s):  0.8061413764953613\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  3\n",
      "one move takes time(s):  0.9494669437408447\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  4\n",
      "due to rollout lim, final rollout times:  2186 time elapsed:  2.0000181198120117\n",
      "one move takes time(s):  2.001197099685669\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  5\n",
      "one move takes time(s):  1.5986297130584717\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  6\n",
      "one move takes time(s):  0.40499448776245117\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  7\n",
      "one move takes time(s):  0.7652602195739746\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  8\n",
      "one move takes time(s):  1.6035702228546143\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  9\n",
      "one move takes time(s):  1.1553797721862793\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  10\n",
      "one move takes time(s):  0.6733031272888184\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  11\n",
      "one move takes time(s):  1.746694564819336\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  12\n",
      "one move takes time(s):  0.908820390701294\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  13\n",
      "one move takes time(s):  0.6365740299224854\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  14\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.42978668212890625\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  15\n",
      "one move takes time(s):  0.8317687511444092\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  16\n",
      "one move takes time(s):  0.7470242977142334\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  17\n",
      "one move takes time(s):  0.9049067497253418\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  18\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.4375531673431396\n",
      "one move takes time(s):  2.4388949871063232\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  19\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.164316177368164\n",
      "one move takes time(s):  3.1663119792938232\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  20\n",
      "one move takes time(s):  1.6331977844238281\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  21\n",
      "one move takes time(s):  1.0110077857971191\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  22\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.860680103302002\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  23\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.0042245388031006\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.006054639816284\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  24\n",
      "one move takes time(s):  1.6575698852539062\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  25\n",
      "one move takes time(s):  1.8924336433410645\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  26\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.3246827125549316\n",
      "one move takes time(s):  2.3260719776153564\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  27\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.346254587173462\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.3474690914154053\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  28\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.4476497173309326\n",
      "one move takes time(s):  3.4504685401916504\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  29\n",
      "one move takes time(s):  1.1832239627838135\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  30\n",
      "one move takes time(s):  1.9933583736419678\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  31\n",
      "one move takes time(s):  1.6521317958831787\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  32\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.458853006362915\n",
      "one move takes time(s):  2.460319995880127\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  33\n",
      "one move takes time(s):  1.5910706520080566\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  34\n",
      "one move takes time(s):  0.9708590507507324\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  35\n",
      "one move takes time(s):  1.5421018600463867\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  36\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.681649684906006\n",
      "one move takes time(s):  3.6832058429718018\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  37\n",
      "one move takes time(s):  1.8759021759033203\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  38\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.739687204360962\n",
      "one move takes time(s):  3.7417449951171875\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  39\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  4.399196624755859\n",
      "one move takes time(s):  4.401574373245239\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  40\n",
      "one move takes time(s):  1.4614732265472412\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  41\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.401061773300171\n",
      "one move takes time(s):  2.402538776397705\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  42\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.214540243148804\n",
      "one move takes time(s):  5.216663122177124\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  43\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  4.209270477294922\n",
      "one move takes time(s):  4.211544513702393\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  44\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.677161693572998\n",
      "one move takes time(s):  5.678745746612549\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  45\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.811676502227783\n",
      "one move takes time(s):  2.812852382659912\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  46\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  4.454037666320801\n",
      "one move takes time(s):  4.455188989639282\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  47\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.383080244064331\n",
      "one move takes time(s):  3.3851478099823\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  48\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.09175181388855\n",
      "one move takes time(s):  3.0944864749908447\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  49\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  4.4076759815216064\n",
      "one move takes time(s):  4.411737442016602\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  50\n",
      "one move takes time(s):  1.4834020137786865\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  51\n",
      "one move takes time(s):  1.689708948135376\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  52\n",
      "one move takes time(s):  1.633415937423706\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  53\n",
      "one move takes time(s):  1.2028419971466064\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  54\n",
      "one move takes time(s):  1.607856035232544\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  55\n",
      "one move takes time(s):  1.4055562019348145\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  56\n",
      "one move takes time(s):  1.5208351612091064\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  57\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.639073133468628\n",
      "one move takes time(s):  5.641367673873901\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  58\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.8680434226989746\n",
      "one move takes time(s):  3.8697640895843506\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  59\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.217949390411377\n",
      "one move takes time(s):  3.2192888259887695\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  60\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.190337657928467\n",
      "one move takes time(s):  5.192015171051025\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  61\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.9348292350769043\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  62\n",
      "one move takes time(s):  1.9498872756958008\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  63\n",
      "one move takes time(s):  0.8849914073944092\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one move takes time(s):  0.8544659614562988\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  65\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.5916268825531006\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  66\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.8661670684814453\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  67\n",
      "one move takes time(s):  1.4016268253326416\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  68\n",
      "one move takes time(s):  0.5149927139282227\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  69\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.013920783996582\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  70\n",
      "one move takes time(s):  0.5887372493743896\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  71\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.7829866409301758\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  72\n",
      "one move takes time(s):  0.5552680492401123\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  73\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.47490477561950684\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  74\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.46875882148742676\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  75\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.23442649841308594\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  76\n",
      "one move takes time(s):  0.32625722885131836\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  77\n",
      "one move takes time(s):  0.1936781406402588\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  78\n",
      "one move takes time(s):  0.3958899974822998\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  79\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.23282694816589355\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  80\n",
      "one move takes time(s):  0.32373547554016113\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  81\n",
      "one move takes time(s):  0.29799866676330566\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  82\n",
      "due to rollout lim, final rollout times:  2022 time elapsed:  3.7897326946258545\n",
      "one move takes time(s):  3.7908782958984375\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  83\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.16408705711364746\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  84\n",
      "one move takes time(s):  0.11688375473022461\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  85\n",
      "one move takes time(s):  0.11447620391845703\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  86\n",
      "one move takes time(s):  0.14844202995300293\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  87\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.06560850143432617\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  88\n",
      "one move takes time(s):  0.04883122444152832\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  89\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.0462956428527832\n",
      "no. of data collected:  8\n",
      "this self play has 90 steps, time elapsed 150.24967670440674\n",
      "winner is 1.0\n",
      "\n",
      "\n",
      "start testing against min max\n",
      "mm_dep is:  1\n",
      "####               ####\n",
      "#### start testing ####\n",
      "due to time lim, final rollout times:  0 time elapsed:  14.670323848724365\n",
      "testing took time:  136.0178985595703\n",
      "win steps:  57.035085217759416 lose steps:  73.1363303925771\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of P is 0.912 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm_dep is:  1\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  95.10790348052979\n",
      "win steps:  55.489358750566026 lose steps:  25.29031850156153\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of Q is 0.752 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm_dep is:  3\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  126.64497399330139\n",
      "win steps:  69.9999862745125 lose steps:  94.99998061224885\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of P is 0.51 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm_dep is:  2\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  144.66975331306458\n",
      "win steps:  0.0 lose steps:  63.864996806750156\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of Q is 0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "episode 0 finished\n",
      "episode 1 now start\n",
      "left space is 45\n",
      "start new self play\n",
      "-1 's turn, step no.  0\n",
      "one move takes time(s):  0.27872228622436523\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  1\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.37721896171569824\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  2\n",
      "one move takes time(s):  0.6616039276123047\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  3\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.020277976989746\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  4\n",
      "due to rollout lim, final rollout times:  2064 time elapsed:  2.0147745609283447\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.0157833099365234\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  5\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  6.0862717628479\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  6.088106155395508\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  6\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.4602222442626953\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.461642265319824\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  7\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.7608304023742676\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.7619080543518066\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  8\n",
      "one move takes time(s):  1.2804265022277832\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  9\n",
      "one move takes time(s):  0.9583683013916016\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  10\n",
      "one move takes time(s):  1.6973049640655518\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  11\n",
      "one move takes time(s):  0.9785716533660889\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  12\n",
      "one move takes time(s):  1.0104115009307861\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  13\n",
      "one move takes time(s):  1.7216026782989502\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  14\n",
      "one move takes time(s):  1.6635208129882812\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  15\n",
      "one move takes time(s):  1.4355576038360596\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  16\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.192035675048828\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.193546772003174\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  17\n",
      "one move takes time(s):  0.7254254817962646\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  18\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.930724859237671\n",
      "one move takes time(s):  3.9320688247680664\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  19\n",
      "one move takes time(s):  1.5845270156860352\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  20\n",
      "one move takes time(s):  1.8284940719604492\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  21\n",
      "due to rollout lim, final rollout times:  2152 time elapsed:  2.0018601417541504\n",
      "one move takes time(s):  2.0032739639282227\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  22\n",
      "one move takes time(s):  1.276228666305542\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  23\n",
      "one move takes time(s):  1.8663654327392578\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  24\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.5323429107666016\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  3.53458309173584\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  25\n",
      "one move takes time(s):  1.3053905963897705\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  26\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.0601391792297363\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.0616869926452637\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  27\n",
      "one move takes time(s):  1.4718143939971924\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  28\n",
      "one move takes time(s):  1.524219274520874\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  29\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.9254109859466553\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  30\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.1162126064300537\n",
      "one move takes time(s):  2.1177117824554443\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  31\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.067530632019043\n",
      "one move takes time(s):  5.069248914718628\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  32\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.758343458175659\n",
      "one move takes time(s):  2.760759115219116\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.050743341445923\n",
      "one move takes time(s):  2.0524210929870605\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  34\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.461106538772583\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  35\n",
      "one move takes time(s):  1.3778965473175049\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  36\n",
      "one move takes time(s):  1.1112861633300781\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  37\n",
      "one move takes time(s):  1.5525250434875488\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  38\n",
      "one move takes time(s):  1.8264341354370117\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  39\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.9365200996398926\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  40\n",
      "one move takes time(s):  1.1772568225860596\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  41\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.9280910491943359\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  42\n",
      "one move takes time(s):  1.2547059059143066\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  43\n",
      "one move takes time(s):  0.48544883728027344\n",
      "no. of data collected:  32\n",
      "-1 's turn, step no.  44\n",
      "one move takes time(s):  0.9103412628173828\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  45\n",
      "one move takes time(s):  0.7395350933074951\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  46\n",
      "one move takes time(s):  0.5651462078094482\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  47\n",
      "one move takes time(s):  0.6366350650787354\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  48\n",
      "one move takes time(s):  1.0490086078643799\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  49\n",
      "one move takes time(s):  0.7614705562591553\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  50\n",
      "one move takes time(s):  1.0198688507080078\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  51\n",
      "one move takes time(s):  0.8130640983581543\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  52\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.5110738277435303\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  53\n",
      "one move takes time(s):  0.4116706848144531\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  54\n",
      "one move takes time(s):  1.0669116973876953\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  55\n",
      "one move takes time(s):  0.7240924835205078\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  56\n",
      "due to rollout lim, final rollout times:  2007 time elapsed:  4.117436647415161\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  4.1190245151519775\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  57\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.570713996887207\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  58\n",
      "one move takes time(s):  0.9063727855682373\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  59\n",
      "one move takes time(s):  0.8102707862854004\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  60\n",
      "one move takes time(s):  0.7948410511016846\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  61\n",
      "one move takes time(s):  0.5118825435638428\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  62\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.4866106510162354\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  63\n",
      "one move takes time(s):  1.2194716930389404\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  64\n",
      "one move takes time(s):  1.0631585121154785\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  65\n",
      "one move takes time(s):  0.40003371238708496\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  66\n",
      "one move takes time(s):  0.5248017311096191\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  67\n",
      "one move takes time(s):  0.7994439601898193\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  68\n",
      "one move takes time(s):  1.7445299625396729\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  69\n",
      "one move takes time(s):  1.3923382759094238\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  70\n",
      "one move takes time(s):  1.1259069442749023\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  71\n",
      "one move takes time(s):  0.7711186408996582\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  72\n",
      "one move takes time(s):  0.6994113922119141\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  73\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.7572383880615234\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  74\n",
      "one move takes time(s):  0.6253464221954346\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  75\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.8728930950164795\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  76\n",
      "one move takes time(s):  0.48997044563293457\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  77\n",
      "one move takes time(s):  0.43535828590393066\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  78\n",
      "one move takes time(s):  0.3595426082611084\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  79\n",
      "one move takes time(s):  0.20299601554870605\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  80\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.29732441902160645\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  81\n",
      "one move takes time(s):  0.16856956481933594\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  82\n",
      "one move takes time(s):  0.2281055450439453\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  83\n",
      "one move takes time(s):  0.14618325233459473\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  84\n",
      "one move takes time(s):  0.0759117603302002\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  85\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.04614591598510742\n",
      "no. of data collected:  8\n",
      "this self play has 86 steps, time elapsed 111.09091806411743\n",
      "winner is 1.0\n",
      "episode 1 finished\n",
      "episode 2 now start\n",
      "left space is 45\n",
      "start new self play\n",
      "-1 's turn, step no.  0\n",
      "one move takes time(s):  0.2788870334625244\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  1\n",
      "one move takes time(s):  0.3872663974761963\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  2\n",
      "one move takes time(s):  0.7398638725280762\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  3\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.8977236747741699\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  4\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.1434862613677979\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  5\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.6653375625610352\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  6\n",
      "one move takes time(s):  1.0317456722259521\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  7\n",
      "one move takes time(s):  0.9660594463348389\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  8\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.273202419281006\n",
      "one move takes time(s):  3.2755637168884277\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  9\n",
      "due to time lim, final rollout times:  326 time elapsed:  15.451812028884888\n",
      "one move takes time(s):  15.524316310882568\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  10\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  4.075166702270508\n",
      "one move takes time(s):  4.077147006988525\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  11\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.437698602676392\n",
      "one move takes time(s):  5.439387321472168\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  12\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  6.130883455276489\n",
      "one move takes time(s):  6.133838415145874\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  13\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.9078400135040283\n",
      "one move takes time(s):  3.9095683097839355\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  14\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  4.162283658981323\n",
      "one move takes time(s):  4.164633750915527\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  15\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  7.755749464035034\n",
      "one move takes time(s):  7.757403373718262\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  16\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.2671029567718506\n",
      "one move takes time(s):  3.268817901611328\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  17\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.0804083347320557\n",
      "one move takes time(s):  2.0825934410095215\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.9831395149230957\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  3.9850704669952393\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  19\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.103501796722412\n",
      "one move takes time(s):  3.105172634124756\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  20\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.880441188812256\n",
      "one move takes time(s):  2.8826980590820312\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  21\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.0198912620544434\n",
      "one move takes time(s):  2.021362781524658\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  22\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.7700538635253906\n",
      "one move takes time(s):  2.7716078758239746\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  23\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.308389186859131\n",
      "one move takes time(s):  2.3102314472198486\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  24\n",
      "one move takes time(s):  1.3071770668029785\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  25\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  7.848681688308716\n",
      "one move takes time(s):  7.8510847091674805\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  26\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.5376155376434326\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  27\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.9357385635375977\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.9383928775787354\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  28\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.795724630355835\n",
      "one move takes time(s):  2.7972054481506348\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  29\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  6.0431389808654785\n",
      "one move takes time(s):  6.045158624649048\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  30\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.613443613052368\n",
      "one move takes time(s):  2.614870548248291\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  31\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.2740952968597412\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  32\n",
      "one move takes time(s):  0.8352029323577881\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  33\n",
      "one move takes time(s):  1.3461806774139404\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  34\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.0532872676849365\n",
      "one move takes time(s):  2.0550363063812256\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  35\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.02419376373291\n",
      "one move takes time(s):  2.025496006011963\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  36\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.408241033554077\n",
      "one move takes time(s):  2.409400224685669\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  37\n",
      "due to time lim, final rollout times:  580 time elapsed:  8.002997875213623\n",
      "one move takes time(s):  8.00460147857666\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  38\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.457531690597534\n",
      "one move takes time(s):  2.4587268829345703\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  39\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.101863145828247\n",
      "one move takes time(s):  2.103200674057007\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  40\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.3621809482574463\n",
      "one move takes time(s):  2.3640496730804443\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  41\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.74359393119812\n",
      "one move takes time(s):  2.7447521686553955\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  42\n",
      "due to rollout lim, final rollout times:  2083 time elapsed:  2.011824131011963\n",
      "one move takes time(s):  2.0134949684143066\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  43\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.6314992904663086\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  44\n",
      "due to rollout lim, final rollout times:  2140 time elapsed:  2.0001301765441895\n",
      "one move takes time(s):  2.001375436782837\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  45\n",
      "one move takes time(s):  1.1390581130981445\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  46\n",
      "one move takes time(s):  1.0688812732696533\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  47\n",
      "one move takes time(s):  1.0112030506134033\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  48\n",
      "one move takes time(s):  1.1073882579803467\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  49\n",
      "due to rollout lim, final rollout times:  2117 time elapsed:  2.0041604042053223\n",
      "one move takes time(s):  2.005314350128174\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  50\n",
      "due to rollout lim, final rollout times:  2089 time elapsed:  2.002375364303589\n",
      "one move takes time(s):  2.0042619705200195\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  51\n",
      "one move takes time(s):  1.4625828266143799\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  52\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.0196187496185303\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  53\n",
      "due to rollout lim, final rollout times:  2025 time elapsed:  2.006908655166626\n",
      "one move takes time(s):  2.0087599754333496\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  54\n",
      "one move takes time(s):  1.3141038417816162\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  55\n",
      "one move takes time(s):  0.6814389228820801\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  56\n",
      "one move takes time(s):  0.4115931987762451\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  57\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.3975255489349365\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  58\n",
      "one move takes time(s):  0.3604867458343506\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  59\n",
      "one move takes time(s):  0.49048447608947754\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  60\n",
      "one move takes time(s):  0.3442535400390625\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  61\n",
      "one move takes time(s):  0.39155077934265137\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  62\n",
      "one move takes time(s):  0.30791234970092773\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  63\n",
      "one move takes time(s):  0.27323484420776367\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  64\n",
      "one move takes time(s):  0.18448162078857422\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  65\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.24069905281066895\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  66\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  7.008477687835693\n",
      "one move takes time(s):  7.0096940994262695\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  67\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.2066178321838379\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  68\n",
      "one move takes time(s):  0.18437647819519043\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  69\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.2790224552154541\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  70\n",
      "one move takes time(s):  0.24568963050842285\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  71\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.20771002769470215\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  72\n",
      "one move takes time(s):  0.12335610389709473\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  73\n",
      "one move takes time(s):  0.15574860572814941\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  74\n",
      "one move takes time(s):  0.0996856689453125\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  75\n",
      "one move takes time(s):  0.1755053997039795\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  76\n",
      "one move takes time(s):  0.1256110668182373\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  77\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.07665443420410156\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  78\n",
      "one move takes time(s):  0.06737971305847168\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  79\n",
      "one move takes time(s):  0.06554412841796875\n",
      "no. of data collected:  24\n",
      "-1 's turn, step no.  80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one move takes time(s):  0.06219601631164551\n",
      "no. of data collected:  24\n",
      "1 's turn, step no.  81\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.054244041442871094\n",
      "no. of data collected:  8\n",
      "this self play has 82 steps, time elapsed 166.88370156288147\n",
      "winner is 1.0\n",
      "episode 2 finished\n",
      "episode 3 now start\n",
      "left space is 45\n",
      "start new self play\n",
      "-1 's turn, step no.  0\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.2719700336456299\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  1\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  0.3847939968109131\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  2\n",
      "one move takes time(s):  0.5361490249633789\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  3\n",
      "one move takes time(s):  1.1309075355529785\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  4\n",
      "due to rollout lim, final rollout times:  2005 time elapsed:  2.00004243850708\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.0013773441314697\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  5\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.0000669956207275\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  6\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.63531231880188\n",
      "one move takes time(s):  2.636385202407837\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  7\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  1.6688075065612793\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  8\n",
      "one move takes time(s):  1.2702670097351074\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  9\n",
      "one move takes time(s):  1.951535940170288\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  10\n",
      "one move takes time(s):  1.4709160327911377\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  11\n",
      "one move takes time(s):  1.7194674015045166\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  12\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.429267644882202\n",
      "\n",
      "\n",
      "This is a random move\n",
      "\n",
      "\n",
      "one move takes time(s):  2.4306869506835938\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  13\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.800081491470337\n",
      "one move takes time(s):  2.8016140460968018\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  14\n",
      "one move takes time(s):  1.5989885330200195\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  15\n",
      "due to rollout lim, final rollout times:  2011 time elapsed:  2.0000996589660645\n",
      "one move takes time(s):  2.001239776611328\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  16\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  2.0287203788757324\n",
      "one move takes time(s):  2.0298960208892822\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  17\n",
      "due to rollout lim, final rollout times:  2179 time elapsed:  2.000108003616333\n",
      "one move takes time(s):  2.0013883113861084\n",
      "no. of data collected:  16\n",
      "-1 's turn, step no.  18\n",
      "one move takes time(s):  1.932281732559204\n",
      "no. of data collected:  16\n",
      "1 's turn, step no.  19\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.0097150802612305\n",
      "one move takes time(s):  3.0118114948272705\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  20\n",
      "due to time lim, final rollout times:  6 time elapsed:  26.681640148162842\n",
      "one move takes time(s):  26.727010488510132\n",
      "no. of data collected:  0\n",
      "1 's turn, step no.  21\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  3.4331939220428467\n",
      "one move takes time(s):  3.4352827072143555\n",
      "no. of data collected:  8\n",
      "-1 's turn, step no.  22\n",
      "due to rollout lim, final rollout times:  2001 time elapsed:  5.736376762390137\n",
      "one move takes time(s):  5.73788046836853\n",
      "no. of data collected:  8\n",
      "1 's turn, step no.  23\n"
     ]
    }
   ],
   "source": [
    "##### player._dep_lim = 4\n",
    "player._mode = 4 # almost mode 1\n",
    "player.reinforcement_learning(episode=320, left_space_max=None, rollout_times=2000, train_interval=5, \\\n",
    "                              life_span=40, t_lim=8, self_play_verbose=False, train_mode=0, \\\n",
    "                              T=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm_dep is:  1\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  92.18110156059265\n",
      "win steps:  55.88499720575014 lose steps:  35.31999293600141\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of Q is 0.8 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  113.77895212173462\n",
      "win steps:  58.052171389036026 lose steps:  74.19996290001855\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of P is 0.92 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm_dep is:  2\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  82.00473427772522\n",
      "win steps:  80.99999138297964 lose steps:  16.999998396226566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of Q is 0.47 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  144.61132621765137\n",
      "win steps:  71.99999294117715 lose steps:  95.99999020408264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of P is 0.51 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm_dep is:  3\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  28.193013429641724\n",
      "win steps:  0.0 lose steps:  14.199998580000141\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of Q is 0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  123.78511095046997\n",
      "win steps:  69.99998571428863 lose steps:  94.99998137255267\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of P is 0.49 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mm_dep is:  4\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  649.7168228626251\n",
      "win steps:  0.0 lose steps:  55.1999889600022\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of Q is 0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####               ####\n",
      "#### start testing ####\n",
      "testing took time:  1164.9451265335083\n",
      "win steps:  0.0 lose steps:  97.5799804840039\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        win ratio of P is 0 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'P': 0, 'Q': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.tester(mm_dep=1, Q=True, P=True, BOTH=False, mode=1, times=250, dep_lim=1, rollout_times=400, verbose=False)\n",
    "player.tester(mm_dep=2, Q=True, P=True, BOTH=False, mode=1, times=200, dep_lim=1, rollout_times=400, verbose=False)\n",
    "player.tester(mm_dep=3, Q=True, P=True, BOTH=False, mode=1, times=100, dep_lim=1, rollout_times=400, verbose=False)\n",
    "player.tester(mm_dep=4, Q=True, P=True, BOTH=False, mode=1, times=50, dep_lim=1, rollout_times=400, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = player._network._model.get_weights()\n",
    "fig = plt.gcf()\n",
    "print(w[0].shape)\n",
    "show_w = w[0]\n",
    "for in_layer in range(6):\n",
    "    for out_layer in range(64):\n",
    "        print('in', in_layer, 'out', out_layer)\n",
    "        plt.imshow(show_w[..., in_layer, out_layer], cmap='gray')\n",
    "        plt.pause(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
