{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:09.236354Z",
     "start_time": "2018-03-04T22:31:09.229711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "import line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:09.531121Z",
     "start_time": "2018-03-04T22:31:09.523040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython\n",
    "from Cython.Compiler.Options import directive_scopes, directive_types\n",
    "directive_types['linetrace'] = True\n",
    "directive_types['binding'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:12.604376Z",
     "start_time": "2018-03-04T22:31:09.733382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import importlib\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from math import sqrt, log, exp\n",
    "from numpy import unravel_index\n",
    "from random import choice, random, sample\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.models import Sequential, Model, load_model\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "K.set_image_dim_ordering('th')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:14.564269Z",
     "start_time": "2018-03-04T22:31:12.624928Z"
    }
   },
   "outputs": [],
   "source": [
    "class Ataxx:\n",
    "    def __init__(self, board=None):\n",
    "        if board is None:                  # if there is no initialization given\n",
    "            self.data = np.zeros((7, 7), dtype=np.int8)   # then generate a board with starting init, and black(-1) takes first turn\n",
    "            self.data[0, 0] = -1           \n",
    "            self.data[6, 6] = -1\n",
    "            self.data[0, 6] = 1\n",
    "            self.data[6, 0] = 1\n",
    "        else:\n",
    "            self.data = board.copy()\n",
    "            \n",
    "    def reset(self, board=None):\n",
    "        if board is None:\n",
    "            self.data = np.zeros((7, 7), dtype=np.int8)\n",
    "            self.data[0, 0] = -1           \n",
    "            self.data[6, 6] = -1\n",
    "            self.data[0, 6] = 1\n",
    "            self.data[6, 0] = 1\n",
    "        else:\n",
    "            self.data = board.copy()\n",
    "        \n",
    "    def get_feature_map(self, turn, move):\n",
    "        out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "        \n",
    "        # edge\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j == 0 or j == 8 or k == 0 or k == 8:\n",
    "                    out[0, j, k] = 1\n",
    "         \n",
    "        # my pieces\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j > 0 and j < 8 and k > 0 and k < 8:\n",
    "                    if self.data[j-1, k-1] == turn:\n",
    "                        out[1, j, k] = 1\n",
    "        \n",
    "        # op pieces\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j > 0 and j < 8 and k > 0 and k < 8:\n",
    "                    if self.data[j-1, k-1] == -turn:\n",
    "                        out[2, j, k] = 1\n",
    "         \n",
    "        # last move\n",
    "        if not move is None:               \n",
    "            out[3, move[0][0]+1, move[0][1]+1] = 1\n",
    "            out[4, move[1][0]+1, move[1][1]+1] = 1\n",
    "            \n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            for j in range(9):\n",
    "                for k in range(9):\n",
    "                    out[5, j, k] = 1\n",
    "        return np.array(out)\n",
    "    \n",
    "    def plot(self, is_next_move=False, turn=None):                        # plot the board\n",
    "        image = self.data.copy()\n",
    "        if is_next_move:\n",
    "            if turn not in [-1, 1]:\n",
    "                raise ValueError(\"Turn must be -1 or 1, or Must input a turn for next moves\")\n",
    "            else:\n",
    "                next_moves = self.get_moves(turn)\n",
    "                if len(next_moves) == 0:\n",
    "                    raise ValueError(\"Game is over already\")\n",
    "                next_pos = list(zip(*next_moves))[1]\n",
    "                for pos in next_pos:\n",
    "                    image[pos] = turn / 2\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.xticks(range(7), range(7))\n",
    "        plt.yticks(range(7), range(7))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_greedy_move(self, turn, moves=None):\n",
    "        best_score = -50\n",
    "        # get all possible moves if not provided\n",
    "        if moves is None:\n",
    "            moves, corr_dict = self.get_moves(turn)\n",
    "            for item in corr_dict:\n",
    "                moves.append(item)\n",
    "        \n",
    "        if len(moves) == 0:\n",
    "            raise ValueError('No Possible Moves')\n",
    "        \n",
    "        best_moves = []\n",
    "        # calculate greedy move\n",
    "        for (x0, y0), (x1, y1) in moves:\n",
    "            tmp_score = 0\n",
    "            if abs(x0-x1) <= 1 and abs(y0-y1) <= 1:\n",
    "                tmp_score += 1\n",
    "            for dr in range(-1, 2):\n",
    "                for dc in range(-1, 2):\n",
    "                    try:\n",
    "                        if x1+dr >= 0 and y1+dc >= 0:\n",
    "                            tmp_score += self.data[x1+dr, y1+dc] == -turn\n",
    "                    except:\n",
    "                        pass\n",
    "            if tmp_score > best_score:\n",
    "                best_moves = [((x0, y0), (x1, y1))]\n",
    "                best_score = tmp_score\n",
    "            elif tmp_score == best_score:\n",
    "                best_moves.append(((x0, y0), (x1, y1)))\n",
    "        return choice(best_moves)\n",
    "                \n",
    "    def is_valid(self, turn, pos, get_pos=False):\n",
    "        r = pos[0]\n",
    "        c = pos[1]\n",
    "        if self.data[r, c] != 0:\n",
    "            if not get_pos:\n",
    "                return False\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            for dr in range(-2, 3):\n",
    "                for dc in range(-2, 3):\n",
    "                    new_r = r+dr\n",
    "                    new_c = c+dc\n",
    "                    if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and self.data[new_r, new_c] == turn:\n",
    "                        if not get_pos:\n",
    "                            return True\n",
    "                        else:\n",
    "                            yield new_r, new_c, dr, dc\n",
    "            if not get_pos:\n",
    "                return False\n",
    "        \n",
    "    def get_moves(self, turn):\n",
    "        action_mask = np.zeros(792, dtype=np.int8)\n",
    "        next_moves = []\n",
    "        corr_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "                for new_r, new_c, dr, dc in self.is_valid(turn, (r, c), True): # duplicate move\n",
    "                    if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and self.data[new_r, new_c] == turn:\n",
    "                        if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                            if has_duplicate_move: \n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                corr_dict[cur_move] = dup_move\n",
    "                            elif self.data[new_r, new_c] == turn:\n",
    "                                dup_move = ((new_r, new_c), (r, c))\n",
    "                                next_moves.append(dup_move) \n",
    "                                has_duplicate_move = True\n",
    "                        elif self.data[new_r, new_c] == turn:\n",
    "                            cur_move = ((new_r, new_c), (r, c))\n",
    "                            next_moves.append(cur_move) \n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "        return next_moves, corr_dict\n",
    "        \n",
    "    def move_to(self, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "        \n",
    "        if not self.is_valid(turn, pos1):\n",
    "            raise ValueError(\"This move: \" + str((pos0, pos1)) + \" of turn: \" + str(turn) + \" is invalid\") \n",
    "        elif self.data[x0, y0] != turn:\n",
    "            raise ValueError(\"The starting position is not your piece\")\n",
    "        else:\n",
    "            self.data[x1, y1] = turn\n",
    "            if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "                self.data[x0, y0] = 0\n",
    "\n",
    "            for dr in range(-1, 2):                  # infection mode!!!!\n",
    "                for dc in range(-1, 2):\n",
    "                    if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                        if self.data[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                            self.data[x1+dr, y1+dc] = turn\n",
    "    \n",
    "    def evaluate(self, turn, this_turn, max_score=1, min_score=0.001):\n",
    "        turn_no=0\n",
    "        op_no=0\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if self.data[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif self.data[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        if len(self.get_moves(this_turn)[0]) == 0:# if one of them can no longer move, count and end\n",
    "            if turn_no > op_no:\n",
    "                return max_score\n",
    "            else:\n",
    "                return -max_score\n",
    "        else:\n",
    "            value = turn_no - op_no\n",
    "        return value * min_score\n",
    "    \n",
    "    @staticmethod    \n",
    "    def get_manual_q(turn, board):\n",
    "        '''consider linear growth of win prob with regard to n_diff\n",
    "        when diff >= 10, the slope grow a bit\n",
    "        when diff >= 35, consider win prob close to 1 or -1\n",
    "        ''' \n",
    "        turn_no = 0\n",
    "        op_no = 0\n",
    "        max1=0.9\n",
    "        max2=0.95\n",
    "        # get no diff of turns\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if board[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif board[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        diff = turn_no - op_no\n",
    "        if abs(diff) > 30:\n",
    "            return diff / abs(diff)\n",
    "        else:\n",
    "            return diff / 30\n",
    "        \n",
    "        # ignore the rest for now\n",
    "        sign = diff\n",
    "        diff = abs(diff)\n",
    "        if diff < 35:\n",
    "            diff = (diff / 35) ** 2 * max1\n",
    "        else:\n",
    "            diff = max2\n",
    "\n",
    "        if sign < 0:\n",
    "            return -diff\n",
    "        else:\n",
    "            return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:32:48.872352Z",
     "start_time": "2018-03-04T22:32:44.760920Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "def is_valid(np.int8_t[:, :] board, turn, pos):\n",
    "    cdef int dr, dc, r = pos[0], c = pos[1], new_r, new_c\n",
    "    if board[r, c] != 0:\n",
    "        return False\n",
    "    else:\n",
    "        for dr in range(-2, 3):\n",
    "            for dc in range(-2, 3):\n",
    "                new_r = r+dr\n",
    "                new_c = c+dc\n",
    "                if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                    return True\n",
    "        return False \n",
    "\n",
    "def get_moves(np.int8_t[:, :] board, turn):\n",
    "    cdef int r, c, dr, dc, new_r, new_c\n",
    "    next_moves = []\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "            if is_valid(board, turn, (r, c)): # duplicate move\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r+dr\n",
    "                        new_c = c+dc\n",
    "                        if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                            if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                                if not has_duplicate_move and board[new_r, new_c] == turn:\n",
    "                                    dup_move = ((new_r, new_c), (r, c))\n",
    "                                    next_moves.append(dup_move) \n",
    "                                    has_duplicate_move = True\n",
    "                            elif board[new_r, new_c] == turn:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                next_moves.append(cur_move) \n",
    "                            else:\n",
    "                                continue\n",
    "    return next_moves\n",
    "\n",
    "def get_moves_with_mask(np.int8_t[:, :] board, turn, dict policy_dict):\n",
    "    cdef int r, c, dr, dc, new_r, new_c\n",
    "    cdef np.int8_t[:] action_mask = np.zeros(792, dtype=np.int8)\n",
    "    next_moves = []\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "            if is_valid(board, turn, (r, c)): # duplicate move\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r+dr\n",
    "                        new_c = c+dc\n",
    "                        if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                            if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                if not has_duplicate_move:\n",
    "                                    next_moves.append(cur_move) \n",
    "                                    has_duplicate_move = True\n",
    "                            elif board[new_r, new_c] == turn:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                next_moves.append(cur_move) \n",
    "                            action_mask[policy_dict[cur_move]] = 1\n",
    "    return next_moves, np.array(action_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:14.621033Z",
     "start_time": "2018-03-04T22:31:14.608824Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyValueNetwork():\n",
    "    def __init__(self, model_name):\n",
    "        self._sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=24))\n",
    "        K.set_session(self._sess)\n",
    "        \n",
    "        self._model = load_model(model_name)\n",
    "        print(\"successfully loaded the model\")\n",
    "           \n",
    "    def predict(self, feature_map, action_mask):        \n",
    "        return self._sess.run(self._model.outputs, feed_dict={self._model.inputs[0]: feature_map.reshape(-1, 6, 9, 9), \\\n",
    "                                self._model.inputs[1]: action_mask.reshape(-1, 792), K.learning_phase(): 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:14.952229Z",
     "start_time": "2018-03-04T22:31:14.641881Z"
    }
   },
   "outputs": [],
   "source": [
    "'''These methods are for normal Min max'''\n",
    "class MinMaxNormal():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(board, turn):\n",
    "        turn_no = 0\n",
    "        op_no = 0\n",
    "        # get no diff of turns\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if board[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif board[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        return (turn_no - op_no)\n",
    "\n",
    "    @staticmethod\n",
    "    def move_to(board, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "\n",
    "    def min_max(self, board, turn, target_turn, depth=3, alpha=-100, beta=100, is_max=True, is_root=True):\n",
    "        '''A recursive alpha beta pruning min_max function\n",
    "        return: board evaluation, chosen move\n",
    "        NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "        if is_root:\n",
    "            best_moves = []\n",
    "        else:\n",
    "            best_move = ((0, 0), (0, 0))\n",
    "    \n",
    "        next_moves = get_moves(board, turn)\n",
    "        \n",
    "        if depth == 0 or len(next_moves) == 0: # start to do pruning and selecting once the recursion reaches the end\n",
    "            result = self.evaluate(board, target_turn)\n",
    "            return result, None\n",
    "        else:\n",
    "            if is_max:\n",
    "                alpha = -100\n",
    "            else:\n",
    "                beta = 100\n",
    "\n",
    "            for move in next_moves:\n",
    "                result, _ = self.min_max(self.move_to(board, turn, move[0], move[1]), \\\n",
    "                                    -turn, target_turn, depth-1, alpha, beta, not is_max, False)\n",
    "                # prun the searching tree or update alpha and beta respectively\n",
    "                if is_max:\n",
    "                    if result >= beta:\n",
    "                        return 100, None\n",
    "                    elif result > alpha:\n",
    "                        alpha = result\n",
    "                        if is_root:\n",
    "                            best_moves = [move]\n",
    "                        else:\n",
    "                            best_move = move\n",
    "                    elif result == alpha and is_root:\n",
    "                        best_moves.append(move)\n",
    "                else:\n",
    "                    if result <= alpha:\n",
    "                        return -100, None\n",
    "                    elif result < beta:\n",
    "                        beta = result\n",
    "                        if is_root:\n",
    "                            best_moves = [move]\n",
    "                        else:\n",
    "                            best_move = move\n",
    "                    elif result == beta and is_root:\n",
    "                        best_moves.append(move)\n",
    "            if is_max:\n",
    "                if is_root:\n",
    "                    return alpha, choice(best_moves)\n",
    "                else:\n",
    "                    return alpha, best_move\n",
    "            else:\n",
    "                if is_root:\n",
    "                    return beta, choice(best_moves)\n",
    "                else:\n",
    "                    return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:16.311380Z",
     "start_time": "2018-03-04T22:31:14.985968Z"
    }
   },
   "outputs": [],
   "source": [
    "class MinMaxZero():\n",
    "    def __init__(self, p_thresh, c_thresh, model_name=\"AtaxxZero.h5\"):\n",
    "        self._evaluator = PolicyValueNetwork(model_name)\n",
    "        self._policy_dict = self.get_policy_dict()\n",
    "        self._p_thresh = p_thresh\n",
    "        self._c_thresh = c_thresh\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_policy_dict():\n",
    "        '''Get the relation between policy no. and policy'''\n",
    "        index=0\n",
    "        policy_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r + dr\n",
    "                        new_c = c + dc\n",
    "                        if (dr != 0 or dc != 0) and (new_r < 7 and new_r >= 0) and (new_c < 7 and new_c >= 0):\n",
    "                            policy_dict[((r, c), (new_r, new_c))] = index\n",
    "                            index += 1\n",
    "        return policy_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_feature_map(board, turn, pre_move):\n",
    "        out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "\n",
    "        # edge\n",
    "        out[0, 0, :] = 1\n",
    "        out[0, 8, :] = 1\n",
    "        out[0, :, 0] = 1\n",
    "        out[0, :, 8] = 1\n",
    "\n",
    "        # my pieces\n",
    "        out[1, 1:8, 1:8] = (board == turn)\n",
    "\n",
    "        # op pieces\n",
    "        out[2, 1:8, 1:8] = (board == -turn)\n",
    "        \n",
    "        # last move\n",
    "        if not pre_move is None:               \n",
    "            out[3, pre_move[0][0]+1, pre_move[0][1]+1] = 1\n",
    "            out[4, pre_move[1][0]+1, pre_move[1][1]+1] = 1\n",
    "\n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            out[5, ...] = 1\n",
    "        return out\n",
    "    \n",
    "    def evaluate(self, feature_map, action_mask, turn, target_turn):\n",
    "        result = self._evaluator.predict(feature_map, action_mask)\n",
    "        p = result[0][0]\n",
    "        if turn == target_turn:\n",
    "            q = result[1][0][0]\n",
    "        else:\n",
    "            q = -result[1][0][0]\n",
    "        return p, q\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_to(board, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "    \n",
    "    def min_max(self, board, turn, target_turn, depth=3, alpha=-100, beta=100, is_max=True, is_root=True, \\\n",
    "                pre_move=None, t_lim=1):\n",
    "        '''A recursive alpha beta pruning min_max function\n",
    "        return: board evaluation, chosen move\n",
    "        NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "        if is_root:\n",
    "            best_moves = []\n",
    "            self._start = time.time()\n",
    "        else:\n",
    "            best_move = ((0, 0), (0, 0))\n",
    "        \n",
    "        # get next moves\n",
    "        next_moves, action_mask = get_moves_with_mask(board, turn, self._policy_dict)\n",
    "        # stop searching if the game is over\n",
    "        if len(next_moves) == 0:\n",
    "            diff = (board == target_turn).sum() - (board == -target_turn).sum()\n",
    "            if diff > 0:\n",
    "                return 1, None\n",
    "            elif diff < 0:\n",
    "                return -1, None\n",
    "            else:\n",
    "                turn_no = (board == turn).sum()\n",
    "                if turn == -target_turn:\n",
    "                    turn_no = 49 - turn_no # set turn_no to represent the number of target turn pieces\n",
    "                if turn_no >= 45:\n",
    "                    return 1, None\n",
    "                else:\n",
    "                    return -1, None\n",
    "        else: # otherwise calculate p and q and do the NN pruned minmax searching\n",
    "            feature_map = self.get_feature_map(board, turn, pre_move)\n",
    "            p, q = self.evaluate(feature_map, action_mask, turn, target_turn)\n",
    "            \n",
    "        if depth == 0: # once the recursion reaches the end, return the leaf node value\n",
    "            return q, None\n",
    "        else:\n",
    "            # generate move corresponding p list\n",
    "            move_prob = []\n",
    "            all_prob = 0.0\n",
    "            for move in next_moves:\n",
    "                prob = p[self._policy_dict[move]]\n",
    "                move_prob.append((move, prob))\n",
    "                all_prob += prob\n",
    "            move_prob = sorted(move_prob, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            if is_max:\n",
    "                alpha = -100\n",
    "            else:\n",
    "                beta = 100\n",
    "\n",
    "            sum_prob = 0.0\n",
    "            counter = 0\n",
    "            counter_thresh = len(move_prob) / self._c_thresh\n",
    "            prob_thresh = all_prob * self._p_thresh\n",
    "            # display_move_prob(move_prob)\n",
    "            for move, prob in move_prob:\n",
    "                sum_prob += prob\n",
    "                counter += 1\n",
    "                # do searching\n",
    "                result, _ = self.min_max(self.move_to(board, turn, move[0], move[1]), \\\n",
    "                                    -turn, target_turn, depth-1, alpha, beta, not is_max, False, move, t_lim)\n",
    "                # record all the move and its alpha value\n",
    "                if is_root:\n",
    "                    try:\n",
    "                        move_value.append((move, result))\n",
    "                    except:\n",
    "                        move_value = [(move, result)]\n",
    "                # prun the searching tree or update alpha and beta respectively\n",
    "                if is_max:\n",
    "                    if result >= beta:\n",
    "                        return 100, None\n",
    "                    elif result > alpha:\n",
    "                        alpha = result\n",
    "                        best_move = move\n",
    "                else:\n",
    "                    if result <= alpha:\n",
    "                        return -100, None\n",
    "                    elif result < beta:\n",
    "                        beta = result\n",
    "                        best_move = move\n",
    "\n",
    "                if (sum_prob >= prob_thresh and counter >= counter_thresh) or time.time() - self._start >= t_lim:\n",
    "                    break\n",
    "                    \n",
    "            if is_root: # incorporate ramdom characteristic\n",
    "                move_value = sorted(move_value, key=lambda x: x[1], reverse=True)\n",
    "                max_value = alpha\n",
    "                for move, value in move_value:\n",
    "                    if value >= max_value - 0.1 * abs(max_value):\n",
    "                        try:\n",
    "                            best_move.append(move)\n",
    "                        except:\n",
    "                            best_move = [move]\n",
    "                    else:\n",
    "                        break\n",
    "                # print(len(best_move))\n",
    "                best_move = choice(best_move)\n",
    "\n",
    "            if is_max:\n",
    "                return alpha, best_move\n",
    "            else:\n",
    "                return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:31:16.337352Z",
     "start_time": "2018-03-04T22:31:16.327111Z"
    }
   },
   "outputs": [],
   "source": [
    "def actor(mm_obj, board, turn, depth, pre_move):\n",
    "    try:\n",
    "        _, best_move = mm_obj.min_max(board, turn, turn, depth=depth, pre_move=pre_move, t_lim=3)\n",
    "    except:\n",
    "        _, best_move = mm_obj.min_max(board, turn, turn, depth=depth)\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T22:34:05.693676Z",
     "start_time": "2018-03-04T22:34:05.597080Z"
    }
   },
   "outputs": [],
   "source": [
    "def actor_compare(minmax_normal, minmax_zero, N_D, Z_D, rounds, verbose=False):\n",
    "    zero_win = 0\n",
    "    time_zero = []\n",
    "    time_normal = []\n",
    "    for r in range(rounds):\n",
    "        print(\"Round:\", r)\n",
    "        steps = 0\n",
    "        a = Ataxx()\n",
    "        turn = -1\n",
    "        zero_turn = choice([-1, 1])\n",
    "        if verbose:\n",
    "            print(\"zero chose color\", zero_turn)\n",
    "        best_move = None\n",
    "        while True:\n",
    "            steps += 1\n",
    "            if verbose:\n",
    "                a.plot()\n",
    "            if turn != zero_turn:\n",
    "                s = time.time()\n",
    "                best_move = actor(minmax_normal, a.data, turn, N_D, best_move)\n",
    "                span = time.time() - s\n",
    "                time_normal.append(span)\n",
    "            else:\n",
    "                s = time.time()\n",
    "                best_move = actor(minmax_zero, a.data, turn, Z_D, best_move)\n",
    "                span = time.time() - s\n",
    "                time_zero.append(span)\n",
    "            a.move_to(turn, best_move[0], best_move[1])\n",
    "            turn = -turn\n",
    "            result = a.evaluate(zero_turn, turn)\n",
    "            if result == 1:\n",
    "                print(\"minmax zero win!!\")\n",
    "                zero_win += 1\n",
    "                break\n",
    "            elif result == -1:\n",
    "                print(\"minmax normal win!!\")\n",
    "                break\n",
    "    print(\"In the previous \", rounds, \" rounds, zero win ratio is: \", float(zero_win) / float(rounds))\n",
    "    time_zero = np.array(time_zero)\n",
    "    time_normal = np.array(time_normal)\n",
    "    print(\"On average, for minmax normal with depth\", N_D, \\\n",
    "          \", each move takes time: \", time_normal.mean(),\\\n",
    "          \"max time elapsed:\", time_normal.max())\n",
    "    print(\"On average, for minmax zero with depth\", Z_D, \\\n",
    "          \", each move takes time: \", time_zero.mean(),\\\n",
    "          \"max time elapsed:\", time_zero.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T00:35:23.076608Z",
     "start_time": "2018-03-05T00:35:22.957404Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def actor_compare_bunch(normal_para, zero_para, N_D, Z_D, rounds):\n",
    "    minmax_normal = MinMaxZero(*normal_para)\n",
    "    minmax_zero = MinMaxZero(*zero_para)\n",
    "    zero_win = 0\n",
    "    for r in range(rounds):\n",
    "        print(\"Round:\", r)\n",
    "        steps = 0\n",
    "        a = Ataxx()\n",
    "        turn = -1\n",
    "        zero_turn = choice([-1, 1])\n",
    "        if verbose:\n",
    "            print(\"zero chose color\", zero_turn)\n",
    "        best_move = None\n",
    "        while True:\n",
    "            steps += 1\n",
    "            if verbose:\n",
    "                a.plot()\n",
    "            if turn != zero_turn:\n",
    "                s = time.time()\n",
    "                best_move = actor(minmax_normal, a.data, turn, N_D, best_move)\n",
    "                span = time.time() - s\n",
    "                time_normal.append(span)\n",
    "            else:\n",
    "                s = time.time()\n",
    "                best_move = actor(minmax_zero, a.data, turn, Z_D, best_move)\n",
    "                span = time.time() - s\n",
    "                time_zero.append(span)\n",
    "            a.move_to(turn, best_move[0], best_move[1])\n",
    "            turn = -turn\n",
    "            result = a.evaluate(zero_turn, turn)\n",
    "            if result == 1:\n",
    "                print(\"minmax zero win!!\")\n",
    "                zero_win += 1\n",
    "                break\n",
    "            elif result == -1:\n",
    "                print(\"minmax normal win!!\")\n",
    "                break\n",
    "    return float(zero_win) / float(rounds)\n",
    "\n",
    "def actor_compare_parallel(minmax_normal, minmax_zero, N_D, Z_D, rounds):\n",
    "    zero_win = 0\n",
    "    bunch = (rounds / 24)\n",
    "    rounds = 24 * bunch\n",
    "    para = [(minmax_normal, minmax_zero, N_D, Z_D, bunch)] * 24\n",
    "    with Pool(processes=24) as pool:\n",
    "        result = pool.starmap(actor_compare_bunch, para)\n",
    "    print(\"In the previous \", rounds, \" rounds, zero win ratio is: \", sum(result) / float(rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T00:35:23.702152Z",
     "start_time": "2018-03-05T00:35:23.698005Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_para = (1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "zero_para = (0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T00:36:09.397530Z",
     "start_time": "2018-03-05T00:35:24.532670Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded the model\n",
      "successfully loaded the model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d1bf97c4a741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# minmax_normal = MinMaxNormal()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mminmax_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AtaxxZero_91.2p_82.4q.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mactor_compare_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_para\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_para\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mactor_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmax_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-f9cb6daa6b96>\u001b[0m in \u001b[0;36mactor_compare_parallel\u001b[0;34m(minmax_normal, minmax_zero, N_D, Z_D, rounds)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpara\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmax_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbunch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_compare_bunch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In the previous \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" rounds, zero win ratio is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minmax_normal = MinMaxZero(1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "# minmax_normal = MinMaxNormal()\n",
    "minmax_zero = MinMaxZero(0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "actor_compare_parallel(normal_para, zero_para, 2, 2, 100)\n",
    "actor_compare(minmax_normal, minmax_zero, 2, 2, 100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:16:42.378737Z",
     "start_time": "2018-03-01T22:16:39.108087Z"
    }
   },
   "outputs": [],
   "source": [
    "a = Ataxx()\n",
    "%lprun -f minmax_zero.min_max _, best_move = minmax_zero.min_max(a.data, turn, turn, depth=6, pre_move=best_move, t_lim=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
