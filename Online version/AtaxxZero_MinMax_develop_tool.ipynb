{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:34.618080Z",
     "start_time": "2018-03-06T18:48:34.606895Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "import line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:35.252230Z",
     "start_time": "2018-03-06T18:48:34.752384Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "from Cython.Compiler.Options import directive_scopes, directive_types\n",
    "directive_types['linetrace'] = True\n",
    "directive_types['binding'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:37.962773Z",
     "start_time": "2018-03-06T18:48:35.273432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import importlib\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from math import sqrt, log, exp\n",
    "from numpy import unravel_index\n",
    "from random import choice, random, sample\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.models import Sequential, Model, load_model\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "K.set_image_dim_ordering('th')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:39.860214Z",
     "start_time": "2018-03-06T18:48:37.965515Z"
    }
   },
   "outputs": [],
   "source": [
    "class Ataxx:\n",
    "    def __init__(self, board=None):\n",
    "        if board is None:                  # if there is no initialization given\n",
    "            self.data = np.zeros((7, 7), dtype=np.int8)   # then generate a board with starting init, and black(-1) takes first turn\n",
    "            self.data[0, 0] = -1           \n",
    "            self.data[6, 6] = -1\n",
    "            self.data[0, 6] = 1\n",
    "            self.data[6, 0] = 1\n",
    "        else:\n",
    "            self.data = board.copy()\n",
    "            \n",
    "    def reset(self, board=None):\n",
    "        if board is None:\n",
    "            self.data = np.zeros((7, 7), dtype=np.int8)\n",
    "            self.data[0, 0] = -1           \n",
    "            self.data[6, 6] = -1\n",
    "            self.data[0, 6] = 1\n",
    "            self.data[6, 0] = 1\n",
    "        else:\n",
    "            self.data = board.copy()\n",
    "        \n",
    "    def get_feature_map(self, turn, move):\n",
    "        out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "        \n",
    "        # edge\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j == 0 or j == 8 or k == 0 or k == 8:\n",
    "                    out[0, j, k] = 1\n",
    "         \n",
    "        # my pieces\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j > 0 and j < 8 and k > 0 and k < 8:\n",
    "                    if self.data[j-1, k-1] == turn:\n",
    "                        out[1, j, k] = 1\n",
    "        \n",
    "        # op pieces\n",
    "        for j in range(9):\n",
    "            for k in range(9):\n",
    "                if j > 0 and j < 8 and k > 0 and k < 8:\n",
    "                    if self.data[j-1, k-1] == -turn:\n",
    "                        out[2, j, k] = 1\n",
    "         \n",
    "        # last move\n",
    "        if not move is None:               \n",
    "            out[3, move[0][0]+1, move[0][1]+1] = 1\n",
    "            out[4, move[1][0]+1, move[1][1]+1] = 1\n",
    "            \n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            for j in range(9):\n",
    "                for k in range(9):\n",
    "                    out[5, j, k] = 1\n",
    "        return np.array(out)\n",
    "    \n",
    "    def plot(self, is_next_move=False, turn=None):                        # plot the board\n",
    "        image = self.data.copy()\n",
    "        if is_next_move:\n",
    "            if turn not in [-1, 1]:\n",
    "                raise ValueError(\"Turn must be -1 or 1, or Must input a turn for next moves\")\n",
    "            else:\n",
    "                next_moves = self.get_moves(turn)\n",
    "                if len(next_moves) == 0:\n",
    "                    raise ValueError(\"Game is over already\")\n",
    "                next_pos = list(zip(*next_moves))[1]\n",
    "                for pos in next_pos:\n",
    "                    image[pos] = turn / 2\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.xticks(range(7), range(7))\n",
    "        plt.yticks(range(7), range(7))\n",
    "        plt.show()\n",
    "        \n",
    "    def get_greedy_move(self, turn, moves=None):\n",
    "        best_score = -50\n",
    "        # get all possible moves if not provided\n",
    "        if moves is None:\n",
    "            moves, corr_dict = self.get_moves(turn)\n",
    "            for item in corr_dict:\n",
    "                moves.append(item)\n",
    "        \n",
    "        if len(moves) == 0:\n",
    "            raise ValueError('No Possible Moves')\n",
    "        \n",
    "        best_moves = []\n",
    "        # calculate greedy move\n",
    "        for (x0, y0), (x1, y1) in moves:\n",
    "            tmp_score = 0\n",
    "            if abs(x0-x1) <= 1 and abs(y0-y1) <= 1:\n",
    "                tmp_score += 1\n",
    "            for dr in range(-1, 2):\n",
    "                for dc in range(-1, 2):\n",
    "                    try:\n",
    "                        if x1+dr >= 0 and y1+dc >= 0:\n",
    "                            tmp_score += self.data[x1+dr, y1+dc] == -turn\n",
    "                    except:\n",
    "                        pass\n",
    "            if tmp_score > best_score:\n",
    "                best_moves = [((x0, y0), (x1, y1))]\n",
    "                best_score = tmp_score\n",
    "            elif tmp_score == best_score:\n",
    "                best_moves.append(((x0, y0), (x1, y1)))\n",
    "        return choice(best_moves)\n",
    "                \n",
    "    def is_valid(self, turn, pos, get_pos=False):\n",
    "        r = pos[0]\n",
    "        c = pos[1]\n",
    "        if self.data[r, c] != 0:\n",
    "            if not get_pos:\n",
    "                return False\n",
    "            else:\n",
    "                return\n",
    "        else:\n",
    "            for dr in range(-2, 3):\n",
    "                for dc in range(-2, 3):\n",
    "                    new_r = r+dr\n",
    "                    new_c = c+dc\n",
    "                    if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and self.data[new_r, new_c] == turn:\n",
    "                        if not get_pos:\n",
    "                            return True\n",
    "                        else:\n",
    "                            yield new_r, new_c, dr, dc\n",
    "            if not get_pos:\n",
    "                return False\n",
    "        \n",
    "    def get_moves(self, turn):\n",
    "        action_mask = np.zeros(792, dtype=np.int8)\n",
    "        next_moves = []\n",
    "        corr_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "                for new_r, new_c, dr, dc in self.is_valid(turn, (r, c), True): # duplicate move\n",
    "                    if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and self.data[new_r, new_c] == turn:\n",
    "                        if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                            if has_duplicate_move: \n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                corr_dict[cur_move] = dup_move\n",
    "                            elif self.data[new_r, new_c] == turn:\n",
    "                                dup_move = ((new_r, new_c), (r, c))\n",
    "                                next_moves.append(dup_move) \n",
    "                                has_duplicate_move = True\n",
    "                        elif self.data[new_r, new_c] == turn:\n",
    "                            cur_move = ((new_r, new_c), (r, c))\n",
    "                            next_moves.append(cur_move) \n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "        return next_moves, corr_dict\n",
    "        \n",
    "    def move_to(self, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "        \n",
    "        if not self.is_valid(turn, pos1):\n",
    "            raise ValueError(\"This move: \" + str((pos0, pos1)) + \" of turn: \" + str(turn) + \" is invalid\") \n",
    "        elif self.data[x0, y0] != turn:\n",
    "            raise ValueError(\"The starting position is not your piece\")\n",
    "        else:\n",
    "            self.data[x1, y1] = turn\n",
    "            if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "                self.data[x0, y0] = 0\n",
    "\n",
    "            for dr in range(-1, 2):                  # infection mode!!!!\n",
    "                for dc in range(-1, 2):\n",
    "                    if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                        if self.data[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                            self.data[x1+dr, y1+dc] = turn\n",
    "    \n",
    "    def evaluate(self, turn, this_turn, max_score=1, min_score=0.001):\n",
    "        turn_no=0\n",
    "        op_no=0\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if self.data[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif self.data[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        if len(self.get_moves(this_turn)[0]) == 0:# if one of them can no longer move, count and end\n",
    "            if turn_no > op_no:\n",
    "                return max_score\n",
    "            else:\n",
    "                return -max_score\n",
    "        else:\n",
    "            value = turn_no - op_no\n",
    "        return value * min_score\n",
    "    \n",
    "    @staticmethod    \n",
    "    def get_manual_q(turn, board):\n",
    "        '''consider linear growth of win prob with regard to n_diff\n",
    "        when diff >= 10, the slope grow a bit\n",
    "        when diff >= 35, consider win prob close to 1 or -1\n",
    "        ''' \n",
    "        turn_no = 0\n",
    "        op_no = 0\n",
    "        max1=0.9\n",
    "        max2=0.95\n",
    "        # get no diff of turns\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if board[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif board[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        diff = turn_no - op_no\n",
    "        if abs(diff) > 30:\n",
    "            return diff / abs(diff)\n",
    "        else:\n",
    "            return diff / 30\n",
    "        \n",
    "        # ignore the rest for now\n",
    "        sign = diff\n",
    "        diff = abs(diff)\n",
    "        if diff < 35:\n",
    "            diff = (diff / 35) ** 2 * max1\n",
    "        else:\n",
    "            diff = max2\n",
    "\n",
    "        if sign < 0:\n",
    "            return -diff\n",
    "        else:\n",
    "            return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:39.871678Z",
     "start_time": "2018-03-06T18:48:39.863445Z"
    }
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "def is_valid(np.int8_t[:, :] board, turn, pos):\n",
    "    cdef int dr, dc, r = pos[0], c = pos[1], new_r, new_c\n",
    "    if board[r, c] != 0:\n",
    "        return False\n",
    "    else:\n",
    "        for dr in range(-2, 3):\n",
    "            for dc in range(-2, 3):\n",
    "                new_r = r+dr\n",
    "                new_c = c+dc\n",
    "                if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                    return True\n",
    "        return False \n",
    "\n",
    "def get_moves(np.int8_t[:, :] board, turn):\n",
    "    cdef int r, c, dr, dc, new_r, new_c\n",
    "    next_moves = []\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "            if is_valid(board, turn, (r, c)): # duplicate move\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r+dr\n",
    "                        new_c = c+dc\n",
    "                        if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                            if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                                if not has_duplicate_move and board[new_r, new_c] == turn:\n",
    "                                    dup_move = ((new_r, new_c), (r, c))\n",
    "                                    next_moves.append(dup_move) \n",
    "                                    has_duplicate_move = True\n",
    "                            elif board[new_r, new_c] == turn:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                next_moves.append(cur_move) \n",
    "                            else:\n",
    "                                continue\n",
    "    return next_moves\n",
    "\n",
    "def get_moves_with_mask(np.int8_t[:, :] board, turn, dict policy_dict):\n",
    "    cdef int r, c, dr, dc, new_r, new_c\n",
    "    cdef np.int8_t[:] action_mask = np.zeros(792, dtype=np.int8)\n",
    "    next_moves = []\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            has_duplicate_move = False      # move within the radius of one of another friendly piece is called\n",
    "            if is_valid(board, turn, (r, c)): # duplicate move\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r+dr\n",
    "                        new_c = c+dc\n",
    "                        if new_r >= 0 and new_c >= 0 and new_r < 7 and new_c < 7 and board[new_r, new_c] == turn:\n",
    "                            if abs(dr) <= 1 and abs(dc) <=1:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                if not has_duplicate_move:\n",
    "                                    next_moves.append(cur_move) \n",
    "                                    has_duplicate_move = True\n",
    "                            elif board[new_r, new_c] == turn:\n",
    "                                cur_move = ((new_r, new_c), (r, c))\n",
    "                                next_moves.append(cur_move) \n",
    "                            action_mask[policy_dict[cur_move]] = 1\n",
    "    return next_moves, np.array(action_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:39.976441Z",
     "start_time": "2018-03-06T18:48:39.962516Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyValueNetwork():\n",
    "    def __init__(self, model_name):\n",
    "        self._sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=24))\n",
    "        K.set_session(self._sess)\n",
    "        \n",
    "        self._model = load_model(model_name)\n",
    "        print(\"successfully loaded the model\")\n",
    "           \n",
    "    def predict(self, feature_map, action_mask):        \n",
    "        return self._sess.run(self._model.outputs, feed_dict={self._model.inputs[0]: feature_map.reshape(-1, 6, 9, 9), \\\n",
    "                                self._model.inputs[1]: action_mask.reshape(-1, 792), K.learning_phase(): 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:40.286651Z",
     "start_time": "2018-03-06T18:48:39.979298Z"
    }
   },
   "outputs": [],
   "source": [
    "'''These methods are for normal Min max'''\n",
    "class MinMaxNormal():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(board, turn):\n",
    "        turn_no = 0\n",
    "        op_no = 0\n",
    "        # get no diff of turns\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                if board[r, c] == turn:\n",
    "                    turn_no += 1\n",
    "                elif board[r, c] == -turn:\n",
    "                    op_no += 1\n",
    "        return (turn_no - op_no)\n",
    "\n",
    "    @staticmethod\n",
    "    def move_to(board, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "\n",
    "    def min_max(self, board, turn, target_turn, depth, alpha=-100, beta=100, is_max=True, is_root=True):\n",
    "        '''A recursive alpha beta pruning min_max function\n",
    "        return: board evaluation, chosen move\n",
    "        NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "        if is_root:\n",
    "            best_moves = []\n",
    "        else:\n",
    "            best_move = ((0, 0), (0, 0))\n",
    "    \n",
    "        next_moves = get_moves(board, turn)\n",
    "        \n",
    "        if depth == 0 or len(next_moves) == 0: # start to do pruning and selecting once the recursion reaches the end\n",
    "            result = self.evaluate(board, target_turn)\n",
    "            return result, None\n",
    "        else:\n",
    "            if is_max:\n",
    "                alpha = -100\n",
    "            else:\n",
    "                beta = 100\n",
    "\n",
    "            for move in next_moves:\n",
    "                result, _ = self.min_max(self.move_to(board, turn, move[0], move[1]), \\\n",
    "                                    -turn, target_turn, depth-1, alpha, beta, not is_max, False)\n",
    "                # prun the searching tree or update alpha and beta respectively\n",
    "                if is_max:\n",
    "                    if result >= beta:\n",
    "                        return 100, None\n",
    "                    elif result > alpha:\n",
    "                        alpha = result\n",
    "                        if is_root:\n",
    "                            best_moves = [move]\n",
    "                        else:\n",
    "                            best_move = move\n",
    "                    elif result == alpha and is_root:\n",
    "                        best_moves.append(move)\n",
    "                else:\n",
    "                    if result <= alpha:\n",
    "                        return -100, None\n",
    "                    elif result < beta:\n",
    "                        beta = result\n",
    "                        if is_root:\n",
    "                            best_moves = [move]\n",
    "                        else:\n",
    "                            best_move = move\n",
    "                    elif result == beta and is_root:\n",
    "                        best_moves.append(move)\n",
    "            if is_max:\n",
    "                if is_root:\n",
    "                    return alpha, choice(best_moves)\n",
    "                else:\n",
    "                    return alpha, best_move\n",
    "            else:\n",
    "                if is_root:\n",
    "                    return beta, choice(best_moves)\n",
    "                else:\n",
    "                    return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:41.227461Z",
     "start_time": "2018-03-06T18:48:40.289304Z"
    }
   },
   "outputs": [],
   "source": [
    "class MinMaxQ():\n",
    "    def __init__(self, model_name=\"AtaxxZero.h5\"):\n",
    "        self._evaluator = PolicyValueNetwork(model_name)\n",
    "        self._policy_dict = self.get_policy_dict()\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_policy_dict():\n",
    "        '''Get the relation between policy no. and policy'''\n",
    "        index=0\n",
    "        policy_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r + dr\n",
    "                        new_c = c + dc\n",
    "                        if (dr != 0 or dc != 0) and (new_r < 7 and new_r >= 0) and (new_c < 7 and new_c >= 0):\n",
    "                            policy_dict[((r, c), (new_r, new_c))] = index\n",
    "                            index += 1\n",
    "        return policy_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_feature_map(board, turn, pre_move):\n",
    "        out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "\n",
    "        # edge\n",
    "        out[0, 0, :] = 1\n",
    "        out[0, 8, :] = 1\n",
    "        out[0, :, 0] = 1\n",
    "        out[0, :, 8] = 1\n",
    "\n",
    "        # my pieces\n",
    "        out[1, 1:8, 1:8] = (board == turn)\n",
    "\n",
    "        # op pieces\n",
    "        out[2, 1:8, 1:8] = (board == -turn)\n",
    "        \n",
    "        # last move\n",
    "        if not pre_move is None:               \n",
    "            out[3, pre_move[0][0]+1, pre_move[0][1]+1] = 1\n",
    "            out[4, pre_move[1][0]+1, pre_move[1][1]+1] = 1\n",
    "\n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            out[5, ...] = 1\n",
    "        return out\n",
    "    \n",
    "    def evaluate(self, feature_map, action_mask, turn, target_turn):\n",
    "        result = self._evaluator.predict(feature_map, action_mask)\n",
    "        p = result[0][0]\n",
    "        if turn == target_turn:\n",
    "            q = result[1][0][0]\n",
    "        else:\n",
    "            q = -result[1][0][0]\n",
    "        return q\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_to(board, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "    \n",
    "    def min_max(self, board, turn, target_turn, depth=3, alpha=-100, beta=100, is_max=True, is_root=True, \\\n",
    "                pre_move=None, t_lim=1):\n",
    "        '''A recursive alpha beta pruning min_max function\n",
    "        return: board evaluation, chosen move\n",
    "        NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "        if is_root:\n",
    "            best_moves = []\n",
    "            self._start = time.time()\n",
    "        else:\n",
    "            best_move = ((0, 0), (0, 0))\n",
    "        \n",
    "        # get next moves\n",
    "        next_moves, action_mask = get_moves_with_mask(board, turn, self._policy_dict)\n",
    "        # stop searching if the game is over\n",
    "        if len(next_moves) == 0:\n",
    "            diff = (board == target_turn).sum() - (board == -target_turn).sum()\n",
    "            if diff > 0:\n",
    "                return 1, None\n",
    "            elif diff < 0:\n",
    "                return -1, None\n",
    "            else:\n",
    "                turn_no = (board == turn).sum()\n",
    "                if turn == -target_turn:\n",
    "                    turn_no = 49 - turn_no # set turn_no to represent the number of target turn pieces\n",
    "                if turn_no >= 45:\n",
    "                    return 1, None\n",
    "                else:\n",
    "                    return -1, None\n",
    "        elif depth == 0: # do cluster evaluation of all possible next boards\n",
    "            feature_map = self.get_feature_map(board, turn, pre_move)\n",
    "            q = self.evaluate(feature_map, action_mask, turn, target_turn)\n",
    "            return q, None\n",
    "        else:\n",
    "            # generate move corresponding p list\n",
    "            if is_max:\n",
    "                alpha = -100\n",
    "            else:\n",
    "                beta = 100\n",
    "                \n",
    "            for move in next_moves:\n",
    "                # do searching\n",
    "                result, _ = self.min_max(self.move_to(board, turn, move[0], move[1]), \\\n",
    "                                    -turn, target_turn, depth-1, alpha, beta, not is_max, False, move, t_lim)\n",
    "                # record all the move and its alpha value\n",
    "                if is_root:\n",
    "                    try:\n",
    "                        move_value.append((move, result))\n",
    "                    except:\n",
    "                        move_value = [(move, result)]\n",
    "                # prun the searching tree or update alpha and beta respectively\n",
    "                if is_max:\n",
    "                    if result >= beta:\n",
    "                        return 100, None\n",
    "                    elif result > alpha:\n",
    "                        alpha = result\n",
    "                        best_move = move\n",
    "                else:\n",
    "                    if result <= alpha:\n",
    "                        return -100, None\n",
    "                    elif result < beta:\n",
    "                        beta = result\n",
    "                        best_move = move\n",
    "\n",
    "                if time.time() - self._start >= t_lim:\n",
    "                    break\n",
    "                    \n",
    "            if is_root: # incorporate ramdom characteristic\n",
    "                move_value = sorted(move_value, key=lambda x: x[1], reverse=True)\n",
    "                max_value = alpha\n",
    "                for move, value in move_value:\n",
    "                    if value >= max_value - 0.1 * abs(max_value):\n",
    "                        try:\n",
    "                            best_move.append(move)\n",
    "                        except:\n",
    "                            best_move = [move]\n",
    "                    else:\n",
    "                        break\n",
    "                # print(len(best_move))\n",
    "                best_move = choice(best_move)\n",
    "\n",
    "            if is_max:\n",
    "                return alpha, best_move\n",
    "            else:\n",
    "                return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:42.688792Z",
     "start_time": "2018-03-06T18:48:41.303304Z"
    }
   },
   "outputs": [],
   "source": [
    "class MinMaxQCluster():\n",
    "    def __init__(self, model_name=\"AtaxxZero.h5\"):\n",
    "        self._evaluator = PolicyValueNetwork(model_name)\n",
    "        self._policy_dict = self.get_policy_dict()\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_policy_dict():\n",
    "        '''Get the relation between policy no. and policy'''\n",
    "        index=0\n",
    "        policy_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r + dr\n",
    "                        new_c = c + dc\n",
    "                        if (dr != 0 or dc != 0) and (new_r < 7 and new_r >= 0) and (new_c < 7 and new_c >= 0):\n",
    "                            policy_dict[((r, c), (new_r, new_c))] = index\n",
    "                            index += 1\n",
    "        return policy_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_feature_map(board, turn, pre_move):\n",
    "        out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "\n",
    "        # edge\n",
    "        out[0, 0, :] = 1\n",
    "        out[0, 8, :] = 1\n",
    "        out[0, :, 0] = 1\n",
    "        out[0, :, 8] = 1\n",
    "\n",
    "        # my pieces\n",
    "        out[1, 1:8, 1:8] = (board == turn)\n",
    "\n",
    "        # op pieces\n",
    "        out[2, 1:8, 1:8] = (board == -turn)\n",
    "        \n",
    "        # last move\n",
    "        if not pre_move is None:               \n",
    "            out[3, pre_move[0][0]+1, pre_move[0][1]+1] = 1\n",
    "            out[4, pre_move[1][0]+1, pre_move[1][1]+1] = 1\n",
    "\n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            out[5, ...] = 1\n",
    "        return out\n",
    "    \n",
    "    @staticmethod \n",
    "    def judge(board, target_turn):\n",
    "        '''evaluate the board with rules'''\n",
    "        diff = (board == target_turn).sum() - (board == -target_turn).sum()\n",
    "        if diff > 0:\n",
    "            return 1\n",
    "        elif diff < 0:\n",
    "            return -1\n",
    "        else:\n",
    "            turn_no = (board == turn).sum()\n",
    "            if turn == -target_turn:\n",
    "                turn_no = 49 - turn_no # set turn_no to represent the number of target turn pieces\n",
    "            if turn_no >= 45:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_to(board, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "    \n",
    "    def min_max(self, board, turn, target_turn, depth=3, alpha=-100, beta=100, is_max=True, is_root=True, \\\n",
    "                pre_move=None, t_lim=1):\n",
    "        '''A recursive alpha beta pruning min_max function\n",
    "        return: board evaluation, chosen move\n",
    "        NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "        if is_root:\n",
    "            best_moves = []\n",
    "            self._start = time.time()\n",
    "        else:\n",
    "            best_move = ((0, 0), (0, 0))\n",
    "        \n",
    "        # get next moves\n",
    "        next_moves, action_mask = get_moves_with_mask(board, turn, self._policy_dict)\n",
    "        # stop searching if the game is over\n",
    "        if len(next_moves) == 0:\n",
    "            return self.judge(board, target_turn), None\n",
    "        elif depth == 1 and not is_root: # do cluster evaluation of all possible next boards\n",
    "            new_feature_map = []\n",
    "            new_action_mask = []\n",
    "            # prepare all the input for a \n",
    "            for move in next_moves:\n",
    "                temp_board = self.move_to(board, turn, move[0], move[1])\n",
    "                temp_moves, temp_action_mask = get_moves_with_mask(temp_board, -turn, self._policy_dict)\n",
    "                if len(temp_moves) == 0:\n",
    "                    result = self.judge(temp_board, target_turn) == 1\n",
    "                    if result and is_max:\n",
    "                        return 1, None\n",
    "                    elif not result and not is_max:\n",
    "                        return -1, None\n",
    "                    else:\n",
    "                        continue\n",
    "                new_feature_map.append(self.get_feature_map(temp_board, -turn, move))\n",
    "                new_action_mask.append(temp_action_mask)\n",
    "                \n",
    "            feature_map = np.concatenate(new_feature_map, axis=0)\n",
    "            action_mask = np.concatenate(new_action_mask, axis=0)\n",
    "            q = self._evaluator.predict(feature_map, action_mask)[1][:, 0]\n",
    "            return q.min(), None \n",
    "        elif depth == 1 and is_root:\n",
    "            new_feature_map = []\n",
    "            new_action_mask = []\n",
    "            move_index_value = np.zeros(len(next_moves))\n",
    "            # prepare all the input for a \n",
    "            counter = 0\n",
    "            for move in next_moves:\n",
    "                temp_board = self.move_to(board, turn, move[0], move[1])\n",
    "                temp_moves, temp_action_mask = get_moves_with_mask(temp_board, -turn, self._policy_dict)\n",
    "                if len(temp_moves) == 0:\n",
    "                    result = self.judge(temp_board, -turn)\n",
    "                    move_index_value[counter] = result\n",
    "                    if result == -1:\n",
    "                        return -result, move\n",
    "                new_feature_map.append(self.get_feature_map(temp_board, -turn, move))\n",
    "                new_action_mask.append(temp_action_mask)\n",
    "                counter += 1\n",
    "                \n",
    "            feature_map = np.concatenate(new_feature_map, axis=0)\n",
    "            action_mask = np.concatenate(new_action_mask, axis=0)\n",
    "            q = self._evaluator.predict(feature_map, action_mask)[1][:, 0]\n",
    "            for i in range(len(next_moves)):\n",
    "                if move_index_value[i] == 0:\n",
    "                    move_index_value[i] = q[i]\n",
    "            arg_min = move_index_value.argmin()\n",
    "            return -move_index_value[arg_min], next_moves[arg_min]\n",
    "        else:\n",
    "            # generate move corresponding p list\n",
    "            if is_max:\n",
    "                alpha = -100\n",
    "            else:\n",
    "                beta = 100\n",
    "                \n",
    "            for move in next_moves:\n",
    "                # do searching\n",
    "                result, _ = self.min_max(self.move_to(board, turn, move[0], move[1]), \\\n",
    "                                    -turn, target_turn, depth-1, alpha, beta, not is_max, False, move, t_lim)\n",
    "                # record all the move and its alpha value\n",
    "                if is_root:\n",
    "                    try:\n",
    "                        move_value.append((move, result))\n",
    "                    except:\n",
    "                        move_value = [(move, result)]\n",
    "                # prun the searching tree or update alpha and beta respectively\n",
    "                if is_max:\n",
    "                    if result >= beta:\n",
    "                        return 100, None\n",
    "                    elif result > alpha:\n",
    "                        alpha = result\n",
    "                        best_move = move\n",
    "                else:\n",
    "                    if result <= alpha:\n",
    "                        return -100, None\n",
    "                    elif result < beta:\n",
    "                        beta = result\n",
    "                        best_move = move\n",
    "\n",
    "                if time.time() - self._start >= t_lim:\n",
    "                    break\n",
    "                    \n",
    "            if is_root: # incorporate ramdom characteristic\n",
    "                move_value = sorted(move_value, key=lambda x: x[1], reverse=True)\n",
    "                max_value = alpha\n",
    "                for move, value in move_value:\n",
    "                    if value >= max_value - 0.1 * abs(max_value):\n",
    "                        try:\n",
    "                            best_move.append(move)\n",
    "                        except:\n",
    "                            best_move = [move]\n",
    "                    else:\n",
    "                        break\n",
    "                # print(len(best_move))\n",
    "                best_move = choice(best_move)\n",
    "\n",
    "            if is_max:\n",
    "                return alpha, best_move\n",
    "            else:\n",
    "                return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:43.976692Z",
     "start_time": "2018-03-06T18:48:42.691402Z"
    }
   },
   "outputs": [],
   "source": [
    "class MinMaxZero():\n",
    "    def __init__(self, p_thresh, c_thresh, model_name=\"AtaxxZero.h5\"):\n",
    "        self._evaluator = PolicyValueNetwork(model_name)\n",
    "        self._policy_dict = self.get_policy_dict()\n",
    "        self._p_thresh = p_thresh\n",
    "        self._c_thresh = c_thresh\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_policy_dict():\n",
    "        '''Get the relation between policy no. and policy'''\n",
    "        index=0\n",
    "        policy_dict = {}\n",
    "        for r in range(7):\n",
    "            for c in range(7):\n",
    "                for dr in range(-2, 3):\n",
    "                    for dc in range(-2, 3):\n",
    "                        new_r = r + dr\n",
    "                        new_c = c + dc\n",
    "                        if (dr != 0 or dc != 0) and (new_r < 7 and new_r >= 0) and (new_c < 7 and new_c >= 0):\n",
    "                            policy_dict[((r, c), (new_r, new_c))] = index\n",
    "                            index += 1\n",
    "        return policy_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def get_feature_map(board, turn, pre_move):\n",
    "        out = np.zeros((6, 9, 9), dtype=np.int8)\n",
    "        # define 1 edge\n",
    "\n",
    "        # edge\n",
    "        out[0, 0, :] = 1\n",
    "        out[0, 8, :] = 1\n",
    "        out[0, :, 0] = 1\n",
    "        out[0, :, 8] = 1\n",
    "\n",
    "        # my pieces\n",
    "        out[1, 1:8, 1:8] = (board == turn)\n",
    "\n",
    "        # op pieces\n",
    "        out[2, 1:8, 1:8] = (board == -turn)\n",
    "        \n",
    "        # last move\n",
    "        if not pre_move is None:               \n",
    "            out[3, pre_move[0][0]+1, pre_move[0][1]+1] = 1\n",
    "            out[4, pre_move[1][0]+1, pre_move[1][1]+1] = 1\n",
    "\n",
    "        # whose first\n",
    "        if turn == -1:\n",
    "            out[5, ...] = 1\n",
    "        return out\n",
    "    \n",
    "    def evaluate(self, feature_map, action_mask, turn, target_turn):\n",
    "        result = self._evaluator.predict(feature_map, action_mask)\n",
    "        p = result[0][0]\n",
    "        if turn == target_turn:\n",
    "            q = result[1][0][0]\n",
    "        else:\n",
    "            q = -result[1][0][0]\n",
    "        return p, q\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal_evaluate(board, target_turn):\n",
    "        return (board == target_turn).sum() - (board == -target_turn).sum()\n",
    "    \n",
    "    @staticmethod \n",
    "    def judge(board, end_turn, target_turn):\n",
    "        '''evaluate the board with rules'''\n",
    "        diff = (2 * (board == end_turn).sum() - 49) \n",
    "        if target_turn != end_turn:\n",
    "            diff = -diff\n",
    "        if diff > 0:\n",
    "            return 1\n",
    "        elif diff <= 0:\n",
    "            return -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def move_to(board, turn, pos0, pos1):\n",
    "        x0 = pos0[0]\n",
    "        y0 = pos0[1]\n",
    "        x1 = pos1[0]\n",
    "        y1 = pos1[1]\n",
    "\n",
    "        board = board.copy()\n",
    "        board[x1, y1] = turn\n",
    "        if abs(x0 - x1) > 1 or abs(y0 - y1) > 1:   # jump move\n",
    "            board[x0, y0] = 0\n",
    "\n",
    "        for dr in range(-1, 2):                  # infection mode!!!!\n",
    "            for dc in range(-1, 2):\n",
    "                if x1+dr >= 0 and y1+dc >= 0 and x1+dr < 7 and y1+dc < 7:\n",
    "                    if board[x1+dr, y1+dc] == -turn:  # convert any piece of the opponent to 'turn'\n",
    "                        board[x1+dr, y1+dc] = turn\n",
    "        return board\n",
    "    \n",
    "    def min_max(self, board, turn, target_turn, is_normal=False, depth=3, alpha=-100, beta=100, is_max=True, is_root=True, \\\n",
    "                pre_move=None, t_lim=1):\n",
    "        '''A recursive alpha beta pruning min_max function\n",
    "        return: board evaluation, chosen move\n",
    "        NB. for board evaluation, if the searching was pruned, it will return 100 for a minimizer and -100 for a maximizer'''\n",
    "        if is_root:\n",
    "            best_moves = []\n",
    "            self._start = time.time()\n",
    "        else:\n",
    "            best_move = ((0, 0), (0, 0))\n",
    "        \n",
    "        # get next moves\n",
    "        next_moves, action_mask = get_moves_with_mask(board, turn, self._policy_dict)\n",
    "        # stop searching if the game is over\n",
    "        if len(next_moves) == 0:\n",
    "            return self.judge(board, turn, target_turn), None\n",
    "        elif not is_normal: # otherwise calculate p and q and do the NN pruned minmax searching\n",
    "            feature_map = self.get_feature_map(board, turn, pre_move)\n",
    "            p, q = self.evaluate(feature_map, action_mask, turn, target_turn)\n",
    "            \n",
    "        if depth == 0:\n",
    "            if not is_normal: # once the recursion reaches the end, return the leaf node value\n",
    "                return q, None\n",
    "            else:\n",
    "                return self.normal_evaluate(board, target_turn), None\n",
    "        else:\n",
    "            # generate move corresponding p list\n",
    "            move_prob = []\n",
    "            all_prob = 0.0\n",
    "            if not is_normal:\n",
    "                for move in next_moves:\n",
    "                    prob = p[self._policy_dict[move]]\n",
    "                    move_prob.append((move, prob))\n",
    "                    all_prob += prob\n",
    "                move_prob = sorted(move_prob, key=lambda x: x[1], reverse=True)\n",
    "            else:\n",
    "                move_prob = [(move, 1) for move in next_moves]\n",
    "\n",
    "            if is_max:\n",
    "                alpha = -100\n",
    "            else:\n",
    "                beta = 100\n",
    "\n",
    "            if not is_normal:\n",
    "                sum_prob = 0.0\n",
    "                counter = 0\n",
    "                counter_thresh = len(move_prob) / self._c_thresh\n",
    "                prob_thresh = all_prob * self._p_thresh\n",
    "            # display_move_prob(move_prob)\n",
    "            for move, prob in move_prob:\n",
    "                if not is_normal:\n",
    "                    sum_prob += prob\n",
    "                    counter += 1\n",
    "                # do searching\n",
    "                result, _ = self.min_max(self.move_to(board, turn, move[0], move[1]), \\\n",
    "                                    -turn, target_turn, is_normal, depth-1, alpha, beta, not is_max, False, move, t_lim)\n",
    "                # record all the move and its alpha value\n",
    "                if is_root:\n",
    "                    try:\n",
    "                        move_value.append((move, result))\n",
    "                    except:\n",
    "                        move_value = [(move, result)]\n",
    "                # prun the searching tree or update alpha and beta respectively\n",
    "                if is_max:\n",
    "                    if result >= beta:\n",
    "                        return 100, None\n",
    "                    elif result > alpha:\n",
    "                        alpha = result\n",
    "                        best_move = move\n",
    "                else:\n",
    "                    if result <= alpha:\n",
    "                        return -100, None\n",
    "                    elif result < beta:\n",
    "                        beta = result\n",
    "                        best_move = move\n",
    "\n",
    "                if time.time() - self._start >= t_lim:\n",
    "                    break\n",
    "                elif not is_normal and sum_prob >= prob_thresh and counter >= counter_thresh:\n",
    "                    break\n",
    "                    \n",
    "            if is_root: # incorporate ramdom characteristic\n",
    "                move_value = sorted(move_value, key=lambda x: x[1], reverse=True)\n",
    "                max_value = alpha\n",
    "                for move, value in move_value:\n",
    "                    if value >= max_value - 0.1 * abs(max_value):\n",
    "                        try:\n",
    "                            best_move.append(move)\n",
    "                        except:\n",
    "                            best_move = [move]\n",
    "                    else:\n",
    "                        break\n",
    "                # print(len(best_move))\n",
    "                best_move = choice(best_move)\n",
    "\n",
    "            if is_max:\n",
    "                return alpha, best_move\n",
    "            else:\n",
    "                return beta, best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:43.986377Z",
     "start_time": "2018-03-06T18:48:43.979506Z"
    }
   },
   "outputs": [],
   "source": [
    "def actor(mm_obj, board, turn, depth, pre_move):\n",
    "    try:\n",
    "        _, best_move = mm_obj.min_max(board, turn, turn, depth=depth, pre_move=pre_move, t_lim=6)\n",
    "    except:\n",
    "        _, best_move = mm_obj.min_max(board, turn, turn, depth=depth)\n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:48:44.184440Z",
     "start_time": "2018-03-06T18:48:44.073367Z"
    }
   },
   "outputs": [],
   "source": [
    "def actor_compare(minmax_common, minmax_normal, minmax_zero, C_D, N_D, Z_D, common_steps, rounds, verbose=False):\n",
    "    zero_win = 0\n",
    "    time_zero = []\n",
    "    time_normal = []\n",
    "    for r in range(rounds):\n",
    "        print(\"Round:\", r)\n",
    "        steps = 0\n",
    "        a = Ataxx()\n",
    "        turn = -1\n",
    "        zero_turn = choice([-1, 1])\n",
    "        if verbose:\n",
    "            print(\"zero chose color\", zero_turn)\n",
    "        best_move = None\n",
    "        while True:\n",
    "            steps += 1\n",
    "            if verbose:\n",
    "                a.plot()\n",
    "            if steps > common_steps:\n",
    "                if turn != zero_turn:\n",
    "                    s = time.time()\n",
    "                    best_move = actor(minmax_normal, a.data, turn, N_D, best_move)\n",
    "                    span = time.time() - s\n",
    "                    time_normal.append(span)\n",
    "                else:\n",
    "                    s = time.time()\n",
    "                    best_move = actor(minmax_zero, a.data, turn, Z_D, best_move)\n",
    "                    span = time.time() - s\n",
    "                    time_zero.append(span)\n",
    "            else:\n",
    "                best_move = actor(minmax_common, a.data, turn, C_D, best_move)\n",
    "            a.move_to(turn, best_move[0], best_move[1])\n",
    "            turn = -turn\n",
    "            result = a.evaluate(zero_turn, turn)\n",
    "            if result == 1:\n",
    "                print(\"minmax zero win!!\")\n",
    "                zero_win += 1\n",
    "                break\n",
    "            elif result == -1:\n",
    "                print(\"minmax normal win!!\")\n",
    "                break\n",
    "            elif steps > 300:\n",
    "                print(\"too many steps, give up round\")\n",
    "                zero_win = 0\n",
    "                break\n",
    "    print(\"In the previous \", rounds, \" rounds, zero win ratio is: \", float(zero_win) / float(rounds))\n",
    "    time_zero = np.array(time_zero)\n",
    "    time_normal = np.array(time_normal)\n",
    "    print(\"On average, for minmax normal with depth\", N_D, \\\n",
    "          \", each move takes time: \", time_normal.mean(),\\\n",
    "          \"max time elapsed:\", time_normal.max())\n",
    "    print(\"On average, for minmax zero with depth\", Z_D, \\\n",
    "          \", each move takes time: \", time_zero.mean(),\\\n",
    "          \"max time elapsed:\", time_zero.max())\n",
    "    return float(zero_win) / float(rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T22:47:11.418913Z",
     "start_time": "2018-03-06T22:15:35.211711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded the model\n",
      "successfully loaded the model\n",
      "Round: 0\n",
      "minmax zero win!!\n",
      "Round: 1\n",
      "minmax normal win!!\n",
      "Round: 2\n",
      "minmax zero win!!\n",
      "Round: 3\n",
      "minmax normal win!!\n",
      "Round: 4\n",
      "minmax zero win!!\n",
      "Round: 5\n",
      "minmax zero win!!\n",
      "Round: 6\n",
      "minmax zero win!!\n",
      "Round: 7\n",
      "minmax zero win!!\n",
      "Round: 8\n",
      "minmax zero win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax normal win!!\n",
      "Round: 11\n",
      "minmax zero win!!\n",
      "Round: 12\n",
      "minmax zero win!!\n",
      "Round: 13\n",
      "minmax zero win!!\n",
      "Round: 14\n",
      "minmax zero win!!\n",
      "Round: 15\n",
      "minmax normal win!!\n",
      "Round: 16\n",
      "minmax zero win!!\n",
      "Round: 17\n",
      "minmax normal win!!\n",
      "Round: 18\n",
      "minmax zero win!!\n",
      "Round: 19\n",
      "minmax zero win!!\n",
      "Round: 20\n",
      "minmax zero win!!\n",
      "Round: 21\n",
      "minmax zero win!!\n",
      "Round: 22\n",
      "minmax zero win!!\n",
      "Round: 23\n",
      "minmax normal win!!\n",
      "Round: 24\n",
      "minmax zero win!!\n",
      "Round: 25\n",
      "minmax zero win!!\n",
      "Round: 26\n",
      "minmax zero win!!\n",
      "Round: 27\n",
      "minmax zero win!!\n",
      "Round: 28\n",
      "minmax normal win!!\n",
      "Round: 29\n",
      "minmax zero win!!\n",
      "Round: 30\n",
      "minmax zero win!!\n",
      "Round: 31\n",
      "minmax zero win!!\n",
      "Round: 32\n",
      "minmax normal win!!\n",
      "Round: 33\n",
      "minmax zero win!!\n",
      "Round: 34\n",
      "minmax zero win!!\n",
      "Round: 35\n",
      "minmax zero win!!\n",
      "Round: 36\n",
      "minmax zero win!!\n",
      "Round: 37\n",
      "minmax zero win!!\n",
      "Round: 38\n",
      "minmax zero win!!\n",
      "Round: 39\n",
      "minmax normal win!!\n",
      "Round: 40\n",
      "minmax zero win!!\n",
      "Round: 41\n",
      "minmax zero win!!\n",
      "Round: 42\n",
      "minmax normal win!!\n",
      "Round: 43\n",
      "minmax zero win!!\n",
      "Round: 44\n",
      "minmax zero win!!\n",
      "Round: 45\n",
      "minmax normal win!!\n",
      "Round: 46\n",
      "minmax zero win!!\n",
      "Round: 47\n",
      "minmax zero win!!\n",
      "Round: 48\n",
      "minmax normal win!!\n",
      "Round: 49\n",
      "minmax normal win!!\n",
      "Round: 50\n",
      "minmax zero win!!\n",
      "Round: 51\n",
      "minmax zero win!!\n",
      "Round: 52\n",
      "minmax normal win!!\n",
      "Round: 53\n",
      "minmax zero win!!\n",
      "Round: 54\n",
      "minmax zero win!!\n",
      "Round: 55\n",
      "minmax zero win!!\n",
      "Round: 56\n",
      "minmax zero win!!\n",
      "Round: 57\n",
      "minmax zero win!!\n",
      "Round: 58\n",
      "minmax zero win!!\n",
      "Round: 59\n",
      "minmax zero win!!\n",
      "Round: 60\n",
      "minmax zero win!!\n",
      "Round: 61\n",
      "minmax zero win!!\n",
      "Round: 62\n",
      "minmax zero win!!\n",
      "Round: 63\n",
      "minmax normal win!!\n",
      "Round: 64\n",
      "minmax normal win!!\n",
      "Round: 65\n",
      "minmax zero win!!\n",
      "Round: 66\n",
      "minmax zero win!!\n",
      "Round: 67\n",
      "minmax zero win!!\n",
      "Round: 68\n",
      "minmax zero win!!\n",
      "Round: 69\n",
      "minmax zero win!!\n",
      "Round: 70\n",
      "minmax zero win!!\n",
      "Round: 71\n",
      "minmax zero win!!\n",
      "Round: 72\n",
      "minmax normal win!!\n",
      "Round: 73\n",
      "minmax zero win!!\n",
      "Round: 74\n",
      "minmax zero win!!\n",
      "Round: 75\n",
      "minmax zero win!!\n",
      "Round: 76\n",
      "minmax normal win!!\n",
      "Round: 77\n",
      "minmax zero win!!\n",
      "Round: 78\n",
      "minmax zero win!!\n",
      "Round: 79\n",
      "minmax zero win!!\n",
      "Round: 80\n",
      "minmax zero win!!\n",
      "Round: 81\n",
      "minmax zero win!!\n",
      "Round: 82\n",
      "minmax normal win!!\n",
      "Round: 83\n",
      "minmax normal win!!\n",
      "Round: 84\n",
      "minmax zero win!!\n",
      "Round: 85\n",
      "minmax zero win!!\n",
      "Round: 86\n",
      "minmax zero win!!\n",
      "Round: 87\n",
      "minmax zero win!!\n",
      "Round: 88\n",
      "minmax normal win!!\n",
      "Round: 89\n",
      "minmax zero win!!\n",
      "Round: 90\n",
      "minmax zero win!!\n",
      "Round: 91\n",
      "minmax zero win!!\n",
      "Round: 92\n",
      "minmax zero win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax zero win!!\n",
      "Round: 95\n",
      "minmax normal win!!\n",
      "Round: 96\n",
      "minmax zero win!!\n",
      "Round: 97\n",
      "minmax normal win!!\n",
      "Round: 98\n",
      "minmax zero win!!\n",
      "Round: 99\n",
      "minmax zero win!!\n",
      "common_steps taken are:  5\n",
      "In the previous  100  rounds, zero win ratio is:  0.77\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.0308437865928 max time elapsed: 0.449991941452\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0276159613244 max time elapsed: 0.414402008057\n",
      "Round: 0\n",
      "minmax zero win!!\n",
      "Round: 1\n",
      "minmax zero win!!\n",
      "Round: 2\n",
      "minmax normal win!!\n",
      "Round: 3\n",
      "minmax zero win!!\n",
      "Round: 4\n",
      "minmax normal win!!\n",
      "Round: 5\n",
      "minmax normal win!!\n",
      "Round: 6\n",
      "minmax normal win!!\n",
      "Round: 7\n",
      "minmax zero win!!\n",
      "Round: 8\n",
      "minmax normal win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax normal win!!\n",
      "Round: 11\n",
      "minmax normal win!!\n",
      "Round: 12\n",
      "minmax zero win!!\n",
      "Round: 13\n",
      "minmax normal win!!\n",
      "Round: 14\n",
      "minmax normal win!!\n",
      "Round: 15\n",
      "minmax zero win!!\n",
      "Round: 16\n",
      "minmax normal win!!\n",
      "Round: 17\n",
      "minmax zero win!!\n",
      "Round: 18\n",
      "minmax normal win!!\n",
      "Round: 19\n",
      "minmax normal win!!\n",
      "Round: 20\n",
      "minmax zero win!!\n",
      "Round: 21\n",
      "minmax zero win!!\n",
      "Round: 22\n",
      "minmax normal win!!\n",
      "Round: 23\n",
      "minmax zero win!!\n",
      "Round: 24\n",
      "minmax normal win!!\n",
      "Round: 25\n",
      "minmax normal win!!\n",
      "Round: 26\n",
      "minmax zero win!!\n",
      "Round: 27\n",
      "minmax zero win!!\n",
      "Round: 28\n",
      "minmax normal win!!\n",
      "Round: 29\n",
      "minmax normal win!!\n",
      "Round: 30\n",
      "minmax normal win!!\n",
      "Round: 31\n",
      "minmax zero win!!\n",
      "Round: 32\n",
      "minmax zero win!!\n",
      "Round: 33\n",
      "minmax normal win!!\n",
      "Round: 34\n",
      "minmax normal win!!\n",
      "Round: 35\n",
      "minmax normal win!!\n",
      "Round: 36\n",
      "minmax normal win!!\n",
      "Round: 37\n",
      "minmax zero win!!\n",
      "Round: 38\n",
      "minmax normal win!!\n",
      "Round: 39\n",
      "minmax zero win!!\n",
      "Round: 40\n",
      "minmax zero win!!\n",
      "Round: 41\n",
      "minmax zero win!!\n",
      "Round: 42\n",
      "minmax zero win!!\n",
      "Round: 43\n",
      "minmax normal win!!\n",
      "Round: 44\n",
      "minmax zero win!!\n",
      "Round: 45\n",
      "minmax zero win!!\n",
      "Round: 46\n",
      "minmax zero win!!\n",
      "Round: 47\n",
      "minmax zero win!!\n",
      "Round: 48\n",
      "minmax normal win!!\n",
      "Round: 49\n",
      "minmax zero win!!\n",
      "Round: 50\n",
      "minmax normal win!!\n",
      "Round: 51\n",
      "minmax normal win!!\n",
      "Round: 52\n",
      "minmax zero win!!\n",
      "Round: 53\n",
      "minmax normal win!!\n",
      "Round: 54\n",
      "minmax normal win!!\n",
      "Round: 55\n",
      "minmax zero win!!\n",
      "Round: 56\n",
      "minmax zero win!!\n",
      "Round: 57\n",
      "minmax zero win!!\n",
      "Round: 58\n",
      "minmax normal win!!\n",
      "Round: 59\n",
      "minmax zero win!!\n",
      "Round: 60\n",
      "minmax zero win!!\n",
      "Round: 61\n",
      "minmax zero win!!\n",
      "Round: 62\n",
      "minmax normal win!!\n",
      "Round: 63\n",
      "minmax zero win!!\n",
      "Round: 64\n",
      "minmax zero win!!\n",
      "Round: 65\n",
      "minmax zero win!!\n",
      "Round: 66\n",
      "minmax zero win!!\n",
      "Round: 67\n",
      "minmax normal win!!\n",
      "Round: 68\n",
      "minmax normal win!!\n",
      "Round: 69\n",
      "minmax zero win!!\n",
      "Round: 70\n",
      "minmax normal win!!\n",
      "Round: 71\n",
      "minmax normal win!!\n",
      "Round: 72\n",
      "minmax zero win!!\n",
      "Round: 73\n",
      "minmax normal win!!\n",
      "Round: 74\n",
      "minmax zero win!!\n",
      "Round: 75\n",
      "minmax zero win!!\n",
      "Round: 76\n",
      "minmax zero win!!\n",
      "Round: 77\n",
      "minmax normal win!!\n",
      "Round: 78\n",
      "minmax normal win!!\n",
      "Round: 79\n",
      "minmax normal win!!\n",
      "Round: 80\n",
      "minmax normal win!!\n",
      "Round: 81\n",
      "minmax normal win!!\n",
      "Round: 82\n",
      "minmax normal win!!\n",
      "Round: 83\n",
      "minmax normal win!!\n",
      "Round: 84\n",
      "minmax zero win!!\n",
      "Round: 85\n",
      "minmax normal win!!\n",
      "Round: 86\n",
      "minmax zero win!!\n",
      "Round: 87\n",
      "minmax zero win!!\n",
      "Round: 88\n",
      "minmax zero win!!\n",
      "Round: 89\n",
      "minmax normal win!!\n",
      "Round: 90\n",
      "minmax zero win!!\n",
      "Round: 91\n",
      "minmax zero win!!\n",
      "Round: 92\n",
      "minmax normal win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax zero win!!\n",
      "Round: 95\n",
      "minmax zero win!!\n",
      "Round: 96\n",
      "minmax normal win!!\n",
      "Round: 97\n",
      "minmax zero win!!\n",
      "Round: 98\n",
      "minmax normal win!!\n",
      "Round: 99\n",
      "minmax zero win!!\n",
      "common_steps taken are:  6\n",
      "In the previous  100  rounds, zero win ratio is:  0.52\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.0317174238786 max time elapsed: 0.15899515152\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0340133460861 max time elapsed: 0.206792831421\n",
      "Round: 0\n",
      "minmax zero win!!\n",
      "Round: 1\n",
      "minmax zero win!!\n",
      "Round: 2\n",
      "minmax normal win!!\n",
      "Round: 3\n",
      "minmax zero win!!\n",
      "Round: 4\n",
      "minmax normal win!!\n",
      "Round: 5\n",
      "minmax zero win!!\n",
      "Round: 6\n",
      "minmax zero win!!\n",
      "Round: 7\n",
      "minmax normal win!!\n",
      "Round: 8\n",
      "minmax normal win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax normal win!!\n",
      "Round: 11\n",
      "minmax zero win!!\n",
      "Round: 12\n",
      "minmax normal win!!\n",
      "Round: 13\n",
      "minmax normal win!!\n",
      "Round: 14\n",
      "minmax normal win!!\n",
      "Round: 15\n",
      "minmax zero win!!\n",
      "Round: 16\n",
      "minmax normal win!!\n",
      "Round: 17\n",
      "minmax normal win!!\n",
      "Round: 18\n",
      "minmax normal win!!\n",
      "Round: 19\n",
      "minmax zero win!!\n",
      "Round: 20\n",
      "minmax zero win!!\n",
      "Round: 21\n",
      "minmax normal win!!\n",
      "Round: 22\n",
      "minmax zero win!!\n",
      "Round: 23\n",
      "minmax normal win!!\n",
      "Round: 24\n",
      "minmax normal win!!\n",
      "Round: 25\n",
      "minmax zero win!!\n",
      "Round: 26\n",
      "minmax zero win!!\n",
      "Round: 27\n",
      "minmax normal win!!\n",
      "Round: 28\n",
      "minmax normal win!!\n",
      "Round: 29\n",
      "minmax normal win!!\n",
      "Round: 30\n",
      "minmax normal win!!\n",
      "Round: 31\n",
      "minmax normal win!!\n",
      "Round: 32\n",
      "minmax normal win!!\n",
      "Round: 33\n",
      "minmax zero win!!\n",
      "Round: 34\n",
      "minmax normal win!!\n",
      "Round: 35\n",
      "minmax normal win!!\n",
      "Round: 36\n",
      "minmax normal win!!\n",
      "Round: 37\n",
      "minmax zero win!!\n",
      "Round: 38\n",
      "minmax zero win!!\n",
      "Round: 39\n",
      "minmax normal win!!\n",
      "Round: 40\n",
      "minmax normal win!!\n",
      "Round: 41\n",
      "minmax zero win!!\n",
      "Round: 42\n",
      "minmax zero win!!\n",
      "Round: 43\n",
      "minmax normal win!!\n",
      "Round: 44\n",
      "minmax normal win!!\n",
      "Round: 45\n",
      "minmax zero win!!\n",
      "Round: 46\n",
      "minmax normal win!!\n",
      "Round: 47\n",
      "minmax normal win!!\n",
      "Round: 48\n",
      "minmax zero win!!\n",
      "Round: 49\n",
      "minmax normal win!!\n",
      "Round: 50\n",
      "minmax normal win!!\n",
      "Round: 51\n",
      "minmax zero win!!\n",
      "Round: 52\n",
      "minmax normal win!!\n",
      "Round: 53\n",
      "minmax zero win!!\n",
      "Round: 54\n",
      "minmax zero win!!\n",
      "Round: 55\n",
      "minmax zero win!!\n",
      "Round: 56\n",
      "minmax zero win!!\n",
      "Round: 57\n",
      "minmax normal win!!\n",
      "Round: 58\n",
      "minmax normal win!!\n",
      "Round: 59\n",
      "minmax zero win!!\n",
      "Round: 60\n",
      "minmax zero win!!\n",
      "Round: 61\n",
      "minmax normal win!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 62\n",
      "minmax normal win!!\n",
      "Round: 63\n",
      "minmax zero win!!\n",
      "Round: 64\n",
      "minmax normal win!!\n",
      "Round: 65\n",
      "minmax zero win!!\n",
      "Round: 66\n",
      "minmax zero win!!\n",
      "Round: 67\n",
      "minmax normal win!!\n",
      "Round: 68\n",
      "minmax normal win!!\n",
      "Round: 69\n",
      "minmax normal win!!\n",
      "Round: 70\n",
      "minmax zero win!!\n",
      "Round: 71\n",
      "minmax zero win!!\n",
      "Round: 72\n",
      "minmax normal win!!\n",
      "Round: 73\n",
      "minmax zero win!!\n",
      "Round: 74\n",
      "minmax zero win!!\n",
      "Round: 75\n",
      "minmax zero win!!\n",
      "Round: 76\n",
      "minmax normal win!!\n",
      "Round: 77\n",
      "minmax zero win!!\n",
      "Round: 78\n",
      "minmax normal win!!\n",
      "Round: 79\n",
      "minmax normal win!!\n",
      "Round: 80\n",
      "minmax zero win!!\n",
      "Round: 81\n",
      "minmax zero win!!\n",
      "Round: 82\n",
      "minmax normal win!!\n",
      "Round: 83\n",
      "minmax normal win!!\n",
      "Round: 84\n",
      "minmax normal win!!\n",
      "Round: 85\n",
      "minmax zero win!!\n",
      "Round: 86\n",
      "minmax zero win!!\n",
      "Round: 87\n",
      "minmax normal win!!\n",
      "Round: 88\n",
      "minmax normal win!!\n",
      "Round: 89\n",
      "minmax normal win!!\n",
      "Round: 90\n",
      "minmax zero win!!\n",
      "Round: 91\n",
      "minmax zero win!!\n",
      "Round: 92\n",
      "minmax normal win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax normal win!!\n",
      "Round: 95\n",
      "minmax normal win!!\n",
      "Round: 96\n",
      "minmax zero win!!\n",
      "Round: 97\n",
      "minmax zero win!!\n",
      "Round: 98\n",
      "minmax zero win!!\n",
      "Round: 99\n",
      "minmax normal win!!\n",
      "common_steps taken are:  7\n",
      "In the previous  100  rounds, zero win ratio is:  0.46\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.0295500077056 max time elapsed: 0.157096147537\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0335514533292 max time elapsed: 0.191436052322\n",
      "Round: 0\n",
      "minmax normal win!!\n",
      "Round: 1\n",
      "minmax normal win!!\n",
      "Round: 2\n",
      "minmax zero win!!\n",
      "Round: 3\n",
      "minmax zero win!!\n",
      "Round: 4\n",
      "minmax zero win!!\n",
      "Round: 5\n",
      "minmax zero win!!\n",
      "Round: 6\n",
      "minmax normal win!!\n",
      "Round: 7\n",
      "minmax normal win!!\n",
      "Round: 8\n",
      "minmax zero win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax normal win!!\n",
      "Round: 11\n",
      "minmax zero win!!\n",
      "Round: 12\n",
      "minmax normal win!!\n",
      "Round: 13\n",
      "minmax zero win!!\n",
      "Round: 14\n",
      "minmax zero win!!\n",
      "Round: 15\n",
      "minmax normal win!!\n",
      "Round: 16\n",
      "minmax zero win!!\n",
      "Round: 17\n",
      "minmax normal win!!\n",
      "Round: 18\n",
      "minmax normal win!!\n",
      "Round: 19\n",
      "minmax zero win!!\n",
      "Round: 20\n",
      "minmax normal win!!\n",
      "Round: 21\n",
      "minmax zero win!!\n",
      "Round: 22\n",
      "minmax zero win!!\n",
      "Round: 23\n",
      "minmax zero win!!\n",
      "Round: 24\n",
      "minmax normal win!!\n",
      "Round: 25\n",
      "minmax zero win!!\n",
      "Round: 26\n",
      "minmax normal win!!\n",
      "Round: 27\n",
      "minmax normal win!!\n",
      "Round: 28\n",
      "minmax normal win!!\n",
      "Round: 29\n",
      "minmax zero win!!\n",
      "Round: 30\n",
      "minmax normal win!!\n",
      "Round: 31\n",
      "minmax zero win!!\n",
      "Round: 32\n",
      "minmax zero win!!\n",
      "Round: 33\n",
      "minmax normal win!!\n",
      "Round: 34\n",
      "minmax normal win!!\n",
      "Round: 35\n",
      "minmax normal win!!\n",
      "Round: 36\n",
      "minmax normal win!!\n",
      "Round: 37\n",
      "minmax normal win!!\n",
      "Round: 38\n",
      "minmax zero win!!\n",
      "Round: 39\n",
      "minmax normal win!!\n",
      "Round: 40\n",
      "minmax normal win!!\n",
      "Round: 41\n",
      "minmax zero win!!\n",
      "Round: 42\n",
      "minmax zero win!!\n",
      "Round: 43\n",
      "minmax zero win!!\n",
      "Round: 44\n",
      "minmax zero win!!\n",
      "Round: 45\n",
      "minmax zero win!!\n",
      "Round: 46\n",
      "minmax normal win!!\n",
      "Round: 47\n",
      "minmax zero win!!\n",
      "Round: 48\n",
      "minmax zero win!!\n",
      "Round: 49\n",
      "minmax zero win!!\n",
      "Round: 50\n",
      "minmax zero win!!\n",
      "Round: 51\n",
      "minmax normal win!!\n",
      "Round: 52\n",
      "minmax zero win!!\n",
      "Round: 53\n",
      "minmax normal win!!\n",
      "Round: 54\n",
      "minmax normal win!!\n",
      "Round: 55\n",
      "minmax normal win!!\n",
      "Round: 56\n",
      "minmax zero win!!\n",
      "Round: 57\n",
      "minmax normal win!!\n",
      "Round: 58\n",
      "minmax normal win!!\n",
      "Round: 59\n",
      "minmax zero win!!\n",
      "Round: 60\n",
      "minmax normal win!!\n",
      "Round: 61\n",
      "minmax zero win!!\n",
      "Round: 62\n",
      "minmax zero win!!\n",
      "Round: 63\n",
      "minmax normal win!!\n",
      "Round: 64\n",
      "minmax zero win!!\n",
      "Round: 65\n",
      "minmax normal win!!\n",
      "Round: 66\n",
      "minmax zero win!!\n",
      "Round: 67\n",
      "minmax normal win!!\n",
      "Round: 68\n",
      "minmax normal win!!\n",
      "Round: 69\n",
      "minmax normal win!!\n",
      "Round: 70\n",
      "minmax zero win!!\n",
      "Round: 71\n",
      "minmax normal win!!\n",
      "Round: 72\n",
      "minmax zero win!!\n",
      "Round: 73\n",
      "minmax normal win!!\n",
      "Round: 74\n",
      "minmax normal win!!\n",
      "Round: 75\n",
      "minmax normal win!!\n",
      "Round: 76\n",
      "minmax zero win!!\n",
      "Round: 77\n",
      "minmax normal win!!\n",
      "Round: 78\n",
      "minmax normal win!!\n",
      "Round: 79\n",
      "minmax normal win!!\n",
      "Round: 80\n",
      "minmax normal win!!\n",
      "Round: 81\n",
      "minmax zero win!!\n",
      "Round: 82\n",
      "minmax normal win!!\n",
      "Round: 83\n",
      "minmax zero win!!\n",
      "Round: 84\n",
      "minmax zero win!!\n",
      "Round: 85\n",
      "minmax normal win!!\n",
      "Round: 86\n",
      "minmax zero win!!\n",
      "Round: 87\n",
      "minmax zero win!!\n",
      "Round: 88\n",
      "minmax zero win!!\n",
      "Round: 89\n",
      "minmax zero win!!\n",
      "Round: 90\n",
      "minmax normal win!!\n",
      "Round: 91\n",
      "minmax zero win!!\n",
      "Round: 92\n",
      "minmax normal win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax normal win!!\n",
      "Round: 95\n",
      "minmax zero win!!\n",
      "Round: 96\n",
      "minmax normal win!!\n",
      "Round: 97\n",
      "minmax normal win!!\n",
      "Round: 98\n",
      "minmax normal win!!\n",
      "Round: 99\n",
      "minmax normal win!!\n",
      "common_steps taken are:  8\n",
      "In the previous  100  rounds, zero win ratio is:  0.48\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.0301341833311 max time elapsed: 0.162523031235\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0351763293656 max time elapsed: 0.202037096024\n",
      "Round: 0\n",
      "minmax normal win!!\n",
      "Round: 1\n",
      "minmax normal win!!\n",
      "Round: 2\n",
      "minmax normal win!!\n",
      "Round: 3\n",
      "minmax zero win!!\n",
      "Round: 4\n",
      "minmax normal win!!\n",
      "Round: 5\n",
      "minmax normal win!!\n",
      "Round: 6\n",
      "minmax normal win!!\n",
      "Round: 7\n",
      "minmax zero win!!\n",
      "Round: 8\n",
      "minmax zero win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax zero win!!\n",
      "Round: 11\n",
      "minmax zero win!!\n",
      "Round: 12\n",
      "minmax normal win!!\n",
      "Round: 13\n",
      "minmax zero win!!\n",
      "Round: 14\n",
      "minmax normal win!!\n",
      "Round: 15\n",
      "minmax zero win!!\n",
      "Round: 16\n",
      "minmax zero win!!\n",
      "Round: 17\n",
      "minmax zero win!!\n",
      "Round: 18\n",
      "minmax zero win!!\n",
      "Round: 19\n",
      "minmax normal win!!\n",
      "Round: 20\n",
      "minmax zero win!!\n",
      "Round: 21\n",
      "minmax zero win!!\n",
      "Round: 22\n",
      "minmax zero win!!\n",
      "Round: 23\n",
      "minmax zero win!!\n",
      "Round: 24\n",
      "minmax normal win!!\n",
      "Round: 25\n",
      "minmax zero win!!\n",
      "Round: 26\n",
      "minmax zero win!!\n",
      "Round: 27\n",
      "minmax normal win!!\n",
      "Round: 28\n",
      "minmax normal win!!\n",
      "Round: 29\n",
      "minmax normal win!!\n",
      "Round: 30\n",
      "minmax normal win!!\n",
      "Round: 31\n",
      "minmax zero win!!\n",
      "Round: 32\n",
      "minmax zero win!!\n",
      "Round: 33\n",
      "minmax normal win!!\n",
      "Round: 34\n",
      "minmax normal win!!\n",
      "Round: 35\n",
      "minmax normal win!!\n",
      "Round: 36\n",
      "minmax normal win!!\n",
      "Round: 37\n",
      "minmax normal win!!\n",
      "Round: 38\n",
      "minmax normal win!!\n",
      "Round: 39\n",
      "minmax zero win!!\n",
      "Round: 40\n",
      "minmax normal win!!\n",
      "Round: 41\n",
      "minmax normal win!!\n",
      "Round: 42\n",
      "minmax zero win!!\n",
      "Round: 43\n",
      "minmax normal win!!\n",
      "Round: 44\n",
      "minmax zero win!!\n",
      "Round: 45\n",
      "minmax normal win!!\n",
      "Round: 46\n",
      "minmax normal win!!\n",
      "Round: 47\n",
      "minmax normal win!!\n",
      "Round: 48\n",
      "minmax normal win!!\n",
      "Round: 49\n",
      "minmax normal win!!\n",
      "Round: 50\n",
      "minmax normal win!!\n",
      "Round: 51\n",
      "minmax normal win!!\n",
      "Round: 52\n",
      "minmax zero win!!\n",
      "Round: 53\n",
      "minmax zero win!!\n",
      "Round: 54\n",
      "minmax normal win!!\n",
      "Round: 55\n",
      "minmax zero win!!\n",
      "Round: 56\n",
      "minmax normal win!!\n",
      "Round: 57\n",
      "minmax zero win!!\n",
      "Round: 58\n",
      "minmax normal win!!\n",
      "Round: 59\n",
      "minmax normal win!!\n",
      "Round: 60\n",
      "minmax normal win!!\n",
      "Round: 61\n",
      "minmax normal win!!\n",
      "Round: 62\n",
      "minmax zero win!!\n",
      "Round: 63\n",
      "minmax normal win!!\n",
      "Round: 64\n",
      "minmax zero win!!\n",
      "Round: 65\n",
      "minmax zero win!!\n",
      "Round: 66\n",
      "minmax normal win!!\n",
      "Round: 67\n",
      "minmax normal win!!\n",
      "Round: 68\n",
      "minmax normal win!!\n",
      "Round: 69\n",
      "minmax zero win!!\n",
      "Round: 70\n",
      "minmax normal win!!\n",
      "Round: 71\n",
      "minmax normal win!!\n",
      "Round: 72\n",
      "minmax normal win!!\n",
      "Round: 73\n",
      "minmax normal win!!\n",
      "Round: 74\n",
      "minmax normal win!!\n",
      "Round: 75\n",
      "minmax normal win!!\n",
      "Round: 76\n",
      "minmax zero win!!\n",
      "Round: 77\n",
      "minmax zero win!!\n",
      "Round: 78\n",
      "minmax zero win!!\n",
      "Round: 79\n",
      "minmax zero win!!\n",
      "Round: 80\n",
      "minmax normal win!!\n",
      "Round: 81\n",
      "minmax zero win!!\n",
      "Round: 82\n",
      "minmax zero win!!\n",
      "Round: 83\n",
      "minmax zero win!!\n",
      "Round: 84\n",
      "minmax zero win!!\n",
      "Round: 85\n",
      "minmax zero win!!\n",
      "Round: 86\n",
      "minmax normal win!!\n",
      "Round: 87\n",
      "minmax normal win!!\n",
      "Round: 88\n",
      "minmax normal win!!\n",
      "Round: 89\n",
      "minmax zero win!!\n",
      "Round: 90\n",
      "minmax normal win!!\n",
      "Round: 91\n",
      "minmax normal win!!\n",
      "Round: 92\n",
      "minmax normal win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax normal win!!\n",
      "Round: 95\n",
      "minmax normal win!!\n",
      "Round: 96\n",
      "minmax zero win!!\n",
      "Round: 97\n",
      "minmax normal win!!\n",
      "Round: 98\n",
      "minmax zero win!!\n",
      "Round: 99\n",
      "minmax normal win!!\n",
      "common_steps taken are:  9\n",
      "In the previous  100  rounds, zero win ratio is:  0.43\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.0273397975459 max time elapsed: 0.164400100708\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0384139504003 max time elapsed: 0.367812871933\n",
      "Round: 0\n",
      "minmax zero win!!\n",
      "Round: 1\n",
      "minmax zero win!!\n",
      "Round: 2\n",
      "minmax zero win!!\n",
      "Round: 3\n",
      "minmax zero win!!\n",
      "Round: 4\n",
      "minmax zero win!!\n",
      "Round: 5\n",
      "minmax normal win!!\n",
      "Round: 6\n",
      "minmax zero win!!\n",
      "Round: 7\n",
      "minmax zero win!!\n",
      "Round: 8\n",
      "minmax zero win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax zero win!!\n",
      "Round: 11\n",
      "minmax zero win!!\n",
      "Round: 12\n",
      "minmax zero win!!\n",
      "Round: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax zero win!!\n",
      "Round: 14\n",
      "minmax zero win!!\n",
      "Round: 15\n",
      "minmax normal win!!\n",
      "Round: 16\n",
      "minmax zero win!!\n",
      "Round: 17\n",
      "minmax zero win!!\n",
      "Round: 18\n",
      "minmax zero win!!\n",
      "Round: 19\n",
      "minmax normal win!!\n",
      "Round: 20\n",
      "minmax zero win!!\n",
      "Round: 21\n",
      "minmax zero win!!\n",
      "Round: 22\n",
      "minmax zero win!!\n",
      "Round: 23\n",
      "minmax zero win!!\n",
      "Round: 24\n",
      "minmax normal win!!\n",
      "Round: 25\n",
      "minmax normal win!!\n",
      "Round: 26\n",
      "minmax zero win!!\n",
      "Round: 27\n",
      "minmax normal win!!\n",
      "Round: 28\n",
      "minmax normal win!!\n",
      "Round: 29\n",
      "minmax zero win!!\n",
      "Round: 30\n",
      "minmax zero win!!\n",
      "Round: 31\n",
      "minmax zero win!!\n",
      "Round: 32\n",
      "minmax normal win!!\n",
      "Round: 33\n",
      "minmax zero win!!\n",
      "Round: 34\n",
      "minmax zero win!!\n",
      "Round: 35\n",
      "minmax zero win!!\n",
      "Round: 36\n",
      "minmax zero win!!\n",
      "Round: 37\n",
      "minmax zero win!!\n",
      "Round: 38\n",
      "minmax normal win!!\n",
      "Round: 39\n",
      "minmax zero win!!\n",
      "Round: 40\n",
      "minmax zero win!!\n",
      "Round: 41\n",
      "minmax zero win!!\n",
      "Round: 42\n",
      "minmax zero win!!\n",
      "Round: 43\n",
      "minmax zero win!!\n",
      "Round: 44\n",
      "minmax zero win!!\n",
      "Round: 45\n",
      "minmax zero win!!\n",
      "Round: 46\n",
      "minmax zero win!!\n",
      "Round: 47\n",
      "minmax zero win!!\n",
      "Round: 48\n",
      "minmax zero win!!\n",
      "Round: 49\n",
      "minmax zero win!!\n",
      "Round: 50\n",
      "minmax zero win!!\n",
      "Round: 51\n",
      "minmax zero win!!\n",
      "Round: 52\n",
      "minmax zero win!!\n",
      "Round: 53\n",
      "minmax zero win!!\n",
      "Round: 54\n",
      "minmax zero win!!\n",
      "Round: 55\n",
      "minmax normal win!!\n",
      "Round: 56\n",
      "minmax zero win!!\n",
      "Round: 57\n",
      "minmax zero win!!\n",
      "Round: 58\n",
      "minmax normal win!!\n",
      "Round: 59\n",
      "minmax zero win!!\n",
      "Round: 60\n",
      "minmax zero win!!\n",
      "Round: 61\n",
      "minmax zero win!!\n",
      "Round: 62\n",
      "minmax zero win!!\n",
      "Round: 63\n",
      "minmax zero win!!\n",
      "Round: 64\n",
      "minmax zero win!!\n",
      "Round: 65\n",
      "minmax zero win!!\n",
      "Round: 66\n",
      "minmax zero win!!\n",
      "Round: 67\n",
      "minmax zero win!!\n",
      "Round: 68\n",
      "minmax zero win!!\n",
      "Round: 69\n",
      "minmax normal win!!\n",
      "Round: 70\n",
      "minmax zero win!!\n",
      "Round: 71\n",
      "minmax zero win!!\n",
      "Round: 72\n",
      "minmax zero win!!\n",
      "Round: 73\n",
      "minmax zero win!!\n",
      "Round: 74\n",
      "minmax zero win!!\n",
      "Round: 75\n",
      "minmax zero win!!\n",
      "Round: 76\n",
      "minmax zero win!!\n",
      "Round: 77\n",
      "minmax zero win!!\n",
      "Round: 78\n",
      "minmax normal win!!\n",
      "Round: 79\n",
      "minmax zero win!!\n",
      "Round: 80\n",
      "minmax zero win!!\n",
      "Round: 81\n",
      "minmax normal win!!\n",
      "Round: 82\n",
      "minmax zero win!!\n",
      "Round: 83\n",
      "minmax zero win!!\n",
      "Round: 84\n",
      "minmax zero win!!\n",
      "Round: 85\n",
      "minmax zero win!!\n",
      "Round: 86\n",
      "minmax zero win!!\n",
      "Round: 87\n",
      "minmax zero win!!\n",
      "Round: 88\n",
      "minmax zero win!!\n",
      "Round: 89\n",
      "minmax normal win!!\n",
      "Round: 90\n",
      "minmax zero win!!\n",
      "Round: 91\n",
      "minmax zero win!!\n",
      "Round: 92\n",
      "minmax zero win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax zero win!!\n",
      "Round: 95\n",
      "minmax zero win!!\n",
      "Round: 96\n",
      "minmax zero win!!\n",
      "Round: 97\n",
      "minmax zero win!!\n",
      "Round: 98\n",
      "minmax zero win!!\n",
      "Round: 99\n",
      "minmax zero win!!\n",
      "common_steps taken are:  10\n",
      "In the previous  100  rounds, zero win ratio is:  0.85\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.0261482097279 max time elapsed: 0.199301958084\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0438335345233 max time elapsed: 0.537736177444\n",
      "Round: 0\n",
      "minmax zero win!!\n",
      "Round: 1\n",
      "minmax zero win!!\n",
      "Round: 2\n",
      "minmax zero win!!\n",
      "Round: 3\n",
      "minmax zero win!!\n",
      "Round: 4\n",
      "minmax zero win!!\n",
      "Round: 5\n",
      "minmax zero win!!\n",
      "Round: 6\n",
      "minmax normal win!!\n",
      "Round: 7\n",
      "minmax zero win!!\n",
      "Round: 8\n",
      "minmax zero win!!\n",
      "Round: 9\n",
      "minmax zero win!!\n",
      "Round: 10\n",
      "minmax zero win!!\n",
      "Round: 11\n",
      "minmax zero win!!\n",
      "Round: 12\n",
      "minmax normal win!!\n",
      "Round: 13\n",
      "minmax normal win!!\n",
      "Round: 14\n",
      "minmax zero win!!\n",
      "Round: 15\n",
      "minmax zero win!!\n",
      "Round: 16\n",
      "minmax zero win!!\n",
      "Round: 17\n",
      "minmax zero win!!\n",
      "Round: 18\n",
      "minmax zero win!!\n",
      "Round: 19\n",
      "minmax zero win!!\n",
      "Round: 20\n",
      "minmax normal win!!\n",
      "Round: 21\n",
      "minmax normal win!!\n",
      "Round: 22\n",
      "minmax zero win!!\n",
      "Round: 23\n",
      "minmax zero win!!\n",
      "Round: 24\n",
      "minmax normal win!!\n",
      "Round: 25\n",
      "minmax zero win!!\n",
      "Round: 26\n",
      "minmax zero win!!\n",
      "Round: 27\n",
      "minmax zero win!!\n",
      "Round: 28\n",
      "minmax zero win!!\n",
      "Round: 29\n",
      "minmax zero win!!\n",
      "Round: 30\n",
      "minmax zero win!!\n",
      "Round: 31\n",
      "minmax zero win!!\n",
      "Round: 32\n",
      "minmax zero win!!\n",
      "Round: 33\n",
      "minmax zero win!!\n",
      "Round: 34\n",
      "minmax normal win!!\n",
      "Round: 35\n",
      "minmax zero win!!\n",
      "Round: 36\n",
      "minmax normal win!!\n",
      "Round: 37\n",
      "minmax zero win!!\n",
      "Round: 38\n",
      "minmax zero win!!\n",
      "Round: 39\n",
      "minmax zero win!!\n",
      "Round: 40\n",
      "minmax zero win!!\n",
      "Round: 41\n",
      "minmax zero win!!\n",
      "Round: 42\n",
      "minmax zero win!!\n",
      "Round: 43\n",
      "minmax zero win!!\n",
      "Round: 44\n",
      "minmax normal win!!\n",
      "Round: 45\n",
      "minmax zero win!!\n",
      "Round: 46\n",
      "minmax zero win!!\n",
      "Round: 47\n",
      "minmax normal win!!\n",
      "Round: 48\n",
      "minmax zero win!!\n",
      "Round: 49\n",
      "minmax zero win!!\n",
      "Round: 50\n",
      "minmax zero win!!\n",
      "Round: 51\n",
      "minmax zero win!!\n",
      "Round: 52\n",
      "minmax zero win!!\n",
      "Round: 53\n",
      "minmax zero win!!\n",
      "Round: 54\n",
      "minmax zero win!!\n",
      "Round: 55\n",
      "minmax zero win!!\n",
      "Round: 56\n",
      "minmax zero win!!\n",
      "Round: 57\n",
      "minmax zero win!!\n",
      "Round: 58\n",
      "minmax zero win!!\n",
      "Round: 59\n",
      "minmax zero win!!\n",
      "Round: 60\n",
      "minmax zero win!!\n",
      "Round: 61\n",
      "minmax zero win!!\n",
      "Round: 62\n",
      "minmax normal win!!\n",
      "Round: 63\n",
      "minmax zero win!!\n",
      "Round: 64\n",
      "minmax normal win!!\n",
      "Round: 65\n",
      "minmax zero win!!\n",
      "Round: 66\n",
      "minmax zero win!!\n",
      "Round: 67\n",
      "minmax zero win!!\n",
      "Round: 68\n",
      "minmax normal win!!\n",
      "Round: 69\n",
      "minmax zero win!!\n",
      "Round: 70\n",
      "minmax normal win!!\n",
      "Round: 71\n",
      "minmax zero win!!\n",
      "Round: 72\n",
      "minmax zero win!!\n",
      "Round: 73\n",
      "minmax zero win!!\n",
      "Round: 74\n",
      "minmax zero win!!\n",
      "Round: 75\n",
      "minmax zero win!!\n",
      "Round: 76\n",
      "minmax zero win!!\n",
      "Round: 77\n",
      "minmax zero win!!\n",
      "Round: 78\n",
      "minmax normal win!!\n",
      "Round: 79\n",
      "minmax normal win!!\n",
      "Round: 80\n",
      "minmax zero win!!\n",
      "Round: 81\n",
      "minmax zero win!!\n",
      "Round: 82\n",
      "minmax zero win!!\n",
      "Round: 83\n",
      "minmax normal win!!\n",
      "Round: 84\n",
      "minmax zero win!!\n",
      "Round: 85\n",
      "minmax zero win!!\n",
      "Round: 86\n",
      "minmax normal win!!\n",
      "Round: 87\n",
      "minmax zero win!!\n",
      "Round: 88\n",
      "minmax zero win!!\n",
      "Round: 89\n",
      "minmax zero win!!\n",
      "Round: 90\n",
      "minmax zero win!!\n",
      "Round: 91\n",
      "minmax zero win!!\n",
      "Round: 92\n",
      "minmax zero win!!\n",
      "Round: 93\n",
      "minmax zero win!!\n",
      "Round: 94\n",
      "minmax zero win!!\n",
      "Round: 95\n",
      "minmax zero win!!\n",
      "Round: 96\n",
      "minmax normal win!!\n",
      "Round: 97\n",
      "minmax zero win!!\n",
      "Round: 98\n",
      "minmax zero win!!\n",
      "Round: 99\n",
      "minmax zero win!!\n",
      "common_steps taken are:  11\n",
      "In the previous  100  rounds, zero win ratio is:  0.81\n",
      "On average, for minmax normal with depth 2 , each move takes time:  0.025221744462 max time elapsed: 0.173727989197\n",
      "On average, for minmax zero with depth 2 , each move takes time:  0.0445733321247 max time elapsed: 0.525423049927\n"
     ]
    }
   ],
   "source": [
    "#minmax_normal = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_normal = MinMaxNormal()\n",
    "minmax_normal = MinMaxZero(0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "minmax_zero = MinMaxZero(0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "\n",
    "verbose = False\n",
    "rounds = 100\n",
    "N_D = 2\n",
    "Z_D = 2\n",
    "C_S = 30\n",
    "\n",
    "for common_steps in range(5, 12, 1):\n",
    "    zero_win = 0\n",
    "    time_zero = []\n",
    "    time_normal = []\n",
    "    for r in range(rounds):\n",
    "        print(\"Round:\", r)\n",
    "        steps = 0\n",
    "        a = Ataxx()\n",
    "        turn = -1\n",
    "        zero_turn = choice([-1, 1])\n",
    "        if verbose:\n",
    "            print(\"zero chose color\", zero_turn)\n",
    "        best_move = None\n",
    "        while True:\n",
    "            steps += 1\n",
    "            if verbose:\n",
    "                a.plot()\n",
    "            if turn != zero_turn:\n",
    "                s = time.time()\n",
    "                _, best_move = minmax_normal.min_max(a.data, turn, turn, steps<=4, N_D+(steps<=4), best_move)\n",
    "                span = time.time() - s\n",
    "                time_normal.append(span)\n",
    "            else:\n",
    "                s = time.time()\n",
    "                _, best_move = minmax_zero.min_max(a.data, turn, turn, steps<=common_steps, Z_D+(steps<=common_steps), best_move)\n",
    "                span = time.time() - s\n",
    "                time_zero.append(span)\n",
    "            a.move_to(turn, best_move[0], best_move[1])\n",
    "            turn = -turn\n",
    "            result = a.evaluate(zero_turn, turn)\n",
    "            if result == 1:\n",
    "                print(\"minmax zero win!!\")\n",
    "                zero_win += 1\n",
    "                break\n",
    "            elif result == -1:\n",
    "                print(\"minmax normal win!!\")\n",
    "                break\n",
    "            elif steps > 300:\n",
    "                print(\"too many steps, give up round\")\n",
    "                zero_win = 0\n",
    "                break\n",
    "    print(\"common_steps taken are: \", common_steps)\n",
    "    print(\"In the previous \", rounds, \" rounds, zero win ratio is: \", float(zero_win) / float(rounds))\n",
    "    time_zero = np.array(time_zero)\n",
    "    time_normal = np.array(time_normal)\n",
    "    print(\"On average, for minmax normal with depth\", N_D, \\\n",
    "          \", each move takes time: \", time_normal.mean(),\\\n",
    "          \"max time elapsed:\", time_normal.max())\n",
    "    print(\"On average, for minmax zero with depth\", Z_D, \\\n",
    "          \", each move takes time: \", time_zero.mean(),\\\n",
    "          \"max time elapsed:\", time_zero.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T12:28:53.844259Z",
     "start_time": "2018-03-08T12:28:53.797567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 6, 9, 9)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 7, 7)     3520        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 7, 7)     256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 64, 7, 7)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 7, 7)     36928       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 7, 7)     256         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 64, 7, 7)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, 7, 7)     36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 7, 7)     256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64, 7, 7)     0           batch_normalization_28[0][0]     \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 64, 7, 7)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 2, 7, 7)      130         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 2, 7, 7)      8           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 1, 7, 7)      65          activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2, 7, 7)      0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 1, 7, 7)      4           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 98)           0           activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 1, 7, 7)      0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 792)          78408       flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 792)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 49)           0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 792)          0           dense_16[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          6400        flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 792)          0           multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            129         dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 163,288\n",
      "Trainable params: 162,898\n",
      "Non-trainable params: 390\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a = [(1, 1), (2, 2)]\n",
    "sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cell, I compared two settings for ataxxzero, and find that it is great if we let AtaxxZero after doing normal minmax for 30 or so steps\n",
    "1. in the setting where no common mimmax searching init steps were taken, the win ratio of minmaxZeroQ with dep 2 against minmax_normal with dep 3 is 77%\n",
    "2. in the other setting where normal searching common init steps are taken the win ratio of minmaxZeroQ increased to 92%\n",
    "\n",
    "__The conclusion that relying on AtaxxZero in the start of the game is not useful, as the Q returned is very small, and therefore inaccuracy were amplified in terms of the order of moves. (a similar error may lead to the result that a worse move have a larger Q returned)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T18:43:47.053420Z",
     "start_time": "2018-03-06T18:43:27.556Z"
    }
   },
   "outputs": [],
   "source": [
    "#minmax_common = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_common = MinMaxNormal()\n",
    "minmax_common = None\n",
    "#minmax_normal = MinMaxZero(1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "minmax_normal = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_normal = MinMaxNormal()\n",
    "minmax_zero = MinMaxZero(0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_zero = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_zero = MinMaxQCluster(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#actor_compare(minmax_common, minmax_normal, minmax_zero, 3, 3, 2, 30, 100, verbose=False)\n",
    "actor_compare(minmax_common, minmax_normal, minmax_zero, 3, 3, 2, 0, 100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T22:36:04.230863Z",
     "start_time": "2018-03-05T22:35:16.506Z"
    }
   },
   "outputs": [],
   "source": [
    "#minmax_common = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "minmax_common = MinMaxNormal()\n",
    "#minmax_normal = MinMaxZero(1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "minmax_normal = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_normal = MinMaxNormal()\n",
    "minmax_zero = MinMaxZero(0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_zero = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_zero = MinMaxQCluster(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "actor_compare(minmax_common, minmax_normal, minmax_zero, 3, 2, 2, 30, 100, verbose=False)\n",
    "actor_compare(minmax_common, minmax_normal, minmax_zero, 3, 2, 2, 0, 100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T20:54:23.621332Z",
     "start_time": "2018-03-05T20:54:22.655798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded the model\n"
     ]
    }
   ],
   "source": [
    "a = PolicyValueNetwork(\"AtaxxZero_91.2p_82.4q.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T20:54:23.696713Z",
     "start_time": "2018-03-05T20:54:23.684025Z"
    }
   },
   "outputs": [],
   "source": [
    "def loop_predict(a, N):\n",
    "    f = np.zeros((1, 6, 9, 9))\n",
    "    m = np.ones((1, 792))\n",
    "    for i in range(N):\n",
    "        a.predict(f, m)\n",
    "\n",
    "def cluster_predict(a, N):\n",
    "    f = np.zeros((N, 6, 9, 9))\n",
    "    m = np.ones((N, 792))\n",
    "    a.predict(f, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T20:54:32.304318Z",
     "start_time": "2018-03-05T20:54:32.276627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.8 ms, sys: 11.2 ms, total: 46 ms\n",
      "Wall time: 16.6 ms\n",
      "CPU times: user 24.3 ms, sys: 2.33 ms, total: 26.6 ms\n",
      "Wall time: 5.16 ms\n"
     ]
    }
   ],
   "source": [
    "%time loop_predict(a, 10)\n",
    "%time cluster_predict(a, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T21:22:58.479203Z",
     "start_time": "2018-03-05T21:22:19.949138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded the model\n",
      "p_thresh is: 0.3\n",
      "successfully loaded the model\n",
      "Round: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1dd6037ae42e>\u001b[0m in \u001b[0;36mactor\u001b[0;34m(mm_obj, board, turn, depth, pre_move)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_move\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_move\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mmin_max\u001b[0;34m(self, board, turn, target_turn, depth, alpha, beta, is_max, is_root, pre_move, t_lim)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;31m# do searching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0;34m-\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# record all the move and its alpha value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mmin_max\u001b[0;34m(self, board, turn, target_turn, depth, alpha, beta, is_max, is_root, pre_move, t_lim)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;31m# do searching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0;34m-\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# record all the move and its alpha value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mmove_to\u001b[0;34m(board, turn, pos0, pos1)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# convert any piece of the opponent to 'turn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                         \u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f6f470fa15fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p_thresh is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mminmax_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AtaxxZero_91.2p_82.4q.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmax_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-90a284e37d27>\u001b[0m in \u001b[0;36mactor_compare\u001b[0;34m(minmax_normal, minmax_zero, N_D, Z_D, rounds, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mturn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mzero_turn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mbest_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_move\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtime_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1dd6037ae42e>\u001b[0m in \u001b[0;36mactor\u001b[0;34m(mm_obj, board, turn, depth, pre_move)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_move\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_move\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mmin_max\u001b[0;34m(self, board, turn, target_turn, depth, alpha, beta, is_max, is_root, pre_move, t_lim)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_moves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;31m# do searching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0;34m-\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# record all the move and its alpha value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_root\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mmin_max\u001b[0;34m(self, board, turn, target_turn, depth, alpha, beta, is_max, is_root, pre_move, t_lim)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_moves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;31m# do searching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0;34m-\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# record all the move and its alpha value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_root\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mmin_max\u001b[0;34m(self, board, turn, target_turn, depth, alpha, beta, is_max, is_root, pre_move, t_lim)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# otherwise calculate p and q and do the NN pruned minmax searching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mfeature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_move\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# once the recursion reaches the end, return the leaf node value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cbe426fbc2e6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, feature_map, action_mask, turn, target_turn)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mturn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mturn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget_turn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-38c5da50709d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, feature_map, action_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m792\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#minmax_normal = MinMaxZero(1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "minmax_normal = MinMaxQ(\"AtaxxZero_91.2p_82.4q.h5\")\n",
    "#minmax_normal = MinMaxNormal()\n",
    "result = []\n",
    "xs = []\n",
    "for x in np.linspace(0.3, 1, 15):\n",
    "    print(\"p_thresh is:\", x)\n",
    "    minmax_zero = MinMaxZero(x, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "    result.append(actor_compare(minmax_normal, minmax_zero, 2, 2, 100, verbose=False))\n",
    "    xs.append(x)\n",
    "    \n",
    "plt.plot(xs, result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T00:35:23.076608Z",
     "start_time": "2018-03-05T00:35:22.957404Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def actor_compare_bunch(normal_para, zero_para, N_D, Z_D, rounds):\n",
    "    minmax_normal = MinMaxZero(*normal_para)\n",
    "    minmax_zero = MinMaxZero(*zero_para)\n",
    "    zero_win = 0\n",
    "    for r in range(rounds):\n",
    "        print(\"Round:\", r)\n",
    "        steps = 0\n",
    "        a = Ataxx()\n",
    "        turn = -1\n",
    "        zero_turn = choice([-1, 1])\n",
    "        if verbose:\n",
    "            print(\"zero chose color\", zero_turn)\n",
    "        best_move = None\n",
    "        while True:\n",
    "            steps += 1\n",
    "            if verbose:\n",
    "                a.plot()\n",
    "            if turn != zero_turn:\n",
    "                s = time.time()\n",
    "                best_move = actor(minmax_normal, a.data, turn, N_D, best_move)\n",
    "                span = time.time() - s\n",
    "                time_normal.append(span)\n",
    "            else:\n",
    "                s = time.time()\n",
    "                best_move = actor(minmax_zero, a.data, turn, Z_D, best_move)\n",
    "                span = time.time() - s\n",
    "                time_zero.append(span)\n",
    "            a.move_to(turn, best_move[0], best_move[1])\n",
    "            turn = -turn\n",
    "            result = a.evaluate(zero_turn, turn)\n",
    "            if result == 1:\n",
    "                print(\"minmax zero win!!\")\n",
    "                zero_win += 1\n",
    "                break\n",
    "            elif result == -1:\n",
    "                print(\"minmax normal win!!\")\n",
    "                break\n",
    "    return float(zero_win) / float(rounds)\n",
    "\n",
    "def actor_compare_parallel(minmax_normal, minmax_zero, N_D, Z_D, rounds):\n",
    "    zero_win = 0\n",
    "    bunch = (rounds / 24)\n",
    "    rounds = 24 * bunch\n",
    "    para = [(minmax_normal, minmax_zero, N_D, Z_D, bunch)] * 24\n",
    "    with Pool(processes=24) as pool:\n",
    "        result = pool.starmap(actor_compare_bunch, para)\n",
    "    print(\"In the previous \", rounds, \" rounds, zero win ratio is: \", sum(result) / float(rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T00:35:23.702152Z",
     "start_time": "2018-03-05T00:35:23.698005Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_para = (1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "zero_para = (0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T00:36:09.397530Z",
     "start_time": "2018-03-05T00:35:24.532670Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded the model\n",
      "successfully loaded the model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-d1bf97c4a741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# minmax_normal = MinMaxNormal()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mminmax_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AtaxxZero_91.2p_82.4q.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mactor_compare_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_para\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_para\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mactor_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmax_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-f9cb6daa6b96>\u001b[0m in \u001b[0;36mactor_compare_parallel\u001b[0;34m(minmax_normal, minmax_zero, N_D, Z_D, rounds)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpara\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminmax_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminmax_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbunch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_compare_bunch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In the previous \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" rounds, zero win ratio is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minmax_normal = MinMaxZero(1, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "# minmax_normal = MinMaxNormal()\n",
    "minmax_zero = MinMaxZero(0.9, 100, \"AtaxxZero_91.2p_82.4q.h5\")\n",
    "actor_compare_parallel(normal_para, zero_para, 2, 2, 100)\n",
    "actor_compare(minmax_normal, minmax_zero, 2, 2, 100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-05T20:53:18.454413Z",
     "start_time": "2018-03-05T20:53:17.675103Z"
    }
   },
   "outputs": [],
   "source": [
    "a = Ataxx()\n",
    "%lprun -f minmax_zero.min_max _, best_move = minmax_zero.min_max(a.data, -1, -1, depth=6, pre_move=None, t_lim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 484,
   "position": {
    "height": "506px",
    "left": "1360px",
    "right": "36px",
    "top": "125px",
    "width": "284px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
